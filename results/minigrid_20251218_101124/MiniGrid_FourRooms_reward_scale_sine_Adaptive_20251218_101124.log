wandb: Currently logged in as: hungtrab (hungtrab-hanoi-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run ns0mdxkv
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124/wandb/run-20251218_103259-ns0mdxkv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124
wandb: â­ï¸ View project at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: ðŸš€ View run at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/ns0mdxkv
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
--- Training Start: MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124 ---
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: sine
>>> Initializing PPO with kwargs: ['policy', 'env', 'learning_rate', 'gamma', 'verbose', 'tensorboard_log', 'n_steps', 'batch_size']
Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Logging to logs/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124_0
>>> [DriftAdaptiveCallback] Training Started
    Algorithm: PPO
    Target Param: reward_scale (base=9.8)
    Scale Factor: 0.15
    
    Adaptive Hyperparameters:
      - Learning Rate: 0.000300
      - Clip Range: 0.200 (adapt=True)
      - Entropy Coef: 0.0100 (adapt=True)
-----------------------------------
| adaptive/            |          |
|    adaptation_factor | 1        |
|    algorithm         | PPO      |
|    base_clip_range   | 0.2      |
|    base_lr           | 0.0003   |
|    clip_range        | 0.2      |
|    drift_magnitude   | 0        |
|    ent_coef          | 0.01     |
|    learning_rate     | 0.0003   |
| env/                 |          |
|    base_value        | 9.8      |
|    reward_scale      | 9.8      |
| rollout/             |          |
|    ep_len_mean       | 100      |
|    ep_rew_mean       | 0        |
| time/                |          |
|    fps               | 689      |
|    iterations        | 1        |
|    time_elapsed      | 0        |
|    total_timesteps   | 512      |
-----------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 2            |
|    time_elapsed         | 1            |
|    total_timesteps      | 1024         |
| train/                  |              |
|    approx_kl            | 5.508552e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -4.23        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0214      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000488    |
|    value_loss           | 4.67e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 700          |
|    iterations           | 3            |
|    time_elapsed         | 2            |
|    total_timesteps      | 1536         |
| train/                  |              |
|    approx_kl            | 0.0002498842 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -6.08        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00075     |
|    value_loss           | 1.13e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 722          |
|    iterations           | 4            |
|    time_elapsed         | 2            |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0003395665 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -4.07        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.017       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.000636    |
|    value_loss           | 6.95e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 728          |
|    iterations           | 5            |
|    time_elapsed         | 3            |
|    total_timesteps      | 2560         |
| train/                  |              |
|    approx_kl            | 0.0062134704 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -7.18        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0328      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00374     |
|    value_loss           | 5.94e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 6           |
|    time_elapsed         | 4           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.003850022 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -4.29       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 5.68e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 745          |
|    iterations           | 7            |
|    time_elapsed         | 4            |
|    total_timesteps      | 3584         |
| train/                  |              |
|    approx_kl            | 0.0054174704 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -7.15        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0422      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00547     |
|    value_loss           | 4.09e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 8            |
|    time_elapsed         | 5            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0017505302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -11          |
|    learning_rate        | 0.0003       |
|    loss                 | -0.015       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 5.91e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 9           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.010204405 |
|    clip_fraction        | 0.0191      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -7.07       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.045      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 5.56e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 10           |
|    time_elapsed         | 6            |
|    total_timesteps      | 5120         |
| train/                  |              |
|    approx_kl            | 0.0042461734 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | -7.96        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0314      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00461     |
|    value_loss           | 3.97e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 11           |
|    time_elapsed         | 7            |
|    total_timesteps      | 5632         |
| train/                  |              |
|    approx_kl            | 0.0004888413 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -5.85        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0239      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000818    |
|    value_loss           | 2.33e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 12           |
|    time_elapsed         | 8            |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0031210277 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -1.75        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0365      |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00502     |
|    value_loss           | 1.67e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 13          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6656        |
| train/                  |             |
|    approx_kl            | 0.003252822 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -5.18       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0293     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 2.73e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 14           |
|    time_elapsed         | 9            |
|    total_timesteps      | 7168         |
| train/                  |              |
|    approx_kl            | 0.0022606985 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -6.61        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0285      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00445     |
|    value_loss           | 2.47e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 15           |
|    time_elapsed         | 10           |
|    total_timesteps      | 7680         |
| train/                  |              |
|    approx_kl            | 0.0003435224 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -6.96        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0211      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000952    |
|    value_loss           | 3.2e-06      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 16           |
|    time_elapsed         | 10           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0005843836 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -1.85        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0231      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 1.77e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | 0.00418     |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 17          |
|    time_elapsed         | 11          |
|    total_timesteps      | 8704        |
| train/                  |             |
|    approx_kl            | 0.008382577 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -2.82       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0322     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 2.1e-06     |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 99.8       |
|    ep_rew_mean          | 0.00395    |
| time/                   |            |
|    fps                  | 758        |
|    iterations           | 18         |
|    time_elapsed         | 12         |
|    total_timesteps      | 9216       |
| train/                  |            |
|    approx_kl            | 0.00508856 |
|    clip_fraction        | 0.000781   |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | -0.0142    |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0145    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.00425   |
|    value_loss           | 0.00204    |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | 0.00375     |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 19          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9728        |
| train/                  |             |
|    approx_kl            | 0.007615083 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -3.03       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0485     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 3.22e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.8         |
|    ep_rew_mean          | 0.00364      |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 20           |
|    time_elapsed         | 13           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0027207388 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | -2.65        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0132      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 7.08e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.8         |
|    ep_rew_mean          | 0.00364      |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 21           |
|    time_elapsed         | 14           |
|    total_timesteps      | 10752        |
| train/                  |              |
|    approx_kl            | 0.0056324806 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | -0.981       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0471      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00499     |
|    value_loss           | 3.86e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | 0.00364     |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 22          |
|    time_elapsed         | 14          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.008553694 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -1.9        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 2.51e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.8         |
|    ep_rew_mean          | 0.00364      |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 23           |
|    time_elapsed         | 15           |
|    total_timesteps      | 11776        |
| train/                  |              |
|    approx_kl            | 0.0043822993 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | -5.07        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0228      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00207     |
|    value_loss           | 2.69e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | 0.00364     |
| time/                   |             |
|    fps                  | 757         |
|    iterations           | 24          |
|    time_elapsed         | 16          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011326121 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -1.35       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0457     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 4.27e-06    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.1        |
|    ep_rew_mean          | 0.0106      |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 25          |
|    time_elapsed         | 16          |
|    total_timesteps      | 12800       |
| train/                  |             |
|    approx_kl            | 0.010324091 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0481     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 2.06e-06    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.1        |
|    ep_rew_mean          | 0.0106      |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 26          |
|    time_elapsed         | 17          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.004268709 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.0164      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0448     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 0.00668     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.1        |
|    ep_rew_mean          | 0.0106      |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 27          |
|    time_elapsed         | 18          |
|    total_timesteps      | 13824       |
| train/                  |             |
|    approx_kl            | 0.006234724 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -2.66       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0255     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00453    |
|    value_loss           | 0.000416    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0137       |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 28           |
|    time_elapsed         | 19           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0075710705 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.82        |
|    explained_variance   | -2.65        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0365      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 4.96e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0137      |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 29          |
|    time_elapsed         | 19          |
|    total_timesteps      | 14848       |
| train/                  |             |
|    approx_kl            | 0.011252997 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.039       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0416     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 0.0012      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0193      |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 30          |
|    time_elapsed         | 20          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.008772105 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -2.81       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0354     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 4.09e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0193      |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 31          |
|    time_elapsed         | 21          |
|    total_timesteps      | 15872       |
| train/                  |             |
|    approx_kl            | 0.017056558 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.00153     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0406     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.00424     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0193      |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 32          |
|    time_elapsed         | 21          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.009449277 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -3.07       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 0.000139    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0193      |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 33          |
|    time_elapsed         | 22          |
|    total_timesteps      | 16896       |
| train/                  |             |
|    approx_kl            | 0.009762032 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.536      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.034      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 5.74e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0193       |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 34           |
|    time_elapsed         | 23           |
|    total_timesteps      | 17408        |
| train/                  |              |
|    approx_kl            | 0.0027548233 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -2.42        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0161      |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 2.12e-05     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 97.5       |
|    ep_rew_mean          | 0.0245     |
| time/                   |            |
|    fps                  | 749        |
|    iterations           | 35         |
|    time_elapsed         | 23         |
|    total_timesteps      | 17920      |
| train/                  |            |
|    approx_kl            | 0.00867152 |
|    clip_fraction        | 0.0508     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | -1.2       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0449    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.00932   |
|    value_loss           | 2.08e-05   |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.7        |
|    ep_rew_mean          | 0.0209      |
| time/                   |             |
|    fps                  | 747         |
|    iterations           | 36          |
|    time_elapsed         | 24          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010638662 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.0435      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0121     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 0.00372     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.7         |
|    ep_rew_mean          | 0.0209       |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 37           |
|    time_elapsed         | 25           |
|    total_timesteps      | 18944        |
| train/                  |              |
|    approx_kl            | 0.0041804817 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | -0.761       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000485     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 8.25e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.7        |
|    ep_rew_mean          | 0.0209      |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 38          |
|    time_elapsed         | 26          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.009854183 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.063      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0363     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 4.03e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.4        |
|    ep_rew_mean          | 0.0235      |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 39          |
|    time_elapsed         | 26          |
|    total_timesteps      | 19968       |
| train/                  |             |
|    approx_kl            | 0.009286411 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -0.8        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00825    |
|    value_loss           | 3.3e-05     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.6        |
|    ep_rew_mean          | 0.0291      |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 40          |
|    time_elapsed         | 27          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.012966426 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.0482      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0535     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.000836    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.4        |
|    ep_rew_mean          | 0.0313      |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 41          |
|    time_elapsed         | 28          |
|    total_timesteps      | 20992       |
| train/                  |             |
|    approx_kl            | 0.012861797 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.0193      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0513     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 0.00438     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.9        |
|    ep_rew_mean          | 0.0357      |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 42          |
|    time_elapsed         | 28          |
|    total_timesteps      | 21504       |
| train/                  |             |
|    approx_kl            | 0.010219719 |
|    clip_fraction        | 0.0363      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0408     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.000454    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.9        |
|    ep_rew_mean          | 0.0357      |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 43          |
|    time_elapsed         | 29          |
|    total_timesteps      | 22016       |
| train/                  |             |
|    approx_kl            | 0.005964582 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0363     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 0.00227     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | 0.0333      |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 44          |
|    time_elapsed         | 30          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.007921403 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -5.47       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0333     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 5.52e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 96           |
|    ep_rew_mean          | 0.0333       |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 45           |
|    time_elapsed         | 31           |
|    total_timesteps      | 23040        |
| train/                  |              |
|    approx_kl            | 0.0025438918 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.078        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.025       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.00264      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | 0.0333      |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 46          |
|    time_elapsed         | 31          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.006694598 |
|    clip_fraction        | 0.00996     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -2.25       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0141     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00342    |
|    value_loss           | 8.11e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.0303      |
| time/                   |             |
|    fps                  | 742         |
|    iterations           | 47          |
|    time_elapsed         | 32          |
|    total_timesteps      | 24064       |
| train/                  |             |
|    approx_kl            | 0.012865858 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -3.72       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0466     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 3.2e-05     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.0303      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 48          |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014627309 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -4.97       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00905    |
|    value_loss           | 1.86e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0246      |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 49          |
|    time_elapsed         | 33          |
|    total_timesteps      | 25088       |
| train/                  |             |
|    approx_kl            | 0.013513451 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | -2.04       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 1.72e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.032       |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 50          |
|    time_elapsed         | 34          |
|    total_timesteps      | 25600       |
| train/                  |             |
|    approx_kl            | 0.011744107 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | -2.06       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00301     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 1.62e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.032       |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 51          |
|    time_elapsed         | 35          |
|    total_timesteps      | 26112       |
| train/                  |             |
|    approx_kl            | 0.007067155 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.0319      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 0.00777     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.032       |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 52          |
|    time_elapsed         | 35          |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.016233496 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | -2.14       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.0082     |
|    value_loss           | 0.000163    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.3        |
|    ep_rew_mean          | 0.0432      |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 53          |
|    time_elapsed         | 36          |
|    total_timesteps      | 27136       |
| train/                  |             |
|    approx_kl            | 0.010659853 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | -2.63       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0461     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 5.38e-05    |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 96         |
|    ep_rew_mean          | 0.0379     |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 54         |
|    time_elapsed         | 37         |
|    total_timesteps      | 27648      |
| train/                  |            |
|    approx_kl            | 0.01342969 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | 0.0711     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.03      |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.00395    |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.3        |
|    ep_rew_mean          | 0.0462      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 55          |
|    time_elapsed         | 37          |
|    total_timesteps      | 28160       |
| train/                  |             |
|    approx_kl            | 0.010620654 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | -11.6       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0221     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 5.83e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.3        |
|    ep_rew_mean          | 0.0462      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 56          |
|    time_elapsed         | 38          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.013675329 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0467      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0424     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.00947     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95          |
|    ep_rew_mean          | 0.0514      |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 57          |
|    time_elapsed         | 39          |
|    total_timesteps      | 29184       |
| train/                  |             |
|    approx_kl            | 0.008779084 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | -4.87       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 0.000326    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.1        |
|    ep_rew_mean          | 0.0432      |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 58          |
|    time_elapsed         | 40          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.011300271 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 0.00343     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.7         |
|    ep_rew_mean          | 0.0487       |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 59           |
|    time_elapsed         | 40           |
|    total_timesteps      | 30208        |
| train/                  |              |
|    approx_kl            | 0.0046588047 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.39        |
|    explained_variance   | -2.35        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0253      |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00532     |
|    value_loss           | 0.000101     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.5         |
|    ep_rew_mean          | 0.0525       |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 60           |
|    time_elapsed         | 41           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0030725263 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.42        |
|    explained_variance   | 0.0506       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0166      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00328     |
|    value_loss           | 0.00341      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 94.9         |
|    ep_rew_mean          | 0.0641       |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 61           |
|    time_elapsed         | 42           |
|    total_timesteps      | 31232        |
| train/                  |              |
|    approx_kl            | 0.0029012477 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.41        |
|    explained_variance   | 0.0445       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 0.00429      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 94.2        |
|    ep_rew_mean          | 0.0748      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 62          |
|    time_elapsed         | 42          |
|    total_timesteps      | 31744       |
| train/                  |             |
|    approx_kl            | 0.007508216 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.145       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0434     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00417    |
|    value_loss           | 0.0145      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 94.3        |
|    ep_rew_mean          | 0.0762      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 63          |
|    time_elapsed         | 43          |
|    total_timesteps      | 32256       |
| train/                  |             |
|    approx_kl            | 0.009057246 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00185    |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 0.00934     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 93.6         |
|    ep_rew_mean          | 0.0859       |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 64           |
|    time_elapsed         | 44           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0073320614 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.44        |
|    explained_variance   | 0.256        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0186      |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 0.00344      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 92.7        |
|    ep_rew_mean          | 0.0968      |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 65          |
|    time_elapsed         | 44          |
|    total_timesteps      | 33280       |
| train/                  |             |
|    approx_kl            | 0.007887868 |
|    clip_fraction        | 0.09        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | -0.0116     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0409     |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00782    |
|    value_loss           | 0.00929     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 91.8         |
|    ep_rew_mean          | 0.109        |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 66           |
|    time_elapsed         | 45           |
|    total_timesteps      | 33792        |
| train/                  |              |
|    approx_kl            | 0.0051498576 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.4         |
|    explained_variance   | 0.0171       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00569     |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 0.012        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 91.3         |
|    ep_rew_mean          | 0.115        |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 67           |
|    time_elapsed         | 46           |
|    total_timesteps      | 34304        |
| train/                  |              |
|    approx_kl            | 0.0034972215 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.44        |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0177      |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 0.00573      |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | 0.119      |
| time/                   |            |
|    fps                  | 738        |
|    iterations           | 68         |
|    time_elapsed         | 47         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.00513383 |
|    clip_fraction        | 0.0594     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.126      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00989   |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 0.00344    |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 90.1         |
|    ep_rew_mean          | 0.13         |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 69           |
|    time_elapsed         | 47           |
|    total_timesteps      | 35328        |
| train/                  |              |
|    approx_kl            | 0.0155284675 |
|    clip_fraction        | 0.0883       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.42        |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0397      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.0047       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 90.1         |
|    ep_rew_mean          | 0.13         |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 70           |
|    time_elapsed         | 48           |
|    total_timesteps      | 35840        |
| train/                  |              |
|    approx_kl            | 0.0032091485 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.45        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0354      |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00379     |
|    value_loss           | 0.00721      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.1        |
|    ep_rew_mean          | 0.119       |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 71          |
|    time_elapsed         | 49          |
|    total_timesteps      | 36352       |
| train/                  |             |
|    approx_kl            | 0.018093107 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.052      |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.000363    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 91.1         |
|    ep_rew_mean          | 0.119        |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 72           |
|    time_elapsed         | 50           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0038185373 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | -4.79        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0175      |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 0.000324     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.8        |
|    ep_rew_mean          | 0.111       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 73          |
|    time_elapsed         | 50          |
|    total_timesteps      | 37376       |
| train/                  |             |
|    approx_kl            | 0.008164969 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -6.63       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.027      |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 0.000222    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.3        |
|    ep_rew_mean          | 0.116       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 74          |
|    time_elapsed         | 51          |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.019064147 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | -7.51       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0328     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.000151    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 91.6         |
|    ep_rew_mean          | 0.111        |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 75           |
|    time_elapsed         | 52           |
|    total_timesteps      | 38400        |
| train/                  |              |
|    approx_kl            | 0.0063332953 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00657      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00418     |
|    value_loss           | 0.00364      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.6        |
|    ep_rew_mean          | 0.111       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 76          |
|    time_elapsed         | 52          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.006803856 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | -4.37       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0231     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 0.000265    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.4        |
|    ep_rew_mean          | 0.111       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 77          |
|    time_elapsed         | 53          |
|    total_timesteps      | 39424       |
| train/                  |             |
|    approx_kl            | 0.008915308 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -3.5        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.000237    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91          |
|    ep_rew_mean          | 0.112       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 78          |
|    time_elapsed         | 54          |
|    total_timesteps      | 39936       |
| train/                  |             |
|    approx_kl            | 0.003689574 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0222     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00273    |
|    value_loss           | 0.00335     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 92.1        |
|    ep_rew_mean          | 0.0963      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 79          |
|    time_elapsed         | 55          |
|    total_timesteps      | 40448       |
| train/                  |             |
|    approx_kl            | 0.009567777 |
|    clip_fraction        | 0.0113      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0177     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 0.00443     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 92.1        |
|    ep_rew_mean          | 0.0917      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 80          |
|    time_elapsed         | 55          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.012490472 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -3.39       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0353     |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 0.000549    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 92           |
|    ep_rew_mean          | 0.0895       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 81           |
|    time_elapsed         | 56           |
|    total_timesteps      | 41472        |
| train/                  |              |
|    approx_kl            | 0.0046516373 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0132      |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 0.00378      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 92.2         |
|    ep_rew_mean          | 0.0841       |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 82           |
|    time_elapsed         | 56           |
|    total_timesteps      | 41984        |
| train/                  |              |
|    approx_kl            | 0.0029512118 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0185      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 0.00143      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 93.1        |
|    ep_rew_mean          | 0.0731      |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 83          |
|    time_elapsed         | 57          |
|    total_timesteps      | 42496       |
| train/                  |             |
|    approx_kl            | 0.005895848 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0383     |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 0.00209     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 94          |
|    ep_rew_mean          | 0.061       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 84          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.002653331 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | -4.65       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0217     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 0.0004      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 93.9        |
|    ep_rew_mean          | 0.0596      |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 85          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43520       |
| train/                  |             |
|    approx_kl            | 0.009955764 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.751      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0385     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 0.000286    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 93.9         |
|    ep_rew_mean          | 0.0596       |
| time/                   |              |
|    fps                  | 739          |
|    iterations           | 86           |
|    time_elapsed         | 59           |
|    total_timesteps      | 44032        |
| train/                  |              |
|    approx_kl            | 0.0068531004 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.149        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00685     |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00345     |
|    value_loss           | 0.00268      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | 0.0368      |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 87          |
|    time_elapsed         | 60          |
|    total_timesteps      | 44544       |
| train/                  |             |
|    approx_kl            | 0.012989029 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.98       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 0.000176    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | 0.0368      |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 88          |
|    time_elapsed         | 60          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.007280483 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -2.68       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.041      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 5.46e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | 0.0368      |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 89          |
|    time_elapsed         | 61          |
|    total_timesteps      | 45568       |
| train/                  |             |
|    approx_kl            | 0.004228802 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -0.925      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0203     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00469    |
|    value_loss           | 6.63e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.8         |
|    ep_rew_mean          | 0.0368       |
| time/                   |              |
|    fps                  | 739          |
|    iterations           | 90           |
|    time_elapsed         | 62           |
|    total_timesteps      | 46080        |
| train/                  |              |
|    approx_kl            | 0.0035883712 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -5.49        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0339      |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00708     |
|    value_loss           | 0.000479     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.8        |
|    ep_rew_mean          | 0.0368      |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 91          |
|    time_elapsed         | 63          |
|    total_timesteps      | 46592       |
| train/                  |             |
|    approx_kl            | 0.011298526 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -6.97       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0418     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 8.37e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.8         |
|    ep_rew_mean          | 0.0368       |
| time/                   |              |
|    fps                  | 739          |
|    iterations           | 92           |
|    time_elapsed         | 63           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0036758727 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -0.597       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.028       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 7.74e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.0313      |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 93          |
|    time_elapsed         | 64          |
|    total_timesteps      | 47616       |
| train/                  |             |
|    approx_kl            | 0.007941056 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -7.03       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00765    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 6.4e-05     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.3        |
|    ep_rew_mean          | 0.0313      |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 94          |
|    time_elapsed         | 65          |
|    total_timesteps      | 48128       |
| train/                  |             |
|    approx_kl            | 0.010310931 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -7.02       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0381     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 2.88e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 96.3         |
|    ep_rew_mean          | 0.0313       |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 95           |
|    time_elapsed         | 65           |
|    total_timesteps      | 48640        |
| train/                  |              |
|    approx_kl            | 0.0023359472 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -11.3        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0258      |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 3.61e-05     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 96.9       |
|    ep_rew_mean          | 0.0256     |
| time/                   |            |
|    fps                  | 737        |
|    iterations           | 96         |
|    time_elapsed         | 66         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01533318 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.83      |
|    explained_variance   | -5.38      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0497    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 8.77e-05   |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.5         |
|    ep_rew_mean          | 0.0211       |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 97           |
|    time_elapsed         | 67           |
|    total_timesteps      | 49664        |
| train/                  |              |
|    approx_kl            | 0.0007349709 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | -5.29        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0216      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 2.06e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 96.8         |
|    ep_rew_mean          | 0.0286       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 98           |
|    time_elapsed         | 68           |
|    total_timesteps      | 50176        |
| train/                  |              |
|    approx_kl            | 0.0048309797 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0353      |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00577     |
|    value_loss           | 0.00102      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 97.5          |
|    ep_rew_mean          | 0.0226        |
| time/                   |               |
|    fps                  | 736           |
|    iterations           | 99            |
|    time_elapsed         | 68            |
|    total_timesteps      | 50688         |
| train/                  |               |
|    approx_kl            | 0.00077824446 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.85         |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0114       |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.000955     |
|    value_loss           | 0.00723       |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0187      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 100         |
|    time_elapsed         | 69          |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.011642527 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -4.24       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0578     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 0.000173    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 98.5          |
|    ep_rew_mean          | 0.0145        |
| time/                   |               |
|    fps                  | 736           |
|    iterations           | 101           |
|    time_elapsed         | 70            |
|    total_timesteps      | 51712         |
| train/                  |               |
|    approx_kl            | 0.00052589143 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -3.46         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0167       |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000662     |
|    value_loss           | 2.97e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.5        |
|    ep_rew_mean          | 0.0145      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 102         |
|    time_elapsed         | 70          |
|    total_timesteps      | 52224       |
| train/                  |             |
|    approx_kl            | 0.013030197 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -2.89       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.038      |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.000204    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.5         |
|    ep_rew_mean          | 0.0145       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 103          |
|    time_elapsed         | 71           |
|    total_timesteps      | 52736        |
| train/                  |              |
|    approx_kl            | 0.0029371874 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -1.29        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00897     |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 4.03e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99          |
|    ep_rew_mean          | 0.0119      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 104         |
|    time_elapsed         | 72          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.008587351 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -3.52       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.034      |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 0.000361    |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 98.9       |
|    ep_rew_mean          | 0.0146     |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 105        |
|    time_elapsed         | 73         |
|    total_timesteps      | 53760      |
| train/                  |            |
|    approx_kl            | 0.01127855 |
|    clip_fraction        | 0.0467     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0386    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00788   |
|    value_loss           | 0.000431   |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0231       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 106          |
|    time_elapsed         | 73           |
|    total_timesteps      | 54272        |
| train/                  |              |
|    approx_kl            | 0.0066257115 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.178        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0392      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00504     |
|    value_loss           | 0.000995     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0231      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 107         |
|    time_elapsed         | 74          |
|    total_timesteps      | 54784       |
| train/                  |             |
|    approx_kl            | 0.014130189 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0357     |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.00954     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0231      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 108         |
|    time_elapsed         | 75          |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.007304675 |
|    clip_fraction        | 0.0113      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -2.28       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0265     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 0.000181    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0231       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 109          |
|    time_elapsed         | 75           |
|    total_timesteps      | 55808        |
| train/                  |              |
|    approx_kl            | 0.0023044108 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -7.48        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0105      |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 6.82e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0231      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 110         |
|    time_elapsed         | 76          |
|    total_timesteps      | 56320       |
| train/                  |             |
|    approx_kl            | 0.009403432 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -1.1        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0475     |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 0.000449    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0231       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 111          |
|    time_elapsed         | 77           |
|    total_timesteps      | 56832        |
| train/                  |              |
|    approx_kl            | 0.0016743488 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | -2.53        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0228      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 3.69e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0231       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 112          |
|    time_elapsed         | 77           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0096325725 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -3.01        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00601     |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 0.000115     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0231       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 113          |
|    time_elapsed         | 78           |
|    total_timesteps      | 57856        |
| train/                  |              |
|    approx_kl            | 0.0024348875 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -1.07        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0249      |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 2.48e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0273      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 114         |
|    time_elapsed         | 79          |
|    total_timesteps      | 58368       |
| train/                  |             |
|    approx_kl            | 0.006699752 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -6.97       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.017      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00865    |
|    value_loss           | 0.000148    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | 0.0273       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 115          |
|    time_elapsed         | 79           |
|    total_timesteps      | 58880        |
| train/                  |              |
|    approx_kl            | 0.0018469832 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0237      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00381     |
|    value_loss           | 0.00241      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0249      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 116         |
|    time_elapsed         | 80          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.007840386 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -1.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0113     |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 7.14e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.9        |
|    ep_rew_mean          | 0.0174      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 117         |
|    time_elapsed         | 81          |
|    total_timesteps      | 59904       |
| train/                  |             |
|    approx_kl            | 0.013099523 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -2.01       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0414     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00771    |
|    value_loss           | 8.24e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.9        |
|    ep_rew_mean          | 0.0174      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 118         |
|    time_elapsed         | 82          |
|    total_timesteps      | 60416       |
| train/                  |             |
|    approx_kl            | 0.014303817 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | -2.01       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0299     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 1.86e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.9        |
|    ep_rew_mean          | 0.0174      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 119         |
|    time_elapsed         | 82          |
|    total_timesteps      | 60928       |
| train/                  |             |
|    approx_kl            | 0.008004883 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | -1.28       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0132     |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00876    |
|    value_loss           | 1.32e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.9         |
|    ep_rew_mean          | 0.0174       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 120          |
|    time_elapsed         | 83           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0094172545 |
|    clip_fraction        | 0.0938       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.51        |
|    explained_variance   | -2           |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0281      |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0093      |
|    value_loss           | 4.22e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0271      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 121         |
|    time_elapsed         | 84          |
|    total_timesteps      | 61952       |
| train/                  |             |
|    approx_kl            | 0.002420938 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | -0.449      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 0.000184    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0271      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 122         |
|    time_elapsed         | 84          |
|    total_timesteps      | 62464       |
| train/                  |             |
|    approx_kl            | 0.014535124 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0394     |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 0.00726     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.1         |
|    ep_rew_mean          | 0.0343       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 123          |
|    time_elapsed         | 85           |
|    total_timesteps      | 62976        |
| train/                  |              |
|    approx_kl            | 0.0042775115 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.45        |
|    explained_variance   | -0.622       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0141      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 0.000244     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 97.3       |
|    ep_rew_mean          | 0.0317     |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 124        |
|    time_elapsed         | 86         |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.00547928 |
|    clip_fraction        | 0.0445     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.259      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0378    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.00356   |
|    value_loss           | 0.00506    |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.9        |
|    ep_rew_mean          | 0.0232      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 125         |
|    time_elapsed         | 87          |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.011515312 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -2.99       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00127    |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.000203    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0311      |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 126         |
|    time_elapsed         | 87          |
|    total_timesteps      | 64512       |
| train/                  |             |
|    approx_kl            | 0.014096707 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | -4.68       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00232    |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 3.74e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97           |
|    ep_rew_mean          | 0.0311       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 127          |
|    time_elapsed         | 88           |
|    total_timesteps      | 65024        |
| train/                  |              |
|    approx_kl            | 0.0053456873 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0269      |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 0.00449      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97           |
|    ep_rew_mean          | 0.0311       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 128          |
|    time_elapsed         | 89           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0034020017 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | -0.675       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0333      |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00598     |
|    value_loss           | 0.000179     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0311      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 129         |
|    time_elapsed         | 89          |
|    total_timesteps      | 66048       |
| train/                  |             |
|    approx_kl            | 0.005878768 |
|    clip_fraction        | 0.0348      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | -3.6        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0609     |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00648    |
|    value_loss           | 0.000107    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0311      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 130         |
|    time_elapsed         | 90          |
|    total_timesteps      | 66560       |
| train/                  |             |
|    approx_kl            | 0.008982514 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | -0.868      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0101     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 2.97e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0311      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 131         |
|    time_elapsed         | 91          |
|    total_timesteps      | 67072       |
| train/                  |             |
|    approx_kl            | 0.001760342 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | -1.83       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0137     |
|    n_updates            | 1300        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 3.65e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0311      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 132         |
|    time_elapsed         | 91          |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.005666248 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | -14.2       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0325     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 4.12e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.3         |
|    ep_rew_mean          | 0.0268       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 133          |
|    time_elapsed         | 92           |
|    total_timesteps      | 68096        |
| train/                  |              |
|    approx_kl            | 0.0065163337 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | -3.63        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0319      |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00584     |
|    value_loss           | 1.92e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.3        |
|    ep_rew_mean          | 0.0268      |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 134         |
|    time_elapsed         | 93          |
|    total_timesteps      | 68608       |
| train/                  |             |
|    approx_kl            | 0.008943344 |
|    clip_fraction        | 0.0758      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -2.45       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0188     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 3.88e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.3        |
|    ep_rew_mean          | 0.0268      |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 135         |
|    time_elapsed         | 93          |
|    total_timesteps      | 69120       |
| train/                  |             |
|    approx_kl            | 0.005736244 |
|    clip_fraction        | 0.00879     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -4.23       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0135     |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 1.65e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.3         |
|    ep_rew_mean          | 0.0268       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 136          |
|    time_elapsed         | 94           |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0076531908 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | -2.88        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0265      |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00673     |
|    value_loss           | 0.000154     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.3         |
|    ep_rew_mean          | 0.0268       |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 137          |
|    time_elapsed         | 95           |
|    total_timesteps      | 70144        |
| train/                  |              |
|    approx_kl            | 0.0033211866 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | -7.94        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0212      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 9.09e-06     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.3        |
|    ep_rew_mean          | 0.0268      |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 138         |
|    time_elapsed         | 95          |
|    total_timesteps      | 70656       |
| train/                  |             |
|    approx_kl            | 0.010534627 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -1.42       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 0.000528    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.3         |
|    ep_rew_mean          | 0.0268       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 139          |
|    time_elapsed         | 96           |
|    total_timesteps      | 71168        |
| train/                  |              |
|    approx_kl            | 0.0140982345 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | -0.137       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00981     |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.0107      |
|    value_loss           | 0.00027      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0171       |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 140          |
|    time_elapsed         | 97           |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0032816133 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | -0.953       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0279      |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 0.000443     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.2         |
|    ep_rew_mean          | 0.0171       |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 141          |
|    time_elapsed         | 98           |
|    total_timesteps      | 72192        |
| train/                  |              |
|    approx_kl            | 0.0042924983 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | -2.77        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0296      |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00566     |
|    value_loss           | 4.57e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.2        |
|    ep_rew_mean          | 0.0171      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 142         |
|    time_elapsed         | 98          |
|    total_timesteps      | 72704       |
| train/                  |             |
|    approx_kl            | 0.011064883 |
|    clip_fraction        | 0.0561      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -1.08       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0209     |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 1.66e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 0.0143      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 143         |
|    time_elapsed         | 99          |
|    total_timesteps      | 73216       |
| train/                  |             |
|    approx_kl            | 0.012041348 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -1.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00361     |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 4.94e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.4        |
|    ep_rew_mean          | 0.0143      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 144         |
|    time_elapsed         | 100         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.016584817 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0227     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.00491     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.3        |
|    ep_rew_mean          | 0.00637     |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 145         |
|    time_elapsed         | 101         |
|    total_timesteps      | 74240       |
| train/                  |             |
|    approx_kl            | 0.005070041 |
|    clip_fraction        | 0.00234     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -2.52       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00371    |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00372    |
|    value_loss           | 0.000291    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.3         |
|    ep_rew_mean          | 0.00637      |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 146          |
|    time_elapsed         | 101          |
|    total_timesteps      | 74752        |
| train/                  |              |
|    approx_kl            | 0.0041368008 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | -8.64        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0338      |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00373     |
|    value_loss           | 3.57e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.3         |
|    ep_rew_mean          | 0.00637      |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 147          |
|    time_elapsed         | 102          |
|    total_timesteps      | 75264        |
| train/                  |              |
|    approx_kl            | 0.0063549704 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -1.93        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0342      |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 0.000126     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 99.3         |
|    ep_rew_mean          | 0.00637      |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 148          |
|    time_elapsed         | 103          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0065731835 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | -3.12        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0505      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00778     |
|    value_loss           | 3.4e-05      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 149         |
|    time_elapsed         | 103         |
|    total_timesteps      | 76288       |
| train/                  |             |
|    approx_kl            | 0.012988063 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.978      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.000134    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0117       |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 150          |
|    time_elapsed         | 104          |
|    total_timesteps      | 76800        |
| train/                  |              |
|    approx_kl            | 0.0072384737 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0267      |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00652     |
|    value_loss           | 0.00345      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0117       |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 151          |
|    time_elapsed         | 105          |
|    total_timesteps      | 77312        |
| train/                  |              |
|    approx_kl            | 0.0064612566 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | -1.37        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0488      |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00873     |
|    value_loss           | 0.000387     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 152         |
|    time_elapsed         | 105         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.015125949 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -2.13       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0764     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 3.76e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 153         |
|    time_elapsed         | 106         |
|    total_timesteps      | 78336       |
| train/                  |             |
|    approx_kl            | 0.008375665 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -4.67       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0401     |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 4.94e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 154         |
|    time_elapsed         | 107         |
|    total_timesteps      | 78848       |
| train/                  |             |
|    approx_kl            | 0.004626831 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -1.84       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0276     |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 1.89e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0117       |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 155          |
|    time_elapsed         | 108          |
|    total_timesteps      | 79360        |
| train/                  |              |
|    approx_kl            | 0.0036548078 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | -12.2        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0143      |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00511     |
|    value_loss           | 2.45e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 156         |
|    time_elapsed         | 108         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.010010123 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -6.54       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0302     |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 9.37e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0117       |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 157          |
|    time_elapsed         | 109          |
|    total_timesteps      | 80384        |
| train/                  |              |
|    approx_kl            | 0.0062046195 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | -3.28        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0173      |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 0.000184     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98.8         |
|    ep_rew_mean          | 0.0117       |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 158          |
|    time_elapsed         | 110          |
|    total_timesteps      | 80896        |
| train/                  |              |
|    approx_kl            | 0.0031021638 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.47        |
|    explained_variance   | -3.5         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.038       |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 1.15e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 0.0117      |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 159         |
|    time_elapsed         | 110         |
|    total_timesteps      | 81408       |
| train/                  |             |
|    approx_kl            | 0.016990637 |
|    clip_fraction        | 0.0756      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | -2.95       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00963    |
|    value_loss           | 4.48e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.9        |
|    ep_rew_mean          | 0.024       |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 160         |
|    time_elapsed         | 111         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.010121673 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -6.99       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0464     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 5.56e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.9        |
|    ep_rew_mean          | 0.024       |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 161         |
|    time_elapsed         | 112         |
|    total_timesteps      | 82432       |
| train/                  |             |
|    approx_kl            | 0.009823483 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.191       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0115     |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 0.00983     |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 98            |
|    ep_rew_mean          | 0.0267        |
| time/                   |               |
|    fps                  | 734           |
|    iterations           | 162           |
|    time_elapsed         | 112           |
|    total_timesteps      | 82944         |
| train/                  |               |
|    approx_kl            | 0.00043299806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.6          |
|    explained_variance   | -2.96         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0292       |
|    n_updates            | 1610          |
|    policy_gradient_loss | -0.000981     |
|    value_loss           | 0.000463      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0267      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 163         |
|    time_elapsed         | 113         |
|    total_timesteps      | 83456       |
| train/                  |             |
|    approx_kl            | 0.008196894 |
|    clip_fraction        | 0.00859     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0112     |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 0.00565     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | 0.0267       |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 164          |
|    time_elapsed         | 114          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0023762262 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | -2.27        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.024       |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00384     |
|    value_loss           | 0.000544     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 0.0267      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 165         |
|    time_elapsed         | 115         |
|    total_timesteps      | 84480       |
| train/                  |             |
|    approx_kl            | 0.009721711 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -0.219      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0411     |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00513    |
|    value_loss           | 0.000136    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.7        |
|    ep_rew_mean          | 0.0319      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 166         |
|    time_elapsed         | 115         |
|    total_timesteps      | 84992       |
| train/                  |             |
|    approx_kl            | 0.009612331 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -1.98       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0394     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 4.75e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.7        |
|    ep_rew_mean          | 0.0319      |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 167         |
|    time_elapsed         | 116         |
|    total_timesteps      | 85504       |
| train/                  |             |
|    approx_kl            | 0.010892438 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00985    |
|    value_loss           | 0.00302     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.1        |
|    ep_rew_mean          | 0.0265      |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 168         |
|    time_elapsed         | 117         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.014979723 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | -3.71       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0406     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.00109     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 98.1        |
|    ep_rew_mean          | 0.0265      |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 169         |
|    time_elapsed         | 117         |
|    total_timesteps      | 86528       |
| train/                  |             |
|    approx_kl            | 0.011889253 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -8.22       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0373     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00748    |
|    value_loss           | 0.000156    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.3        |
|    ep_rew_mean          | 0.0365      |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 170         |
|    time_elapsed         | 118         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.005795972 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | -6.33       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0264     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.000235    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 171         |
|    time_elapsed         | 119         |
|    total_timesteps      | 87552       |
| train/                  |             |
|    approx_kl            | 0.016147979 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.028      |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00608     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 172         |
|    time_elapsed         | 120         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.010461868 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.05       |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 0.00129     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.1         |
|    ep_rew_mean          | 0.0395       |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 173          |
|    time_elapsed         | 120          |
|    total_timesteps      | 88576        |
| train/                  |              |
|    approx_kl            | 0.0117107835 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | -4.32        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0131      |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00775     |
|    value_loss           | 0.000205     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 174         |
|    time_elapsed         | 121         |
|    total_timesteps      | 89088       |
| train/                  |             |
|    approx_kl            | 0.011341888 |
|    clip_fraction        | 0.0861      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | -0.662      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0525     |
|    n_updates            | 1730        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.000103    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 175         |
|    time_elapsed         | 122         |
|    total_timesteps      | 89600       |
| train/                  |             |
|    approx_kl            | 0.008955596 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -3.16       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0473     |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00414    |
|    value_loss           | 6.71e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.1         |
|    ep_rew_mean          | 0.0395       |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 176          |
|    time_elapsed         | 123          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 0.0072129024 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | -3.4         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0468      |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00696     |
|    value_loss           | 3.13e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 177         |
|    time_elapsed         | 123         |
|    total_timesteps      | 90624       |
| train/                  |             |
|    approx_kl            | 0.011534262 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | -3.4        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0298     |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.72e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97.1        |
|    ep_rew_mean          | 0.0395      |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 178         |
|    time_elapsed         | 124         |
|    total_timesteps      | 91136       |
| train/                  |             |
|    approx_kl            | 0.009915454 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | -7.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.01        |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 3.02e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 96.4         |
|    ep_rew_mean          | 0.0397       |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 179          |
|    time_elapsed         | 125          |
|    total_timesteps      | 91648        |
| train/                  |              |
|    approx_kl            | 0.0071616047 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | -2.32        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0337      |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00563     |
|    value_loss           | 1.36e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.4        |
|    ep_rew_mean          | 0.0397      |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 180         |
|    time_elapsed         | 125         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.016608704 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0281     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.00854     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 97          |
|    ep_rew_mean          | 0.0306      |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 181         |
|    time_elapsed         | 126         |
|    total_timesteps      | 92672       |
| train/                  |             |
|    approx_kl            | 0.010254574 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | -6.26       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00774     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 0.000272    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 96.5        |
|    ep_rew_mean          | 0.0343      |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 182         |
|    time_elapsed         | 127         |
|    total_timesteps      | 93184       |
| train/                  |             |
|    approx_kl            | 0.012804077 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | -5.71       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0229     |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 0.000261    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 95.9        |
|    ep_rew_mean          | 0.0391      |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 183         |
|    time_elapsed         | 128         |
|    total_timesteps      | 93696       |
| train/                  |             |
|    approx_kl            | 0.003186666 |
|    clip_fraction        | 0.0041      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0126     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 0.00142     |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 95.9       |
|    ep_rew_mean          | 0.0391     |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 184        |
|    time_elapsed         | 128        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.00591345 |
|    clip_fraction        | 0.0238     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.154      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0224    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.00374   |
|    value_loss           | 0.00245    |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.7         |
|    ep_rew_mean          | 0.0379       |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 185          |
|    time_elapsed         | 129          |
|    total_timesteps      | 94720        |
| train/                  |              |
|    approx_kl            | 0.0050486377 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | -3.15        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0216      |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 8.57e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 95.7         |
|    ep_rew_mean          | 0.0379       |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 186          |
|    time_elapsed         | 130          |
|    total_timesteps      | 95232        |
| train/                  |              |
|    approx_kl            | 0.0026203115 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.028       |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00472     |
|    value_loss           | 0.00205      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 94.2         |
|    ep_rew_mean          | 0.0492       |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 187          |
|    time_elapsed         | 130          |
|    total_timesteps      | 95744        |
| train/                  |              |
|    approx_kl            | 0.0047572977 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | -8.18        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0326      |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00646     |
|    value_loss           | 0.000428     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 94.7         |
|    ep_rew_mean          | 0.0421       |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 188          |
|    time_elapsed         | 131          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0057459148 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0139      |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.0036      |
|    value_loss           | 0.00321      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.3        |
|    ep_rew_mean          | 0.0676      |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 189         |
|    time_elapsed         | 132         |
|    total_timesteps      | 96768       |
| train/                  |             |
|    approx_kl            | 0.005261588 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0205     |
|    n_updates            | 1880        |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 0.000785    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 89.8        |
|    ep_rew_mean          | 0.0804      |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 190         |
|    time_elapsed         | 133         |
|    total_timesteps      | 97280       |
| train/                  |             |
|    approx_kl            | 0.006097436 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0149     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 0.00557     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 88.8         |
|    ep_rew_mean          | 0.0884       |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 191          |
|    time_elapsed         | 133          |
|    total_timesteps      | 97792        |
| train/                  |              |
|    approx_kl            | 0.0052104895 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.082        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00426     |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00665     |
|    value_loss           | 0.00578      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 88.8         |
|    ep_rew_mean          | 0.0884       |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 192          |
|    time_elapsed         | 134          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0032523829 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.481        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0152      |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 0.000937     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 88.8         |
|    ep_rew_mean          | 0.0884       |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 193          |
|    time_elapsed         | 135          |
|    total_timesteps      | 98816        |
| train/                  |              |
|    approx_kl            | 0.0062741903 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | -0.309       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00612     |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00426     |
|    value_loss           | 0.000699     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 88.8         |
|    ep_rew_mean          | 0.0884       |
| time/                   |              |
|    fps                  | 730          |
|    iterations           | 194          |
|    time_elapsed         | 136          |
|    total_timesteps      | 99328        |
| train/                  |              |
|    approx_kl            | 0.0064028967 |
|    clip_fraction        | 0.0855       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | -4.88        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.044       |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.000122     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 88          |
|    ep_rew_mean          | 0.0969      |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 195         |
|    time_elapsed         | 136         |
|    total_timesteps      | 99840       |
| train/                  |             |
|    approx_kl            | 0.013899761 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | -0.254      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0419     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 0.000244    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 88.7        |
|    ep_rew_mean          | 0.0937      |
| time/                   |             |
|    fps                  | 729         |
|    iterations           | 196         |
|    time_elapsed         | 137         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.014069239 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.00729     |
-----------------------------------------
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
wandb: updating run metadata
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json; uploading logs/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124_0/events.out.tfevents.1766028781.hungchan-Precision-7560.523201.0
wandb: uploading logs/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124_0/events.out.tfevents.1766028781.hungchan-Precision-7560.523201.0
wandb: uploading history steps 3876-4106, summary, console lines 6310-6479
wandb: 
wandb: Run history:
wandb: adaptive/adaptation_factor â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/base_clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           adaptive/base_lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        adaptive/clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/drift_magnitude â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          adaptive/ent_coef â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     adaptive/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             env/base_value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           env/reward_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                        +12 ...
wandb: 
wandb: Run summary:
wandb: adaptive/adaptation_factor 1
wandb:   adaptive/base_clip_range 0.2
wandb:           adaptive/base_lr 0.0003
wandb:        adaptive/clip_range 0.2
wandb:   adaptive/drift_magnitude 0
wandb:          adaptive/ent_coef 0.01
wandb:     adaptive/learning_rate 0.0003
wandb:             env/base_value 9.8
wandb:           env/reward_scale 9.8
wandb:                global_step 100352
wandb:                        +12 ...
wandb: 
wandb: ðŸš€ View run MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124 at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/ns0mdxkv
wandb: â­ï¸ View project at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: logs/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124/wandb/run-20251218_103259-ns0mdxkv/logs
>>> [DriftAdaptiveCallback] Training Ended
    Final LR: 0.000300
    Last Drift Magnitude: 0.0000
    Final Clip Range: 0.2000
Model saved locally to: models/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124.zip
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at /home/hungchan/Work/Deep-RL/videos/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: sine
Loading PPO model from: models/MiniGrid_FourRooms_reward_scale_sine_Adaptive_20251218_101124.zip

Recording Episode 1/1...
Traceback (most recent call last):
  File "/home/hungchan/Work/Deep-RL/scripts/render.py", line 179, in <module>
    main()
  File "/home/hungchan/Work/Deep-RL/scripts/render.py", line 168, in main
    record_video(
  File "/home/hungchan/Work/Deep-RL/scripts/render.py", line 145, in record_video
    print(f"  Step {step}: {param} = {data['current']:.2f} (Î” = {data['delta']:.2f})")
KeyError: 'delta'
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:434: UserWarning: [33mWARN: Unable to save last video! Did you call close()?[0m
  logger.warn("Unable to save last video! Did you call close()?")
