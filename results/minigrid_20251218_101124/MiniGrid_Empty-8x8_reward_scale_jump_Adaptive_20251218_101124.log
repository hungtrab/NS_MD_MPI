wandb: Currently logged in as: hungtrab (hungtrab-hanoi-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 87x9tpgt
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124/wandb/run-20251218_101347-87x9tpgt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124
wandb: â­ï¸ View project at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: ðŸš€ View run at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/87x9tpgt
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
--- Training Start: MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124 ---
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: jump
>>> Initializing PPO with kwargs: ['policy', 'env', 'learning_rate', 'gamma', 'verbose', 'tensorboard_log', 'n_steps', 'batch_size']
Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Logging to logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124_0
>>> [DriftAdaptiveCallback] Training Started
    Algorithm: PPO
    Target Param: reward_scale (base=9.8)
    Scale Factor: 0.15
    
    Adaptive Hyperparameters:
      - Learning Rate: 0.000300
      - Clip Range: 0.200 (adapt=True)
      - Entropy Coef: 0.0100 (adapt=True)
-----------------------------------
| adaptive/            |          |
|    adaptation_factor | 1        |
|    algorithm         | PPO      |
|    base_clip_range   | 0.2      |
|    base_lr           | 0.0003   |
|    clip_range        | 0.2      |
|    drift_magnitude   | 0        |
|    ent_coef          | 0.01     |
|    learning_rate     | 0.0003   |
| env/                 |          |
|    base_value        | 9.8      |
|    reward_scale      | 9.8      |
| rollout/             |          |
|    ep_len_mean       | 213      |
|    ep_rew_mean       | 0.201    |
| time/                |          |
|    fps               | 689      |
|    iterations        | 1        |
|    time_elapsed      | 0        |
|    total_timesteps   | 512      |
-----------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 230         |
|    ep_rew_mean          | 0.142       |
| time/                   |             |
|    fps                  | 663         |
|    iterations           | 2           |
|    time_elapsed         | 1           |
|    total_timesteps      | 1024        |
| train/                  |             |
|    approx_kl            | 0.002946488 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | -0.000227   |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0312     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.00225     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 238          |
|    ep_rew_mean          | 0.0949       |
| time/                   |              |
|    fps                  | 688          |
|    iterations           | 3            |
|    time_elapsed         | 2            |
|    total_timesteps      | 1536         |
| train/                  |              |
|    approx_kl            | 0.0006463191 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -0.222       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0178      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000699    |
|    value_loss           | 0.000336     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 243          |
|    ep_rew_mean          | 0.0711       |
| time/                   |              |
|    fps                  | 701          |
|    iterations           | 4            |
|    time_elapsed         | 2            |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0044340696 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -2.14        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0417      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00357     |
|    value_loss           | 1.31e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 246         |
|    ep_rew_mean          | 0.0569      |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 5           |
|    time_elapsed         | 3           |
|    total_timesteps      | 2560        |
| train/                  |             |
|    approx_kl            | 0.012109112 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | -2.46       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0272     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.36e-05    |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 247        |
|    ep_rew_mean          | 0.0474     |
| time/                   |            |
|    fps                  | 725        |
|    iterations           | 6          |
|    time_elapsed         | 4          |
|    total_timesteps      | 3072       |
| train/                  |            |
|    approx_kl            | 0.00866263 |
|    clip_fraction        | 0.0523     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.88      |
|    explained_variance   | -3.14      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0323    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0064    |
|    value_loss           | 1.13e-05   |
----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 248        |
|    ep_rew_mean          | 0.0407     |
| time/                   |            |
|    fps                  | 717        |
|    iterations           | 7          |
|    time_elapsed         | 4          |
|    total_timesteps      | 3584       |
| train/                  |            |
|    approx_kl            | 0.01545815 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | -4.68      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0246    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 1.26e-05   |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 249         |
|    ep_rew_mean          | 0.0356      |
| time/                   |             |
|    fps                  | 718         |
|    iterations           | 8           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012489079 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -2.35       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0352     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 8.68e-06    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 250         |
|    ep_rew_mean          | 0.0316      |
| time/                   |             |
|    fps                  | 725         |
|    iterations           | 9           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.012167564 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -1.65       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0189     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 5.97e-06    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 251         |
|    ep_rew_mean          | 0.0285      |
| time/                   |             |
|    fps                  | 725         |
|    iterations           | 10          |
|    time_elapsed         | 7           |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.008643026 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -4.55       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0389     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00687    |
|    value_loss           | 6.28e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 232          |
|    ep_rew_mean          | 0.104        |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 11           |
|    time_elapsed         | 7            |
|    total_timesteps      | 5632         |
| train/                  |              |
|    approx_kl            | 0.0052211415 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | -4.08        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0393      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00425     |
|    value_loss           | 6.73e-06     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 0.123      |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 12         |
|    time_elapsed         | 8          |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.00957175 |
|    clip_fraction        | 0.051      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.0128     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0202    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00974   |
|    value_loss           | 0.017      |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 221          |
|    ep_rew_mean          | 0.153        |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 13           |
|    time_elapsed         | 9            |
|    total_timesteps      | 6656         |
| train/                  |              |
|    approx_kl            | 0.0090692965 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | -0.00285     |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0299      |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00637     |
|    value_loss           | 0.00629      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 223          |
|    ep_rew_mean          | 0.143        |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 14           |
|    time_elapsed         | 9            |
|    total_timesteps      | 7168         |
| train/                  |              |
|    approx_kl            | 0.0024682079 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.0346       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0281      |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00154     |
|    value_loss           | 0.00906      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 225         |
|    ep_rew_mean          | 0.135       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 15          |
|    time_elapsed         | 10          |
|    total_timesteps      | 7680        |
| train/                  |             |
|    approx_kl            | 0.008103939 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | -3.19       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0608     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.000175    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 220          |
|    ep_rew_mean          | 0.16         |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 16           |
|    time_elapsed         | 11           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0070183654 |
|    clip_fraction        | 0.0898       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | -0.83        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0584      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 8.67e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | 0.151       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 17          |
|    time_elapsed         | 11          |
|    total_timesteps      | 8704        |
| train/                  |             |
|    approx_kl            | 0.014059408 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.0634      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0539     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.00807     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 223         |
|    ep_rew_mean          | 0.144       |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 18          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.007686492 |
|    clip_fraction        | 0.00547     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -2.07       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0375     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00321    |
|    value_loss           | 0.000198    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 225         |
|    ep_rew_mean          | 0.137       |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 19          |
|    time_elapsed         | 13          |
|    total_timesteps      | 9728        |
| train/                  |             |
|    approx_kl            | 0.013060092 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | -5.12       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0364     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 4.22e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 223         |
|    ep_rew_mean          | 0.143       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 20          |
|    time_elapsed         | 13          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.009463314 |
|    clip_fraction        | 0.00859     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | -5.41       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 5.92e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | 0.147       |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 21          |
|    time_elapsed         | 14          |
|    total_timesteps      | 10752       |
| train/                  |             |
|    approx_kl            | 0.008398067 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0215     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 0.00311     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 0.141       |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 22          |
|    time_elapsed         | 15          |
|    total_timesteps      | 11264       |
| train/                  |             |
|    approx_kl            | 0.011851511 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.0896      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.00478     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 0.141       |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 23          |
|    time_elapsed         | 15          |
|    total_timesteps      | 11776       |
| train/                  |             |
|    approx_kl            | 0.013122142 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.837      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0508     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.000269    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 214          |
|    ep_rew_mean          | 0.183        |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 24           |
|    time_elapsed         | 16           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0054401676 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00853     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00432     |
|    value_loss           | 0.000644     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 213         |
|    ep_rew_mean          | 0.186       |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 25          |
|    time_elapsed         | 17          |
|    total_timesteps      | 12800       |
| train/                  |             |
|    approx_kl            | 0.013364955 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0584     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0193      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 0.209       |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 26          |
|    time_elapsed         | 17          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.012989352 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.0592      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.00612     |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 0.212      |
| time/                   |            |
|    fps                  | 746        |
|    iterations           | 27         |
|    time_elapsed         | 18         |
|    total_timesteps      | 13824      |
| train/                  |            |
|    approx_kl            | 0.01400215 |
|    clip_fraction        | 0.0574     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | 0.152      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0365    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0065    |
|    value_loss           | 0.0136     |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 0.222       |
| time/                   |             |
|    fps                  | 747         |
|    iterations           | 28          |
|    time_elapsed         | 19          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010902532 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0269     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00501    |
|    value_loss           | 0.00312     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 203          |
|    ep_rew_mean          | 0.228        |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 29           |
|    time_elapsed         | 19           |
|    total_timesteps      | 14848        |
| train/                  |              |
|    approx_kl            | 0.0057359696 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.005        |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.00709      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 202          |
|    ep_rew_mean          | 0.233        |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 30           |
|    time_elapsed         | 20           |
|    total_timesteps      | 15360        |
| train/                  |              |
|    approx_kl            | 0.0029129214 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0313      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00461     |
|    value_loss           | 0.00641      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 0.231       |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 31          |
|    time_elapsed         | 21          |
|    total_timesteps      | 15872       |
| train/                  |             |
|    approx_kl            | 0.010157119 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00424    |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00923    |
|    value_loss           | 0.00421     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 0.238       |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 32          |
|    time_elapsed         | 21          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.015526488 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0174     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 0.000571    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 199         |
|    ep_rew_mean          | 0.243       |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 33          |
|    time_elapsed         | 22          |
|    total_timesteps      | 16896       |
| train/                  |             |
|    approx_kl            | 0.008789243 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.035      |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 0.00602     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 0.24        |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 34          |
|    time_elapsed         | 23          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.006266905 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0212     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 0.00514     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 0.238       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 35          |
|    time_elapsed         | 23          |
|    total_timesteps      | 17920       |
| train/                  |             |
|    approx_kl            | 0.018709429 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -0.104      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0254     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00994    |
|    value_loss           | 0.000855    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 198         |
|    ep_rew_mean          | 0.249       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 36          |
|    time_elapsed         | 24          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010077644 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0311     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.00196     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 0.254       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 37          |
|    time_elapsed         | 25          |
|    total_timesteps      | 18944       |
| train/                  |             |
|    approx_kl            | 0.017050589 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0246     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0101      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 0.267       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 38          |
|    time_elapsed         | 25          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.013733286 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0334     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.0028      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 194          |
|    ep_rew_mean          | 0.264        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 39           |
|    time_elapsed         | 26           |
|    total_timesteps      | 19968        |
| train/                  |              |
|    approx_kl            | 0.0058239475 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.5         |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.021        |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00477     |
|    value_loss           | 0.0107       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 193          |
|    ep_rew_mean          | 0.271        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 40           |
|    time_elapsed         | 27           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0063178316 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.43        |
|    explained_variance   | -8.54        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.022       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.0061      |
|    value_loss           | 0.00181      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 0.288       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 41          |
|    time_elapsed         | 27          |
|    total_timesteps      | 20992       |
| train/                  |             |
|    approx_kl            | 0.004645899 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0242     |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00556    |
|    value_loss           | 0.00294     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | 0.305        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 42           |
|    time_elapsed         | 28           |
|    total_timesteps      | 21504        |
| train/                  |              |
|    approx_kl            | 0.0024559335 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.45        |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.018       |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 0.00728      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 0.325       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 43          |
|    time_elapsed         | 29          |
|    total_timesteps      | 22016       |
| train/                  |             |
|    approx_kl            | 0.012492365 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0461     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 0.0075      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 170          |
|    ep_rew_mean          | 0.363        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 44           |
|    time_elapsed         | 29           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0043478017 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.481        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0053      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.00933      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 0.379       |
| time/                   |             |
|    fps                  | 751         |
|    iterations           | 45          |
|    time_elapsed         | 30          |
|    total_timesteps      | 23040       |
| train/                  |             |
|    approx_kl            | 0.005247043 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000109    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 0.0216      |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 165        |
|    ep_rew_mean          | 0.385      |
| time/                   |            |
|    fps                  | 753        |
|    iterations           | 46         |
|    time_elapsed         | 31         |
|    total_timesteps      | 23552      |
| train/                  |            |
|    approx_kl            | 0.00888513 |
|    clip_fraction        | 0.0287     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.462      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00615    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.00335   |
|    value_loss           | 0.0152     |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | 0.399        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 47           |
|    time_elapsed         | 31           |
|    total_timesteps      | 24064        |
| train/                  |              |
|    approx_kl            | 0.0058973534 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0208      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.0039      |
|    value_loss           | 0.00862      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | 0.418       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 48          |
|    time_elapsed         | 32          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016464986 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.0951      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0247     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | 0.445       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 49          |
|    time_elapsed         | 33          |
|    total_timesteps      | 25088       |
| train/                  |             |
|    approx_kl            | 0.009272438 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0494     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.00651     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | 0.482       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 50          |
|    time_elapsed         | 33          |
|    total_timesteps      | 25600       |
| train/                  |             |
|    approx_kl            | 0.007491846 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0219     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 0.011       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 0.486       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 51          |
|    time_elapsed         | 34          |
|    total_timesteps      | 26112       |
| train/                  |             |
|    approx_kl            | 0.008650806 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.012      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.0155      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 144          |
|    ep_rew_mean          | 0.47         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 52           |
|    time_elapsed         | 35           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0040743807 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00339     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 0.00556      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 0.488       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 53          |
|    time_elapsed         | 35          |
|    total_timesteps      | 27136       |
| train/                  |             |
|    approx_kl            | 0.007380353 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | -4.68       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0258     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 0.00362     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 0.498       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 54          |
|    time_elapsed         | 36          |
|    total_timesteps      | 27648       |
| train/                  |             |
|    approx_kl            | 0.012228668 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 0.0124      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 137          |
|    ep_rew_mean          | 0.501        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 55           |
|    time_elapsed         | 37           |
|    total_timesteps      | 28160        |
| train/                  |              |
|    approx_kl            | 0.0066261543 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0326      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 0.0142       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 0.499       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 56          |
|    time_elapsed         | 37          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.013465505 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0187     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.00429     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 131          |
|    ep_rew_mean          | 0.523        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 57           |
|    time_elapsed         | 38           |
|    total_timesteps      | 29184        |
| train/                  |              |
|    approx_kl            | 0.0065895286 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | -2.17        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0272      |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00856     |
|    value_loss           | 0.00141      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 116          |
|    ep_rew_mean          | 0.582        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 58           |
|    time_elapsed         | 39           |
|    total_timesteps      | 29696        |
| train/                  |              |
|    approx_kl            | 0.0054076896 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.03        |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00607     |
|    value_loss           | 0.0214       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 0.602       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 59          |
|    time_elapsed         | 39          |
|    total_timesteps      | 30208       |
| train/                  |             |
|    approx_kl            | 0.007282746 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.023       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 0.618       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 60          |
|    time_elapsed         | 40          |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.003302486 |
|    clip_fraction        | 0.00996     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00783     |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 0.0138      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.5        |
|    ep_rew_mean          | 0.642       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 61          |
|    time_elapsed         | 41          |
|    total_timesteps      | 31232       |
| train/                  |             |
|    approx_kl            | 0.005234087 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00638     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 99.3        |
|    ep_rew_mean          | 0.643       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 62          |
|    time_elapsed         | 42          |
|    total_timesteps      | 31744       |
| train/                  |             |
|    approx_kl            | 0.001364921 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0114     |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 0.0131      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 97.9         |
|    ep_rew_mean          | 0.649        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 63           |
|    time_elapsed         | 42           |
|    total_timesteps      | 32256        |
| train/                  |              |
|    approx_kl            | 0.0033146765 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | -0.249       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0202      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00596     |
|    value_loss           | 0.00385      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 103          |
|    ep_rew_mean          | 0.631        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 64           |
|    time_elapsed         | 43           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0010425852 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000287     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000352    |
|    value_loss           | 0.00647      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 0.635       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 65          |
|    time_elapsed         | 44          |
|    total_timesteps      | 33280       |
| train/                  |             |
|    approx_kl            | 0.004001888 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.0726      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00296    |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.0049     |
|    value_loss           | 0.00903     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 93.2         |
|    ep_rew_mean          | 0.667        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 66           |
|    time_elapsed         | 44           |
|    total_timesteps      | 33792        |
| train/                  |              |
|    approx_kl            | 0.0036525293 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000271    |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.0186       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 89.2         |
|    ep_rew_mean          | 0.681        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 67           |
|    time_elapsed         | 45           |
|    total_timesteps      | 34304        |
| train/                  |              |
|    approx_kl            | 0.0027789972 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.975       |
|    explained_variance   | 0.38         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00751      |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00516     |
|    value_loss           | 0.0208       |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 84.2       |
|    ep_rew_mean          | 0.7        |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 68         |
|    time_elapsed         | 46         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.00530066 |
|    clip_fraction        | 0.0158     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.029     |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.00325   |
|    value_loss           | 0.0195     |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 81.1        |
|    ep_rew_mean          | 0.711       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 69          |
|    time_elapsed         | 46          |
|    total_timesteps      | 35328       |
| train/                  |             |
|    approx_kl            | 0.005176589 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00178     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00542    |
|    value_loss           | 0.0171      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 73.2         |
|    ep_rew_mean          | 0.741        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 70           |
|    time_elapsed         | 47           |
|    total_timesteps      | 35840        |
| train/                  |              |
|    approx_kl            | 0.0036475947 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.244        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0111      |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00375     |
|    value_loss           | 0.013        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 70.2         |
|    ep_rew_mean          | 0.751        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 71           |
|    time_elapsed         | 48           |
|    total_timesteps      | 36352        |
| train/                  |              |
|    approx_kl            | 0.0076666055 |
|    clip_fraction        | 0.0539       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.44         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0304      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.0117      |
|    value_loss           | 0.0153       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 73.3        |
|    ep_rew_mean          | 0.74        |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 72          |
|    time_elapsed         | 48          |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009284446 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0223     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 0.0115      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 66.2         |
|    ep_rew_mean          | 0.765        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 73           |
|    time_elapsed         | 49           |
|    total_timesteps      | 37376        |
| train/                  |              |
|    approx_kl            | 0.0041598086 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.986       |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000624    |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.0143       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 61.8         |
|    ep_rew_mean          | 0.782        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 74           |
|    time_elapsed         | 50           |
|    total_timesteps      | 37888        |
| train/                  |              |
|    approx_kl            | 0.0027685561 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.98        |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00264     |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00428     |
|    value_loss           | 0.0155       |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 53.5       |
|    ep_rew_mean          | 0.812      |
| time/                   |            |
|    fps                  | 755        |
|    iterations           | 75         |
|    time_elapsed         | 50         |
|    total_timesteps      | 38400      |
| train/                  |            |
|    approx_kl            | 0.00422527 |
|    clip_fraction        | 0.0426     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.902     |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0105    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.00594   |
|    value_loss           | 0.0119     |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 47.8        |
|    ep_rew_mean          | 0.832       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 76          |
|    time_elapsed         | 51          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.004469637 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0165     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 0.0104      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 39.8         |
|    ep_rew_mean          | 0.86         |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 77           |
|    time_elapsed         | 52           |
|    total_timesteps      | 39424        |
| train/                  |              |
|    approx_kl            | 0.0019467106 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.852       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00635     |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 0.0124       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 37.3        |
|    ep_rew_mean          | 0.869       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 78          |
|    time_elapsed         | 52          |
|    total_timesteps      | 39936       |
| train/                  |             |
|    approx_kl            | 0.008491735 |
|    clip_fraction        | 0.0645      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0107      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 31.1         |
|    ep_rew_mean          | 0.891        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 79           |
|    time_elapsed         | 53           |
|    total_timesteps      | 40448        |
| train/                  |              |
|    approx_kl            | 0.0018508427 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.928       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.019       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00664     |
|    value_loss           | 0.0096       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 29.6         |
|    ep_rew_mean          | 0.896        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 80           |
|    time_elapsed         | 54           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0023825513 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.909       |
|    explained_variance   | 0.44         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0138      |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 0.00851      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 27.3         |
|    ep_rew_mean          | 0.904        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 81           |
|    time_elapsed         | 54           |
|    total_timesteps      | 41472        |
| train/                  |              |
|    approx_kl            | 0.0010084915 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.945       |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0122      |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00515     |
|    value_loss           | 0.00865      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 24.6        |
|    ep_rew_mean          | 0.913       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 82          |
|    time_elapsed         | 55          |
|    total_timesteps      | 41984       |
| train/                  |             |
|    approx_kl            | 0.002764496 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 810         |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 0.00543     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 0.922       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 83          |
|    time_elapsed         | 56          |
|    total_timesteps      | 42496       |
| train/                  |             |
|    approx_kl            | 0.005394364 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00557    |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00961    |
|    value_loss           | 0.005       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 21           |
|    ep_rew_mean          | 0.926        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 84           |
|    time_elapsed         | 56           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0052317576 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.645       |
|    explained_variance   | 0.453        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00591     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00815     |
|    value_loss           | 0.00451      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 19.4         |
|    ep_rew_mean          | 0.932        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 85           |
|    time_elapsed         | 57           |
|    total_timesteps      | 43520        |
| train/                  |              |
|    approx_kl            | 0.0024640062 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0074      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00618     |
|    value_loss           | 0.00754      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 0.935        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 86           |
|    time_elapsed         | 58           |
|    total_timesteps      | 44032        |
| train/                  |              |
|    approx_kl            | 0.0045371656 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.506       |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0489      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00801     |
|    value_loss           | 0.00321      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 0.947        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 87           |
|    time_elapsed         | 58           |
|    total_timesteps      | 44544        |
| train/                  |              |
|    approx_kl            | 0.0016841214 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0.385        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00731     |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00611     |
|    value_loss           | 0.00391      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 0.951        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 88           |
|    time_elapsed         | 59           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0018890428 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00596     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00611     |
|    value_loss           | 0.00335      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.9        |
|    ep_rew_mean          | 0.951       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 89          |
|    time_elapsed         | 60          |
|    total_timesteps      | 45568       |
| train/                  |             |
|    approx_kl            | 0.002855454 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0348     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.00142     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.5         |
|    ep_rew_mean          | 0.953        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 90           |
|    time_elapsed         | 60           |
|    total_timesteps      | 46080        |
| train/                  |              |
|    approx_kl            | 0.0040427805 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.295       |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0187      |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 0.00163      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 13.2          |
|    ep_rew_mean          | 0.954         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 91            |
|    time_elapsed         | 61            |
|    total_timesteps      | 46592         |
| train/                  |               |
|    approx_kl            | 0.00083111355 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.251        |
|    explained_variance   | 0.538         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00206      |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.0026       |
|    value_loss           | 0.00138       |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.2        |
|    ep_rew_mean          | 0.957       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 92          |
|    time_elapsed         | 62          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.002195076 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00165    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 0.000745    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.957        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 93           |
|    time_elapsed         | 63           |
|    total_timesteps      | 47616        |
| train/                  |              |
|    approx_kl            | 0.0072883368 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.803        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0217      |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.000193     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 0.957        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 94           |
|    time_elapsed         | 63           |
|    total_timesteps      | 48128        |
| train/                  |              |
|    approx_kl            | 0.0010404928 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.553        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000748    |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.0047      |
|    value_loss           | 0.000917     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 0.957        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 95           |
|    time_elapsed         | 64           |
|    total_timesteps      | 48640        |
| train/                  |              |
|    approx_kl            | 0.0032608914 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.519        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0272       |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00154     |
|    value_loss           | 0.00181      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 0.958        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 96           |
|    time_elapsed         | 65           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0007125689 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00574     |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 0.000639     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.958        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 97           |
|    time_elapsed         | 65           |
|    total_timesteps      | 49664        |
| train/                  |              |
|    approx_kl            | 0.0002590496 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.536        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000432    |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 0.00119      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 0.88          |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 98            |
|    time_elapsed         | 66            |
|    total_timesteps      | 50176         |
| train/                  |               |
|    approx_kl            | 0.00051705085 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0966       |
|    explained_variance   | 0.622         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00243      |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.00395      |
|    value_loss           | 0.00104       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 0.677         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 99            |
|    time_elapsed         | 67            |
|    total_timesteps      | 50688         |
| train/                  |               |
|    approx_kl            | 0.00079911435 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.105        |
|    explained_variance   | -0.0256       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0121        |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.00144      |
|    value_loss           | 0.0316        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 100          |
|    time_elapsed         | 67           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0033556554 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | -1.03        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00226     |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.00797      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 101          |
|    time_elapsed         | 68           |
|    total_timesteps      | 51712        |
| train/                  |              |
|    approx_kl            | 0.0008518114 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | -2.31        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0101      |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 0.000465     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 102          |
|    time_elapsed         | 69           |
|    total_timesteps      | 52224        |
| train/                  |              |
|    approx_kl            | 0.0009309626 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0108      |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00548     |
|    value_loss           | 6.03e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.479        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 103          |
|    time_elapsed         | 70           |
|    total_timesteps      | 52736        |
| train/                  |              |
|    approx_kl            | 0.0005653009 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.515        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0204      |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 0.000203     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.6        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 104         |
|    time_elapsed         | 70          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.003237988 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00529     |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 0.000158    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 105          |
|    time_elapsed         | 71           |
|    total_timesteps      | 53760        |
| train/                  |              |
|    approx_kl            | 0.0036318381 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0108      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.0163      |
|    value_loss           | 1.98e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 0.479       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 106         |
|    time_elapsed         | 72          |
|    total_timesteps      | 54272       |
| train/                  |             |
|    approx_kl            | 0.017727751 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0803     |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.021      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 1.72e-05    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 107           |
|    time_elapsed         | 72            |
|    total_timesteps      | 54784         |
| train/                  |               |
|    approx_kl            | 0.00029728096 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.067        |
|    explained_variance   | 0.573         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0136       |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00245      |
|    value_loss           | 0.000251      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 108          |
|    time_elapsed         | 73           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0017693437 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0458      |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00999     |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00646     |
|    value_loss           | 3.06e-06     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 0.479         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 109           |
|    time_elapsed         | 74            |
|    total_timesteps      | 55808         |
| train/                  |               |
|    approx_kl            | 0.00018715835 |
|    clip_fraction        | 0.00273       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0382       |
|    explained_variance   | 0.578         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000226     |
|    n_updates            | 1080          |
|    policy_gradient_loss | -0.00139      |
|    value_loss           | 0.000186      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 110          |
|    time_elapsed         | 74           |
|    total_timesteps      | 56320        |
| train/                  |              |
|    approx_kl            | 0.0037435552 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0387      |
|    explained_variance   | 0.633        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00204     |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00452     |
|    value_loss           | 0.000234     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.1        |
|    ep_rew_mean          | 0.481       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 111         |
|    time_elapsed         | 75          |
|    total_timesteps      | 56832       |
| train/                  |             |
|    approx_kl            | 0.006049654 |
|    clip_fraction        | 0.00879     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0327     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00414    |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00703    |
|    value_loss           | 3.91e-06    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 0.479         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 112           |
|    time_elapsed         | 75            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00026998646 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0313       |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000917     |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.00156      |
|    value_loss           | 1.53e-06      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 0.479         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 113           |
|    time_elapsed         | 76            |
|    total_timesteps      | 57856         |
| train/                  |               |
|    approx_kl            | 0.00046796666 |
|    clip_fraction        | 0.00703       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0356       |
|    explained_variance   | 0.565         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0117       |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.00516      |
|    value_loss           | 0.000375      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 114           |
|    time_elapsed         | 77            |
|    total_timesteps      | 58368         |
| train/                  |               |
|    approx_kl            | 0.00025766238 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0253       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00319      |
|    n_updates            | 1130          |
|    policy_gradient_loss | -0.00237      |
|    value_loss           | 6.34e-06      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 115         |
|    time_elapsed         | 78          |
|    total_timesteps      | 58880       |
| train/                  |             |
|    approx_kl            | 0.000157865 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0216     |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000141   |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 2.47e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 0.479        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 116          |
|    time_elapsed         | 78           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0011316495 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00028     |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 9.13e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | 0.479       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 117         |
|    time_elapsed         | 79          |
|    total_timesteps      | 59904       |
| train/                  |             |
|    approx_kl            | 0.003144933 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0227     |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 0.000552    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.479        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 118          |
|    time_elapsed         | 80           |
|    total_timesteps      | 60416        |
| train/                  |              |
|    approx_kl            | 0.0015969448 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0187      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00141     |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 4.78e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 119          |
|    time_elapsed         | 80           |
|    total_timesteps      | 60928        |
| train/                  |              |
|    approx_kl            | 0.0005746884 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0.569        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0295      |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 0.000199     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 120           |
|    time_elapsed         | 81            |
|    total_timesteps      | 61440         |
| train/                  |               |
|    approx_kl            | 0.00017222064 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000586     |
|    n_updates            | 1190          |
|    policy_gradient_loss | -0.00133      |
|    value_loss           | 1.51e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 121          |
|    time_elapsed         | 82           |
|    total_timesteps      | 61952        |
| train/                  |              |
|    approx_kl            | 1.641456e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0135      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000138    |
|    n_updates            | 1200         |
|    policy_gradient_loss | -8.17e-06    |
|    value_loss           | 5.26e-07     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 122           |
|    time_elapsed         | 82            |
|    total_timesteps      | 62464         |
| train/                  |               |
|    approx_kl            | 0.00016609405 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0148       |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000202     |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 5.77e-07      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 123         |
|    time_elapsed         | 83          |
|    total_timesteps      | 62976       |
| train/                  |             |
|    approx_kl            | 3.52622e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0128     |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000577   |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.000274   |
|    value_loss           | 1.4e-07     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 124          |
|    time_elapsed         | 84           |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0005403829 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0136      |
|    explained_variance   | 0.725        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000138     |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 0.000113     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 125          |
|    time_elapsed         | 84           |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 5.612825e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0127      |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0016      |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 1.18e-06     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 126           |
|    time_elapsed         | 85            |
|    total_timesteps      | 64512         |
| train/                  |               |
|    approx_kl            | 0.00029905036 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.63e-05     |
|    n_updates            | 1250          |
|    policy_gradient_loss | -0.00219      |
|    value_loss           | 5.36e-07      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 127          |
|    time_elapsed         | 86           |
|    total_timesteps      | 65024        |
| train/                  |              |
|    approx_kl            | 4.939502e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00967     |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000246    |
|    n_updates            | 1260         |
|    policy_gradient_loss | -5.86e-05    |
|    value_loss           | 1.05e-07     |
------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 11             |
|    ep_rew_mean          | 0.481          |
| time/                   |                |
|    fps                  | 754            |
|    iterations           | 128            |
|    time_elapsed         | 86             |
|    total_timesteps      | 65536          |
| train/                  |                |
|    approx_kl            | 1.15056755e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0114        |
|    explained_variance   | 1              |
|    learning_rate        | 0.0003         |
|    loss                 | -0.00102       |
|    n_updates            | 1270           |
|    policy_gradient_loss | -0.000219      |
|    value_loss           | 5.2e-08        |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 129          |
|    time_elapsed         | 87           |
|    total_timesteps      | 66048        |
| train/                  |              |
|    approx_kl            | 0.0005768887 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000203     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 3.36e-07     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 130          |
|    time_elapsed         | 88           |
|    total_timesteps      | 66560        |
| train/                  |              |
|    approx_kl            | 0.0016685867 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0101      |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.84e-05    |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00426     |
|    value_loss           | 6.35e-07     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 131          |
|    time_elapsed         | 89           |
|    total_timesteps      | 67072        |
| train/                  |              |
|    approx_kl            | 0.0001810611 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00939     |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.4e-05      |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 0.000114     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 132           |
|    time_elapsed         | 89            |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 2.7101487e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0088       |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000229     |
|    n_updates            | 1310          |
|    policy_gradient_loss | -0.000124     |
|    value_loss           | 7.81e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 133           |
|    time_elapsed         | 90            |
|    total_timesteps      | 68096         |
| train/                  |               |
|    approx_kl            | 2.3515895e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00829      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.64e-05     |
|    n_updates            | 1320          |
|    policy_gradient_loss | -3.3e-06      |
|    value_loss           | 2.15e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 134           |
|    time_elapsed         | 91            |
|    total_timesteps      | 68608         |
| train/                  |               |
|    approx_kl            | 5.8987644e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00817      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000302     |
|    n_updates            | 1330          |
|    policy_gradient_loss | -8.02e-05     |
|    value_loss           | 8.09e-08      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 135           |
|    time_elapsed         | 92            |
|    total_timesteps      | 69120         |
| train/                  |               |
|    approx_kl            | 0.00019990688 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00797      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.61e-05     |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.00228      |
|    value_loss           | 3.39e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 136           |
|    time_elapsed         | 92            |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 2.1968735e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00775      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000494     |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 8.9e-09       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 137           |
|    time_elapsed         | 93            |
|    total_timesteps      | 70144         |
| train/                  |               |
|    approx_kl            | 6.7498535e-05 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0078       |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.95e-05     |
|    n_updates            | 1360          |
|    policy_gradient_loss | -0.00213      |
|    value_loss           | 3.27e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 138           |
|    time_elapsed         | 94            |
|    total_timesteps      | 70656         |
| train/                  |               |
|    approx_kl            | 0.00019766914 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00721      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000103     |
|    n_updates            | 1370          |
|    policy_gradient_loss | -0.00259      |
|    value_loss           | 3.26e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 139           |
|    time_elapsed         | 94            |
|    total_timesteps      | 71168         |
| train/                  |               |
|    approx_kl            | 0.00029227324 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00707      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0235       |
|    n_updates            | 1380          |
|    policy_gradient_loss | -0.00234      |
|    value_loss           | 3.29e-07      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 140          |
|    time_elapsed         | 95           |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 9.255018e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00749     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000136    |
|    n_updates            | 1390         |
|    policy_gradient_loss | -2.67e-05    |
|    value_loss           | 3.97e-09     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 141           |
|    time_elapsed         | 96            |
|    total_timesteps      | 72192         |
| train/                  |               |
|    approx_kl            | 1.0313233e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00861      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000153     |
|    n_updates            | 1400          |
|    policy_gradient_loss | -2.72e-05     |
|    value_loss           | 8.35e-10      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 142           |
|    time_elapsed         | 96            |
|    total_timesteps      | 72704         |
| train/                  |               |
|    approx_kl            | 0.00029686466 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00825      |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.99e-05     |
|    n_updates            | 1410          |
|    policy_gradient_loss | -0.00192      |
|    value_loss           | 0.000115      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 143          |
|    time_elapsed         | 97           |
|    total_timesteps      | 73216        |
| train/                  |              |
|    approx_kl            | 0.0008133877 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00624     |
|    explained_variance   | 0.734        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000207     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 0.000122     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 144           |
|    time_elapsed         | 98            |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 0.00023651659 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00637      |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000147     |
|    n_updates            | 1430          |
|    policy_gradient_loss | -0.00116      |
|    value_loss           | 1.87e-06      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 145           |
|    time_elapsed         | 98            |
|    total_timesteps      | 74240         |
| train/                  |               |
|    approx_kl            | 1.7113052e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00541      |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.31e-05     |
|    n_updates            | 1440          |
|    policy_gradient_loss | -3.33e-05     |
|    value_loss           | 5.1e-07       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 146           |
|    time_elapsed         | 99            |
|    total_timesteps      | 74752         |
| train/                  |               |
|    approx_kl            | 5.8662146e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00823      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00138      |
|    n_updates            | 1450          |
|    policy_gradient_loss | -0.000444     |
|    value_loss           | 1.24e-07      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.1         |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 147          |
|    time_elapsed         | 100          |
|    total_timesteps      | 75264        |
| train/                  |              |
|    approx_kl            | 0.0044613145 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0114      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0287      |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 2.85e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.1         |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 148          |
|    time_elapsed         | 100          |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0024647042 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00517     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0151      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00518     |
|    value_loss           | 1.18e-06     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 149           |
|    time_elapsed         | 101           |
|    total_timesteps      | 76288         |
| train/                  |               |
|    approx_kl            | 2.3283064e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00433      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -6.9e-05      |
|    n_updates            | 1480          |
|    policy_gradient_loss | -1.32e-05     |
|    value_loss           | 1.83e-08      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 150          |
|    time_elapsed         | 102          |
|    total_timesteps      | 76800        |
| train/                  |              |
|    approx_kl            | 9.587966e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00458     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000355    |
|    n_updates            | 1490         |
|    policy_gradient_loss | -7.24e-05    |
|    value_loss           | 3.94e-09     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 151          |
|    time_elapsed         | 102          |
|    total_timesteps      | 77312        |
| train/                  |              |
|    approx_kl            | 0.0012555994 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00457     |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0003       |
|    loss                 | -7.07e-05    |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 6.36e-07     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 152          |
|    time_elapsed         | 103          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 4.164176e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00428     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000417    |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000179    |
|    value_loss           | 4.65e-09     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 153           |
|    time_elapsed         | 104           |
|    total_timesteps      | 78336         |
| train/                  |               |
|    approx_kl            | 2.1210872e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00418      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000211     |
|    n_updates            | 1520          |
|    policy_gradient_loss | -5.79e-05     |
|    value_loss           | 2.8e-10       |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 154         |
|    time_elapsed         | 104         |
|    total_timesteps      | 78848       |
| train/                  |             |
|    approx_kl            | 0.002081709 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00594    |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | -5.59e-05   |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 0.00012     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 155          |
|    time_elapsed         | 105          |
|    total_timesteps      | 79360        |
| train/                  |              |
|    approx_kl            | 0.0029427083 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00405     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00119     |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 3.16e-06     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 156           |
|    time_elapsed         | 106           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 1.2968667e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00342      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00013      |
|    n_updates            | 1550          |
|    policy_gradient_loss | -4.91e-05     |
|    value_loss           | 9.3e-08       |
-------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 13.6       |
|    ep_rew_mean          | 0.476      |
| time/                   |            |
|    fps                  | 752        |
|    iterations           | 157        |
|    time_elapsed         | 106        |
|    total_timesteps      | 80384      |
| train/                  |            |
|    approx_kl            | 0.12503065 |
|    clip_fraction        | 0.0195     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0162    |
|    explained_variance   | 1          |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0182    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.00483   |
|    value_loss           | 1.14e-08   |
----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 15         |
|    ep_rew_mean          | 0.474      |
| time/                   |            |
|    fps                  | 753        |
|    iterations           | 158        |
|    time_elapsed         | 107        |
|    total_timesteps      | 80896      |
| train/                  |            |
|    approx_kl            | 0.07226189 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0674    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.000705   |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | 0.472         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 159           |
|    time_elapsed         | 108           |
|    total_timesteps      | 81408         |
| train/                  |               |
|    approx_kl            | 0.00083279365 |
|    clip_fraction        | 0.0771        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.205        |
|    explained_variance   | 0.514         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0239       |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.0161       |
|    value_loss           | 0.000323      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | 0.478       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 160         |
|    time_elapsed         | 108         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.006293887 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0319     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.000105    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 0.479        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 161          |
|    time_elapsed         | 109          |
|    total_timesteps      | 82432        |
| train/                  |              |
|    approx_kl            | 0.0033054366 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0173      |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.0298      |
|    value_loss           | 4.57e-05     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 11.5       |
|    ep_rew_mean          | 0.48       |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 162        |
|    time_elapsed         | 109        |
|    total_timesteps      | 82944      |
| train/                  |            |
|    approx_kl            | 0.03228154 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0602    |
|    explained_variance   | 0.904      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0182    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 2.46e-05   |
----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 11.2       |
|    ep_rew_mean          | 0.48       |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 163        |
|    time_elapsed         | 110        |
|    total_timesteps      | 83456      |
| train/                  |            |
|    approx_kl            | 0.07609145 |
|    clip_fraction        | 0.0242     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0107    |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0243    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 6.04e-06   |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 164           |
|    time_elapsed         | 111           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 6.9849193e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00394      |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.67e-05     |
|    n_updates            | 1630          |
|    policy_gradient_loss | -5.33e-07     |
|    value_loss           | 1.26e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 165          |
|    time_elapsed         | 111          |
|    total_timesteps      | 84480        |
| train/                  |              |
|    approx_kl            | 6.193295e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00396     |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000155    |
|    n_updates            | 1640         |
|    policy_gradient_loss | -3.29e-05    |
|    value_loss           | 2.62e-07     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 166          |
|    time_elapsed         | 112          |
|    total_timesteps      | 84992        |
| train/                  |              |
|    approx_kl            | 4.780013e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0044      |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000253    |
|    n_updates            | 1650         |
|    policy_gradient_loss | -9.22e-05    |
|    value_loss           | 5.02e-08     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 167           |
|    time_elapsed         | 113           |
|    total_timesteps      | 85504         |
| train/                  |               |
|    approx_kl            | 4.2737694e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00855      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00203      |
|    n_updates            | 1660          |
|    policy_gradient_loss | -0.000493     |
|    value_loss           | 1.11e-08      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.5        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 168         |
|    time_elapsed         | 113         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.021855572 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0532     |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00448    |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00574    |
|    value_loss           | 2.92e-09    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 0.479       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 169         |
|    time_elapsed         | 114         |
|    total_timesteps      | 86528       |
| train/                  |             |
|    approx_kl            | 0.009375438 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0357     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 2.72e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.4        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 170         |
|    time_elapsed         | 115         |
|    total_timesteps      | 87040       |
| train/                  |             |
|    approx_kl            | 0.051110372 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0498     |
|    explained_variance   | 0.935       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0395     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 1.73e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.1        |
|    ep_rew_mean          | 0.481       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 171         |
|    time_elapsed         | 115         |
|    total_timesteps      | 87552       |
| train/                  |             |
|    approx_kl            | 0.013506408 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00767    |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0163     |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 1.76e-06    |
-----------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 11             |
|    ep_rew_mean          | 0.481          |
| time/                   |                |
|    fps                  | 756            |
|    iterations           | 172            |
|    time_elapsed         | 116            |
|    total_timesteps      | 88064          |
| train/                  |                |
|    approx_kl            | -1.5133992e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0051        |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.0003         |
|    loss                 | -6.96e-05      |
|    n_updates            | 1710           |
|    policy_gradient_loss | -1.24e-06      |
|    value_loss           | 2.34e-07       |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 173          |
|    time_elapsed         | 117          |
|    total_timesteps      | 88576        |
| train/                  |              |
|    approx_kl            | 2.438901e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00461     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000277    |
|    n_updates            | 1720         |
|    policy_gradient_loss | -9.03e-05    |
|    value_loss           | 2.79e-08     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 174           |
|    time_elapsed         | 117           |
|    total_timesteps      | 89088         |
| train/                  |               |
|    approx_kl            | 0.00051951676 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00422      |
|    explained_variance   | 0.602         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.49e-05      |
|    n_updates            | 1730          |
|    policy_gradient_loss | -0.00197      |
|    value_loss           | 0.000177      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 175          |
|    time_elapsed         | 118          |
|    total_timesteps      | 89600        |
| train/                  |              |
|    approx_kl            | 9.289943e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00413     |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000159    |
|    n_updates            | 1740         |
|    policy_gradient_loss | -3.06e-05    |
|    value_loss           | 2.32e-06     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 176          |
|    time_elapsed         | 119          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 1.747394e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00378     |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000185    |
|    n_updates            | 1750         |
|    policy_gradient_loss | -7.97e-05    |
|    value_loss           | 9.38e-08     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 177           |
|    time_elapsed         | 119           |
|    total_timesteps      | 90624         |
| train/                  |               |
|    approx_kl            | 3.7252903e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00412      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -4.93e-05     |
|    n_updates            | 1760          |
|    policy_gradient_loss | -3.35e-07     |
|    value_loss           | 1.18e-08      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 178           |
|    time_elapsed         | 120           |
|    total_timesteps      | 91136         |
| train/                  |               |
|    approx_kl            | 0.00021026295 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00458      |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000122     |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.00223      |
|    value_loss           | 3.31e-07      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 179         |
|    time_elapsed         | 121         |
|    total_timesteps      | 91648       |
| train/                  |             |
|    approx_kl            | 0.000421767 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00362    |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0003      |
|    loss                 | -4.98e-05   |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 2.94e-07    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 180          |
|    time_elapsed         | 121          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0005706062 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00352     |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.09e-05     |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 9.35e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 181           |
|    time_elapsed         | 122           |
|    total_timesteps      | 92672         |
| train/                  |               |
|    approx_kl            | 0.00028269086 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00318      |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.5e-05      |
|    n_updates            | 1800          |
|    policy_gradient_loss | -0.00135      |
|    value_loss           | 8.29e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 182           |
|    time_elapsed         | 123           |
|    total_timesteps      | 93184         |
| train/                  |               |
|    approx_kl            | 0.00051964016 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00338      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000124     |
|    n_updates            | 1810          |
|    policy_gradient_loss | -0.00231      |
|    value_loss           | 4.06e-07      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 183          |
|    time_elapsed         | 123          |
|    total_timesteps      | 93696        |
| train/                  |              |
|    approx_kl            | 7.473864e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00499     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -6.84e-05    |
|    n_updates            | 1820         |
|    policy_gradient_loss | -2.19e-06    |
|    value_loss           | 1.55e-08     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.4        |
|    ep_rew_mean          | 0.48        |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 184         |
|    time_elapsed         | 124         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.003953774 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0031     |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62e-05    |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 0.000284    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 185           |
|    time_elapsed         | 125           |
|    total_timesteps      | 94720         |
| train/                  |               |
|    approx_kl            | 0.00040406967 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00269      |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.94e-05     |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000647     |
|    value_loss           | 1.39e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 186          |
|    time_elapsed         | 125          |
|    total_timesteps      | 95232        |
| train/                  |              |
|    approx_kl            | 0.0005183215 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00379     |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000215    |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 1.06e-06     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 187           |
|    time_elapsed         | 126           |
|    total_timesteps      | 95744         |
| train/                  |               |
|    approx_kl            | 6.1350875e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00449      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00013      |
|    n_updates            | 1860          |
|    policy_gradient_loss | -9.22e-05     |
|    value_loss           | 1.51e-07      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 188           |
|    time_elapsed         | 127           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 1.2316741e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0026       |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000131     |
|    n_updates            | 1870          |
|    policy_gradient_loss | -7.57e-05     |
|    value_loss           | 1.82e-08      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 189           |
|    time_elapsed         | 127           |
|    total_timesteps      | 96768         |
| train/                  |               |
|    approx_kl            | 1.7532147e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00238      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000187     |
|    n_updates            | 1880          |
|    policy_gradient_loss | -8.26e-05     |
|    value_loss           | 2.05e-09      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 190          |
|    time_elapsed         | 128          |
|    total_timesteps      | 97280        |
| train/                  |              |
|    approx_kl            | 5.785143e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00429     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000711    |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.000162    |
|    value_loss           | 4.6e-10      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 191           |
|    time_elapsed         | 129           |
|    total_timesteps      | 97792         |
| train/                  |               |
|    approx_kl            | 0.00019264733 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00654      |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.5e-05      |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.00219      |
|    value_loss           | 3.37e-07      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.481        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 192          |
|    time_elapsed         | 130          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0022934484 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0027      |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000133    |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 3.48e-07     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 0.481       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 193         |
|    time_elapsed         | 130         |
|    total_timesteps      | 98816       |
| train/                  |             |
|    approx_kl            | 6.22822e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00231    |
|    explained_variance   | 1           |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000182   |
|    n_updates            | 1920        |
|    policy_gradient_loss | -6.79e-05   |
|    value_loss           | 1.84e-09    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 194           |
|    time_elapsed         | 131           |
|    total_timesteps      | 99328         |
| train/                  |               |
|    approx_kl            | 8.6962245e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00277      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -5.31e-05     |
|    n_updates            | 1930          |
|    policy_gradient_loss | -1.83e-05     |
|    value_loss           | 2.43e-10      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.481         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 195           |
|    time_elapsed         | 132           |
|    total_timesteps      | 99840         |
| train/                  |               |
|    approx_kl            | 7.6484866e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00304      |
|    explained_variance   | 1             |
|    learning_rate        | 0.0003        |
|    loss                 | -4.99e-05     |
|    n_updates            | 1940          |
|    policy_gradient_loss | -2.61e-05     |
|    value_loss           | 3.42e-11      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.634        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 196          |
|    time_elapsed         | 133          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 3.410969e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00232     |
|    explained_variance   | 1            |
|    learning_rate        | 0.0003       |
|    loss                 | -5.15e-05    |
|    n_updates            | 1950         |
|    policy_gradient_loss | -1.91e-05    |
|    value_loss           | 5.53e-12     |
------------------------------------------
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
wandb: updating run metadata
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124_0/events.out.tfevents.1766027629.hungchan-Precision-7560.501780.0 (+ 1 more)
wandb: uploading output.log; uploading wandb-summary.json; uploading logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124_0/events.out.tfevents.1766027629.hungchan-Precision-7560.501780.0
wandb: uploading logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124_0/events.out.tfevents.1766027629.hungchan-Precision-7560.501780.0
wandb: uploading data
wandb: 
wandb: Run history:
wandb: adaptive/adaptation_factor â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/base_clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           adaptive/base_lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        adaptive/clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/drift_magnitude â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          adaptive/ent_coef â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     adaptive/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             env/base_value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           env/reward_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                        +12 ...
wandb: 
wandb: Run summary:
wandb: adaptive/adaptation_factor 1
wandb:   adaptive/base_clip_range 0.2
wandb:           adaptive/base_lr 0.0003
wandb:        adaptive/clip_range 0.2
wandb:   adaptive/drift_magnitude 0
wandb:          adaptive/ent_coef 0.01
wandb:     adaptive/learning_rate 0.0003
wandb:             env/base_value 9.8
wandb:           env/reward_scale 9.8
wandb:                global_step 100352
wandb:                        +12 ...
wandb: 
wandb: ðŸš€ View run MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124 at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/87x9tpgt
wandb: â­ï¸ View project at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: logs/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124/wandb/run-20251218_101347-87x9tpgt/logs
>>> [DriftAdaptiveCallback] Training Ended
    Final LR: 0.000300
    Last Drift Magnitude: 0.0000
    Final Clip Range: 0.2000
Model saved locally to: models/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124.zip
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at /home/hungchan/Work/Deep-RL/videos/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: jump
Loading PPO model from: models/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124.zip

Recording Episode 1/1...
  Episode finished: 11 steps, reward = 1.0

Videos saved to: videos/MiniGrid_Empty-8x8_reward_scale_jump_Adaptive_20251218_101124
