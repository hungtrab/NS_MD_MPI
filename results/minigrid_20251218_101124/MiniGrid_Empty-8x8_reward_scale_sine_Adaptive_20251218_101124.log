wandb: Currently logged in as: hungtrab (hungtrab-hanoi-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run jxrbx6wp
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124/wandb/run-20251218_101831-jxrbx6wp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124
wandb: â­ï¸ View project at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: ðŸš€ View run at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/jxrbx6wp
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
--- Training Start: MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124 ---
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: sine
>>> Initializing PPO with kwargs: ['policy', 'env', 'learning_rate', 'gamma', 'verbose', 'tensorboard_log', 'n_steps', 'batch_size']
Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Logging to logs/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124_0
>>> [DriftAdaptiveCallback] Training Started
    Algorithm: PPO
    Target Param: reward_scale (base=9.8)
    Scale Factor: 0.15
    
    Adaptive Hyperparameters:
      - Learning Rate: 0.000300
      - Clip Range: 0.200 (adapt=True)
      - Entropy Coef: 0.0100 (adapt=True)
-----------------------------------
| adaptive/            |          |
|    adaptation_factor | 1        |
|    algorithm         | PPO      |
|    base_clip_range   | 0.2      |
|    base_lr           | 0.0003   |
|    clip_range        | 0.2      |
|    drift_magnitude   | 0        |
|    ent_coef          | 0.01     |
|    learning_rate     | 0.0003   |
| env/                 |          |
|    base_value        | 9.8      |
|    reward_scale      | 9.8      |
| rollout/             |          |
|    ep_len_mean       | 256      |
|    ep_rew_mean       | 0        |
| time/                |          |
|    fps               | 776      |
|    iterations        | 1        |
|    time_elapsed      | 0        |
|    total_timesteps   | 512      |
-----------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 256          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 727          |
|    iterations           | 2            |
|    time_elapsed         | 1            |
|    total_timesteps      | 1024         |
| train/                  |              |
|    approx_kl            | 0.0020406244 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -6.78        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00807     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 3.5e-05      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 256         |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 3           |
|    time_elapsed         | 2           |
|    total_timesteps      | 1536        |
| train/                  |             |
|    approx_kl            | 0.013229383 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -4.21       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0508     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.2e-05     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 256          |
|    ep_rew_mean          | 0            |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 4            |
|    time_elapsed         | 2            |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0039014977 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | -8.93        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.011       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 1.05e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 253         |
|    ep_rew_mean          | 0.0233      |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 5           |
|    time_elapsed         | 3           |
|    total_timesteps      | 2560        |
| train/                  |             |
|    approx_kl            | 0.008126441 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -5.94       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.03       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 7.78e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 242          |
|    ep_rew_mean          | 0.0812       |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 6            |
|    time_elapsed         | 4            |
|    total_timesteps      | 3072         |
| train/                  |              |
|    approx_kl            | 0.0064064898 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.0402       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0346      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00658     |
|    value_loss           | 0.000887     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 237         |
|    ep_rew_mean          | 0.109       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 7           |
|    time_elapsed         | 4           |
|    total_timesteps      | 3584        |
| train/                  |             |
|    approx_kl            | 0.008277445 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -0.00399    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0288     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0048     |
|    value_loss           | 0.00874     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 229         |
|    ep_rew_mean          | 0.146       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 8           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009938078 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | -0.0153     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0017      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00316    |
|    value_loss           | 0.00648     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 231         |
|    ep_rew_mean          | 0.145       |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 9           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.012245201 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.00246     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.046      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00723    |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 232         |
|    ep_rew_mean          | 0.147       |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 10          |
|    time_elapsed         | 6           |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.014264717 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.0577     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0503     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.000809    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 229         |
|    ep_rew_mean          | 0.161       |
| time/                   |             |
|    fps                  | 769         |
|    iterations           | 11          |
|    time_elapsed         | 7           |
|    total_timesteps      | 5632        |
| train/                  |             |
|    approx_kl            | 0.015054999 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.086       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00627    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.00124     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 227         |
|    ep_rew_mean          | 0.172       |
| time/                   |             |
|    fps                  | 770         |
|    iterations           | 12          |
|    time_elapsed         | 7           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.012089406 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.0211      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0218     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 0.00494     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 219         |
|    ep_rew_mean          | 0.211       |
| time/                   |             |
|    fps                  | 770         |
|    iterations           | 13          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6656        |
| train/                  |             |
|    approx_kl            | 0.006528884 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.0749      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0276     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 0.0046      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 210         |
|    ep_rew_mean          | 0.26        |
| time/                   |             |
|    fps                  | 768         |
|    iterations           | 14          |
|    time_elapsed         | 9           |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.014385525 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.0226      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000884   |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.0242      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 201          |
|    ep_rew_mean          | 0.304        |
| time/                   |              |
|    fps                  | 773          |
|    iterations           | 15           |
|    time_elapsed         | 9            |
|    total_timesteps      | 7680         |
| train/                  |              |
|    approx_kl            | 0.0046962644 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.39        |
|    explained_variance   | 0.0274       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0251      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00497     |
|    value_loss           | 0.0194       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 0.332       |
| time/                   |             |
|    fps                  | 772         |
|    iterations           | 16          |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015693307 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0304      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0152      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 0.384       |
| time/                   |             |
|    fps                  | 773         |
|    iterations           | 17          |
|    time_elapsed         | 11          |
|    total_timesteps      | 8704        |
| train/                  |             |
|    approx_kl            | 0.005363442 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0344      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00761     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00442    |
|    value_loss           | 0.017       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | 0.379        |
| time/                   |              |
|    fps                  | 774          |
|    iterations           | 18           |
|    time_elapsed         | 11           |
|    total_timesteps      | 9216         |
| train/                  |              |
|    approx_kl            | 0.0086174365 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.0375       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00733      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00392     |
|    value_loss           | 0.0284       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | 0.429       |
| time/                   |             |
|    fps                  | 776         |
|    iterations           | 19          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9728        |
| train/                  |             |
|    approx_kl            | 0.013969209 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | -0.373      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0029     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 0.00316     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | 0.443       |
| time/                   |             |
|    fps                  | 775         |
|    iterations           | 20          |
|    time_elapsed         | 13          |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011436408 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.0538      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00786     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.0254      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | 0.469        |
| time/                   |              |
|    fps                  | 770          |
|    iterations           | 21           |
|    time_elapsed         | 13           |
|    total_timesteps      | 10752        |
| train/                  |              |
|    approx_kl            | 0.0030813313 |
|    clip_fraction        | 0.0607       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.033        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0253      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00769     |
|    value_loss           | 0.0118       |
------------------------------------------
---------------------------------------
| adaptive/               |           |
|    adaptation_factor    | 1         |
|    algorithm            | PPO       |
|    base_clip_range      | 0.2       |
|    base_lr              | 0.0003    |
|    clip_range           | 0.2       |
|    drift_magnitude      | 0         |
|    ent_coef             | 0.01      |
|    learning_rate        | 0.0003    |
| env/                    |           |
|    base_value           | 9.8       |
|    reward_scale         | 9.8       |
| rollout/                |           |
|    ep_len_mean          | 154       |
|    ep_rew_mean          | 0.469     |
| time/                   |           |
|    fps                  | 766       |
|    iterations           | 22        |
|    time_elapsed         | 14        |
|    total_timesteps      | 11264     |
| train/                  |           |
|    approx_kl            | 0.0047225 |
|    clip_fraction        | 0.0463    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.134     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0216   |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.00489  |
|    value_loss           | 0.014     |
---------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 145          |
|    ep_rew_mean          | 0.488        |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 23           |
|    time_elapsed         | 15           |
|    total_timesteps      | 11776        |
| train/                  |              |
|    approx_kl            | 0.0056893886 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.951       |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0137      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0088      |
|    value_loss           | 0.00619      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 142          |
|    ep_rew_mean          | 0.493        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 24           |
|    time_elapsed         | 16           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0033508944 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0175      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.0134       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 142          |
|    ep_rew_mean          | 0.487        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 25           |
|    time_elapsed         | 16           |
|    total_timesteps      | 12800        |
| train/                  |              |
|    approx_kl            | 0.0042007193 |
|    clip_fraction        | 0.06         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0219      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00913     |
|    value_loss           | 0.00672      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | 0.484       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 26          |
|    time_elapsed         | 17          |
|    total_timesteps      | 13312       |
| train/                  |             |
|    approx_kl            | 0.005439615 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.0183      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0197     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0045     |
|    value_loss           | 0.00323     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 138          |
|    ep_rew_mean          | 0.485        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 27           |
|    time_elapsed         | 18           |
|    total_timesteps      | 13824        |
| train/                  |              |
|    approx_kl            | 0.0043887785 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.984       |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000408    |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00748     |
|    value_loss           | 0.00323      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 0.535       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 28          |
|    time_elapsed         | 18          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.005508232 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.17        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00362    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00858    |
|    value_loss           | 0.00866     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 0.56        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 29          |
|    time_elapsed         | 19          |
|    total_timesteps      | 14848       |
| train/                  |             |
|    approx_kl            | 0.004093503 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.179       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00677    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00638    |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 91.8        |
|    ep_rew_mean          | 0.592       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 30          |
|    time_elapsed         | 20          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.004415877 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0252     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 0.00897     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 81.8         |
|    ep_rew_mean          | 0.588        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 31           |
|    time_elapsed         | 20           |
|    total_timesteps      | 15872        |
| train/                  |              |
|    approx_kl            | 0.0015446825 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.762       |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00832      |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 0.00702      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 75.3         |
|    ep_rew_mean          | 0.581        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 32           |
|    time_elapsed         | 21           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0027557034 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.784       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00229     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00409     |
|    value_loss           | 0.00717      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 74.7         |
|    ep_rew_mean          | 0.573        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 33           |
|    time_elapsed         | 22           |
|    total_timesteps      | 16896        |
| train/                  |              |
|    approx_kl            | 0.0034356262 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.801       |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0367      |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00588     |
|    value_loss           | 0.00784      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 71.1        |
|    ep_rew_mean          | 0.566       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 34          |
|    time_elapsed         | 22          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.003453172 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.739      |
|    explained_variance   | -0.0511     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0098     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 0.00392     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 70.6         |
|    ep_rew_mean          | 0.561        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 35           |
|    time_elapsed         | 23           |
|    total_timesteps      | 17920        |
| train/                  |              |
|    approx_kl            | 0.0017528782 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.701       |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00595     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00258     |
|    value_loss           | 0.0102       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 70           |
|    ep_rew_mean          | 0.561        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 36           |
|    time_elapsed         | 24           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0035111946 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.704       |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0155      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.0041      |
|    value_loss           | 0.00587      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 69.7         |
|    ep_rew_mean          | 0.564        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 37           |
|    time_elapsed         | 25           |
|    total_timesteps      | 18944        |
| train/                  |              |
|    approx_kl            | 0.0014571872 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0.141        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00656     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 0.00612      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 66.5         |
|    ep_rew_mean          | 0.58         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 38           |
|    time_elapsed         | 25           |
|    total_timesteps      | 19456        |
| train/                  |              |
|    approx_kl            | 0.0014542155 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.67        |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0147       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00199     |
|    value_loss           | 0.0111       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 65.3        |
|    ep_rew_mean          | 0.588       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 39          |
|    time_elapsed         | 26          |
|    total_timesteps      | 19968       |
| train/                  |             |
|    approx_kl            | 0.003944775 |
|    clip_fraction        | 0.0619      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.191       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0119     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 0.0127      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 65.1         |
|    ep_rew_mean          | 0.607        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 40           |
|    time_elapsed         | 27           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0032501817 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.748       |
|    explained_variance   | 0.0458       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0175      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00728     |
|    value_loss           | 0.00977      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 62.3         |
|    ep_rew_mean          | 0.662        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 41           |
|    time_elapsed         | 27           |
|    total_timesteps      | 20992        |
| train/                  |              |
|    approx_kl            | 0.0011149081 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00602     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 0.0168       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 63.5         |
|    ep_rew_mean          | 0.68         |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 42           |
|    time_elapsed         | 28           |
|    total_timesteps      | 21504        |
| train/                  |              |
|    approx_kl            | 0.0031615589 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0117       |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00548     |
|    value_loss           | 0.0283       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 63.3         |
|    ep_rew_mean          | 0.723        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 43           |
|    time_elapsed         | 29           |
|    total_timesteps      | 22016        |
| train/                  |              |
|    approx_kl            | 0.0015181213 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0127      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.00972      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 62.8         |
|    ep_rew_mean          | 0.758        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 44           |
|    time_elapsed         | 29           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0014353509 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.477       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00487     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 0.0259       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 63.2         |
|    ep_rew_mean          | 0.784        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 45           |
|    time_elapsed         | 30           |
|    total_timesteps      | 23040        |
| train/                  |              |
|    approx_kl            | 0.0013183666 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.552       |
|    explained_variance   | 0.276        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.023       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 0.0136       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 62.7         |
|    ep_rew_mean          | 0.821        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 46           |
|    time_elapsed         | 31           |
|    total_timesteps      | 23552        |
| train/                  |              |
|    approx_kl            | 0.0012635374 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00521     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 0.0125       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 61.7         |
|    ep_rew_mean          | 0.859        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 47           |
|    time_elapsed         | 31           |
|    total_timesteps      | 24064        |
| train/                  |              |
|    approx_kl            | 0.0013048757 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00152      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00478     |
|    value_loss           | 0.025        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 60           |
|    ep_rew_mean          | 0.891        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 48           |
|    time_elapsed         | 32           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0008346514 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.576       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00109     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00458     |
|    value_loss           | 0.0221       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 54           |
|    ep_rew_mean          | 0.959        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 49           |
|    time_elapsed         | 32           |
|    total_timesteps      | 25088        |
| train/                  |              |
|    approx_kl            | 0.0011365009 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0024       |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 0.0242       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 52.1         |
|    ep_rew_mean          | 0.989        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 50           |
|    time_elapsed         | 33           |
|    total_timesteps      | 25600        |
| train/                  |              |
|    approx_kl            | 0.0011570916 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.462       |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00103      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00286     |
|    value_loss           | 0.0357       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 52.7         |
|    ep_rew_mean          | 1.01         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 51           |
|    time_elapsed         | 34           |
|    total_timesteps      | 26112        |
| train/                  |              |
|    approx_kl            | 0.0015956091 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.562       |
|    explained_variance   | 0.394        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0156       |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00338     |
|    value_loss           | 0.0243       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 50.6         |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 52           |
|    time_elapsed         | 35           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0019331416 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00373      |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00352     |
|    value_loss           | 0.0231       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 48.6         |
|    ep_rew_mean          | 1.06         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 53           |
|    time_elapsed         | 35           |
|    total_timesteps      | 27136        |
| train/                  |              |
|    approx_kl            | 0.0011427908 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00371      |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 0.0252       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 43.7         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 54           |
|    time_elapsed         | 36           |
|    total_timesteps      | 27648        |
| train/                  |              |
|    approx_kl            | 0.0032840092 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00614     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.0194       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 39.6         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 55           |
|    time_elapsed         | 37           |
|    total_timesteps      | 28160        |
| train/                  |              |
|    approx_kl            | 0.0032580537 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00755      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 0.0181       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 36.6        |
|    ep_rew_mean          | 1.07        |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 56          |
|    time_elapsed         | 38          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.002188408 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00345     |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 33.5        |
|    ep_rew_mean          | 1.05        |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 57          |
|    time_elapsed         | 38          |
|    total_timesteps      | 29184       |
| train/                  |             |
|    approx_kl            | 0.001243529 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00995    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 32.3        |
|    ep_rew_mean          | 1.03        |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 58          |
|    time_elapsed         | 39          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.002541345 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0107     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00477    |
|    value_loss           | 0.0105      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 28.9         |
|    ep_rew_mean          | 0.99         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 59           |
|    time_elapsed         | 40           |
|    total_timesteps      | 30208        |
| train/                  |              |
|    approx_kl            | 0.0011978019 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00292     |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00325     |
|    value_loss           | 0.0118       |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 25.8       |
|    ep_rew_mean          | 0.95       |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 60         |
|    time_elapsed         | 40         |
|    total_timesteps      | 30720      |
| train/                  |            |
|    approx_kl            | 0.00512442 |
|    clip_fraction        | 0.0855     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.673     |
|    explained_variance   | 0.353      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00569   |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 0.00799    |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 24.4         |
|    ep_rew_mean          | 0.9          |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 61           |
|    time_elapsed         | 41           |
|    total_timesteps      | 31232        |
| train/                  |              |
|    approx_kl            | 0.0039922893 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.724       |
|    explained_variance   | 0.407        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0256      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00739     |
|    value_loss           | 0.00527      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 19.1         |
|    ep_rew_mean          | 0.862        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 62           |
|    time_elapsed         | 42           |
|    total_timesteps      | 31744        |
| train/                  |              |
|    approx_kl            | 0.0023684185 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0236      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 0.00515      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 17.2         |
|    ep_rew_mean          | 0.813        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 63           |
|    time_elapsed         | 42           |
|    total_timesteps      | 32256        |
| train/                  |              |
|    approx_kl            | 0.0032693483 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.575       |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0182      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00653     |
|    value_loss           | 0.00346      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 0.778        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 64           |
|    time_elapsed         | 43           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0028247447 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.411       |
|    explained_variance   | -0.0492      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00887     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00331     |
|    value_loss           | 0.00279      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 14.5         |
|    ep_rew_mean          | 0.747        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 65           |
|    time_elapsed         | 44           |
|    total_timesteps      | 33280        |
| train/                  |              |
|    approx_kl            | 0.0023968206 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | -0.0933      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0272      |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00692     |
|    value_loss           | 0.00212      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 14.4          |
|    ep_rew_mean          | 0.718         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 66            |
|    time_elapsed         | 44            |
|    total_timesteps      | 33792         |
| train/                  |               |
|    approx_kl            | 0.00085653097 |
|    clip_fraction        | 0.018         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.311        |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0123       |
|    n_updates            | 650           |
|    policy_gradient_loss | -0.00429      |
|    value_loss           | 0.00181       |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 0.695       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 67          |
|    time_elapsed         | 45          |
|    total_timesteps      | 34304       |
| train/                  |             |
|    approx_kl            | 0.002479028 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | -0.0439     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0103     |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 0.00143     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 14.4         |
|    ep_rew_mean          | 0.678        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 68           |
|    time_elapsed         | 46           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0013927198 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.291       |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0158      |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.0063      |
|    value_loss           | 0.00295      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 0.67         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 69           |
|    time_elapsed         | 46           |
|    total_timesteps      | 35328        |
| train/                  |              |
|    approx_kl            | 0.0023047063 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0342      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00504     |
|    value_loss           | 0.000895     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 0.671        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 70           |
|    time_elapsed         | 47           |
|    total_timesteps      | 35840        |
| train/                  |              |
|    approx_kl            | 0.0012182195 |
|    clip_fraction        | 0.00898      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.267        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00898     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00584     |
|    value_loss           | 0.00111      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 0.679        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 71           |
|    time_elapsed         | 48           |
|    total_timesteps      | 36352        |
| train/                  |              |
|    approx_kl            | 0.0009492419 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.466        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00683     |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.006       |
|    value_loss           | 0.00132      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.691        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 72           |
|    time_elapsed         | 49           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 6.263098e-05 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.688        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00234     |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.000412     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 0.715        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 73           |
|    time_elapsed         | 49           |
|    total_timesteps      | 37376        |
| train/                  |              |
|    approx_kl            | 0.0014845062 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.581        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0123      |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00494     |
|    value_loss           | 0.00096      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.5        |
|    ep_rew_mean          | 0.741       |
| time/                   |             |
|    fps                  | 751         |
|    iterations           | 74          |
|    time_elapsed         | 50          |
|    total_timesteps      | 37888       |
| train/                  |             |
|    approx_kl            | 0.007213778 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.000208    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 0.772         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 75            |
|    time_elapsed         | 51            |
|    total_timesteps      | 38400         |
| train/                  |               |
|    approx_kl            | 0.00046276662 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0814       |
|    explained_variance   | 0.47          |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0147       |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.00421      |
|    value_loss           | 0.00148       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 0.811         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 76            |
|    time_elapsed         | 51            |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 0.00043077045 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0887       |
|    explained_variance   | 0.573         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0226       |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.00443      |
|    value_loss           | 0.00131       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 0.854        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 77           |
|    time_elapsed         | 52           |
|    total_timesteps      | 39424        |
| train/                  |              |
|    approx_kl            | 0.0022107312 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0789      |
|    explained_variance   | 0.637        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00152      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 0.00113      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.6          |
|    ep_rew_mean          | 0.902         |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 78            |
|    time_elapsed         | 53            |
|    total_timesteps      | 39936         |
| train/                  |               |
|    approx_kl            | 0.00041984604 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0824       |
|    explained_variance   | 0.532         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000929      |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.00361      |
|    value_loss           | 0.00165       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 0.945        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 79           |
|    time_elapsed         | 53           |
|    total_timesteps      | 40448        |
| train/                  |              |
|    approx_kl            | 0.0031354246 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0638      |
|    explained_variance   | 0.88         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00324     |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 0.000377     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.993        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 80           |
|    time_elapsed         | 54           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0014158431 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0542      |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000844     |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00546     |
|    value_loss           | 0.0026       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 81           |
|    time_elapsed         | 55           |
|    total_timesteps      | 41472        |
| train/                  |              |
|    approx_kl            | 0.0016886754 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0489      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00213     |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 0.000403     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.5          |
|    ep_rew_mean          | 1.08          |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 82            |
|    time_elapsed         | 55            |
|    total_timesteps      | 41984         |
| train/                  |               |
|    approx_kl            | 0.00018037495 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0448       |
|    explained_variance   | 0.625         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000172      |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.00216      |
|    value_loss           | 0.0012        |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 1.11          |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 83            |
|    time_elapsed         | 56            |
|    total_timesteps      | 42496         |
| train/                  |               |
|    approx_kl            | 0.00010351522 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0431       |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000336      |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.00171      |
|    value_loss           | 0.000921      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 1.15         |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 84           |
|    time_elapsed         | 57           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0053444505 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0488      |
|    explained_variance   | 0.503        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00628     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00507     |
|    value_loss           | 0.00329      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | 1.19        |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 85          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43520       |
| train/                  |             |
|    approx_kl            | 0.000738099 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0569     |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000142   |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 0.00288     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 1.21         |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 86           |
|    time_elapsed         | 58           |
|    total_timesteps      | 44032        |
| train/                  |              |
|    approx_kl            | 0.0135272285 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0129      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.000192     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.3         |
|    ep_rew_mean          | 1.23         |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 87           |
|    time_elapsed         | 59           |
|    total_timesteps      | 44544        |
| train/                  |              |
|    approx_kl            | 8.223101e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | 0.724        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0154      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 0.000927     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.7        |
|    ep_rew_mean          | 1.24        |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 88          |
|    time_elapsed         | 60          |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.001343806 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0271     |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00307    |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00254    |
|    value_loss           | 6.58e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.9        |
|    ep_rew_mean          | 1.24        |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 89          |
|    time_elapsed         | 61          |
|    total_timesteps      | 45568       |
| train/                  |             |
|    approx_kl            | 0.001826024 |
|    clip_fraction        | 0.00527     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0251     |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00183     |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00385    |
|    value_loss           | 0.00362     |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 90            |
|    time_elapsed         | 61            |
|    total_timesteps      | 46080         |
| train/                  |               |
|    approx_kl            | 0.00010534574 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0265       |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000353      |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.00165      |
|    value_loss           | 0.000873      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1.23          |
| time/                   |               |
|    fps                  | 744           |
|    iterations           | 91            |
|    time_elapsed         | 62            |
|    total_timesteps      | 46592         |
| train/                  |               |
|    approx_kl            | 0.00093105703 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0223       |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00455      |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.00159      |
|    value_loss           | 5.32e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 1.21         |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 92           |
|    time_elapsed         | 63           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0005034059 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0215      |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000334     |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 0.000756     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.19         |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 93           |
|    time_elapsed         | 64           |
|    total_timesteps      | 47616        |
| train/                  |              |
|    approx_kl            | 0.0003965015 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0206      |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000168    |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 7.75e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.16         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 94           |
|    time_elapsed         | 64           |
|    total_timesteps      | 48128        |
| train/                  |              |
|    approx_kl            | 0.0026917139 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0237      |
|    explained_variance   | 0.514        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000547     |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 0.0015       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 95           |
|    time_elapsed         | 65           |
|    total_timesteps      | 48640        |
| train/                  |              |
|    approx_kl            | 0.0025799847 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00461     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 0.000146     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 96           |
|    time_elapsed         | 66           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0073083737 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0249      |
|    explained_variance   | 0.495        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000105    |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 0.0013       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 97           |
|    time_elapsed         | 66           |
|    total_timesteps      | 49664        |
| train/                  |              |
|    approx_kl            | 0.0018686344 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | 0.814        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00117      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.000228     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.995         |
| time/                   |               |
|    fps                  | 743           |
|    iterations           | 98            |
|    time_elapsed         | 67            |
|    total_timesteps      | 50176         |
| train/                  |               |
|    approx_kl            | 2.1568034e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.787         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000155     |
|    n_updates            | 970           |
|    policy_gradient_loss | -3.78e-05     |
|    value_loss           | 0.000251      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.949         |
| time/                   |               |
|    fps                  | 743           |
|    iterations           | 99            |
|    time_elapsed         | 68            |
|    total_timesteps      | 50688         |
| train/                  |               |
|    approx_kl            | 0.00015566242 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0178       |
|    explained_variance   | 0.744         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.52e-05     |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.000229     |
|    value_loss           | 0.000266      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 0.904        |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 100          |
|    time_elapsed         | 68           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0013880777 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.018       |
|    explained_variance   | 0.613        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000532    |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.000517     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.859         |
| time/                   |               |
|    fps                  | 744           |
|    iterations           | 101           |
|    time_elapsed         | 69            |
|    total_timesteps      | 51712         |
| train/                  |               |
|    approx_kl            | 0.00021987909 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | 0.6           |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000102      |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 0.000526      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.817        |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 102          |
|    time_elapsed         | 70           |
|    total_timesteps      | 52224        |
| train/                  |              |
|    approx_kl            | 0.0006787089 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000443     |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000743    |
|    value_loss           | 0.00021      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.78          |
| time/                   |               |
|    fps                  | 745           |
|    iterations           | 103           |
|    time_elapsed         | 70            |
|    total_timesteps      | 52736         |
| train/                  |               |
|    approx_kl            | 1.5401747e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.739         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.5e-06      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -1.99e-06     |
|    value_loss           | 0.000199      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.746         |
| time/                   |               |
|    fps                  | 745           |
|    iterations           | 104           |
|    time_elapsed         | 71            |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00040889648 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00018       |
|    n_updates            | 1030          |
|    policy_gradient_loss | -0.000375     |
|    value_loss           | 0.000152      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.719         |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 105           |
|    time_elapsed         | 72            |
|    total_timesteps      | 53760         |
| train/                  |               |
|    approx_kl            | 0.00011633884 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.854         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000235     |
|    n_updates            | 1040          |
|    policy_gradient_loss | -0.000965     |
|    value_loss           | 0.000108      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.697        |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 106          |
|    time_elapsed         | 72           |
|    total_timesteps      | 54272        |
| train/                  |              |
|    approx_kl            | 0.0005353278 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0159      |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.0003       |
|    loss                 | -3.63e-05    |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000846    |
|    value_loss           | 7.18e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.683         |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 107           |
|    time_elapsed         | 73            |
|    total_timesteps      | 54784         |
| train/                  |               |
|    approx_kl            | 6.2715495e-05 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000145     |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.000445     |
|    value_loss           | 3.35e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.675         |
| time/                   |               |
|    fps                  | 745           |
|    iterations           | 108           |
|    time_elapsed         | 74            |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 0.00022461347 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000335     |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.00131      |
|    value_loss           | 1.32e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 0.675       |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 109         |
|    time_elapsed         | 74          |
|    total_timesteps      | 55808       |
| train/                  |             |
|    approx_kl            | 0.003108859 |
|    clip_fraction        | 0.00547     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0167     |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0145     |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 5.56e-06    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.683         |
| time/                   |               |
|    fps                  | 745           |
|    iterations           | 110           |
|    time_elapsed         | 75            |
|    total_timesteps      | 56320         |
| train/                  |               |
|    approx_kl            | 9.0846326e-05 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000219     |
|    n_updates            | 1090          |
|    policy_gradient_loss | -0.000819     |
|    value_loss           | 7.34e-06      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 0.693       |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 111         |
|    time_elapsed         | 76          |
|    total_timesteps      | 56832       |
| train/                  |             |
|    approx_kl            | 0.002410091 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0568     |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00319    |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 2.28e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.5        |
|    ep_rew_mean          | 0.704       |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 112         |
|    time_elapsed         | 76          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.002514442 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.071      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 0.00202     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.1        |
|    ep_rew_mean          | 0.736       |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 113         |
|    time_elapsed         | 77          |
|    total_timesteps      | 57856       |
| train/                  |             |
|    approx_kl            | 0.006619254 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0332     |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00897    |
|    value_loss           | 0.00242     |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 13.8          |
|    ep_rew_mean          | 0.765         |
| time/                   |               |
|    fps                  | 747           |
|    iterations           | 114           |
|    time_elapsed         | 78            |
|    total_timesteps      | 58368         |
| train/                  |               |
|    approx_kl            | 0.00096419244 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0178       |
|    explained_variance   | 0.688         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00133       |
|    n_updates            | 1130          |
|    policy_gradient_loss | -0.00219      |
|    value_loss           | 0.000778      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 0.811         |
| time/                   |               |
|    fps                  | 747           |
|    iterations           | 115           |
|    time_elapsed         | 78            |
|    total_timesteps      | 58880         |
| train/                  |               |
|    approx_kl            | 0.00051507785 |
|    clip_fraction        | 0.0102        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0152       |
|    explained_variance   | 0.573         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0099       |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.00611      |
|    value_loss           | 0.00165       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.859         |
| time/                   |               |
|    fps                  | 747           |
|    iterations           | 116           |
|    time_elapsed         | 79            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00042487693 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00253      |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000451     |
|    value_loss           | 0.000332      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 0.903       |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 117         |
|    time_elapsed         | 80          |
|    total_timesteps      | 59904       |
| train/                  |             |
|    approx_kl            | 6.07688e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00971    |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | -4.88e-05   |
|    n_updates            | 1160        |
|    policy_gradient_loss | -6.5e-06    |
|    value_loss           | 0.000239    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.95          |
| time/                   |               |
|    fps                  | 748           |
|    iterations           | 118           |
|    time_elapsed         | 80            |
|    total_timesteps      | 60416         |
| train/                  |               |
|    approx_kl            | 1.3317913e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00983      |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.0003        |
|    loss                 | 2.63e-06      |
|    n_updates            | 1170          |
|    policy_gradient_loss | -1.04e-05     |
|    value_loss           | 0.000229      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.995         |
| time/                   |               |
|    fps                  | 748           |
|    iterations           | 119           |
|    time_elapsed         | 81            |
|    total_timesteps      | 60928         |
| train/                  |               |
|    approx_kl            | 0.00019825064 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00935      |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00459      |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000507     |
|    value_loss           | 0.000238      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 120          |
|    time_elapsed         | 82           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 9.697396e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00947     |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.0003       |
|    loss                 | -3.78e-05    |
|    n_updates            | 1190         |
|    policy_gradient_loss | -1.82e-06    |
|    value_loss           | 0.00025      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 1.08        |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 121         |
|    time_elapsed         | 82          |
|    total_timesteps      | 61952       |
| train/                  |             |
|    approx_kl            | 0.000863652 |
|    clip_fraction        | 0.00371     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00913    |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000121    |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00094    |
|    value_loss           | 0.000225    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1.12          |
| time/                   |               |
|    fps                  | 748           |
|    iterations           | 122           |
|    time_elapsed         | 83            |
|    total_timesteps      | 62464         |
| train/                  |               |
|    approx_kl            | 0.00044996967 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.0003        |
|    loss                 | 3.33e-05      |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.000861     |
|    value_loss           | 0.000226      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 1.16         |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 123          |
|    time_elapsed         | 84           |
|    total_timesteps      | 62976        |
| train/                  |              |
|    approx_kl            | 0.0003907273 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0102      |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0155      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 0.000959     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.19         |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 124          |
|    time_elapsed         | 84           |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 9.080395e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00887     |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000191    |
|    n_updates            | 1230         |
|    policy_gradient_loss | -1.24e-05    |
|    value_loss           | 0.000151     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.21          |
| time/                   |               |
|    fps                  | 749           |
|    iterations           | 125           |
|    time_elapsed         | 85            |
|    total_timesteps      | 64000         |
| train/                  |               |
|    approx_kl            | 1.3853423e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00895      |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.45e-05     |
|    n_updates            | 1240          |
|    policy_gradient_loss | -1.17e-06     |
|    value_loss           | 8.14e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.23          |
| time/                   |               |
|    fps                  | 749           |
|    iterations           | 126           |
|    time_elapsed         | 86            |
|    total_timesteps      | 64512         |
| train/                  |               |
|    approx_kl            | 1.6845297e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00866      |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0003        |
|    loss                 | -1.47e-05     |
|    n_updates            | 1250          |
|    policy_gradient_loss | -1.81e-05     |
|    value_loss           | 4.99e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 749           |
|    iterations           | 127           |
|    time_elapsed         | 86            |
|    total_timesteps      | 65024         |
| train/                  |               |
|    approx_kl            | 0.00016227178 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00896      |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.36e-05     |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.00152      |
|    value_loss           | 2.38e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1.25          |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 128           |
|    time_elapsed         | 87            |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00033042696 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00749      |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.04e-05     |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.00173      |
|    value_loss           | 9.55e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 1.24         |
| time/                   |              |
|    fps                  | 750          |
|    iterations           | 129          |
|    time_elapsed         | 87           |
|    total_timesteps      | 66048        |
| train/                  |              |
|    approx_kl            | 0.0004904396 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00711     |
|    explained_variance   | 0.607        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000361     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 0.00111      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.23          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 130           |
|    time_elapsed         | 88            |
|    total_timesteps      | 66560         |
| train/                  |               |
|    approx_kl            | 0.00040772953 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00637      |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.0003        |
|    loss                 | -2.54e-06     |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 1.23e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.22          |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 131           |
|    time_elapsed         | 89            |
|    total_timesteps      | 67072         |
| train/                  |               |
|    approx_kl            | 0.00084102026 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00546      |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.09e-05     |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.00119      |
|    value_loss           | 3.49e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.19         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 132          |
|    time_elapsed         | 89           |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 6.204937e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00528     |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0003       |
|    loss                 | -8.82e-05    |
|    n_updates            | 1310         |
|    policy_gradient_loss | -4.01e-06    |
|    value_loss           | 6.36e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.16          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 133           |
|    time_elapsed         | 90            |
|    total_timesteps      | 68096         |
| train/                  |               |
|    approx_kl            | 2.7939677e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00578      |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.34e-05     |
|    n_updates            | 1320          |
|    policy_gradient_loss | 4.83e-08      |
|    value_loss           | 9.76e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 134          |
|    time_elapsed         | 91           |
|    total_timesteps      | 68608        |
| train/                  |              |
|    approx_kl            | 2.386514e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00622     |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000194    |
|    n_updates            | 1330         |
|    policy_gradient_loss | -3.59e-06    |
|    value_loss           | 0.000137     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 135          |
|    time_elapsed         | 91           |
|    total_timesteps      | 69120        |
| train/                  |              |
|    approx_kl            | 0.0008088059 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00534     |
|    explained_variance   | 0.878        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.41e-05    |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000826    |
|    value_loss           | 0.000164     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.04          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 136           |
|    time_elapsed         | 92            |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 2.9685907e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00491      |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.0003        |
|    loss                 | -1.89e-05     |
|    n_updates            | 1350          |
|    policy_gradient_loss | -3.04e-06     |
|    value_loss           | 0.000213      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.998         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 137           |
|    time_elapsed         | 93            |
|    total_timesteps      | 70144         |
| train/                  |               |
|    approx_kl            | 4.1909516e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00513      |
|    explained_variance   | 0.824         |
|    learning_rate        | 0.0003        |
|    loss                 | -1.28e-05     |
|    n_updates            | 1360          |
|    policy_gradient_loss | 2.7e-07       |
|    value_loss           | 0.00023       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.953         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 138           |
|    time_elapsed         | 93            |
|    total_timesteps      | 70656         |
| train/                  |               |
|    approx_kl            | 0.00048400136 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00508      |
|    explained_variance   | 0.793         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.13e-05     |
|    n_updates            | 1370          |
|    policy_gradient_loss | -0.000298     |
|    value_loss           | 0.000254      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.905        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 139          |
|    time_elapsed         | 94           |
|    total_timesteps      | 71168        |
| train/                  |              |
|    approx_kl            | 0.0017889729 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00601     |
|    explained_variance   | 0.63         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00017     |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 0.000543     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.862        |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 140          |
|    time_elapsed         | 95           |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 5.122274e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00654     |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.47e-06     |
|    n_updates            | 1390         |
|    policy_gradient_loss | -1.33e-06    |
|    value_loss           | 0.000217     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.82         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 141          |
|    time_elapsed         | 95           |
|    total_timesteps      | 72192        |
| train/                  |              |
|    approx_kl            | 6.680796e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00593     |
|    explained_variance   | 0.753        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000466    |
|    n_updates            | 1400         |
|    policy_gradient_loss | -4.7e-05     |
|    value_loss           | 0.000229     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.782         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 142           |
|    time_elapsed         | 96            |
|    total_timesteps      | 72704         |
| train/                  |               |
|    approx_kl            | 0.00039453595 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00447      |
|    explained_variance   | 0.789         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000101      |
|    n_updates            | 1410          |
|    policy_gradient_loss | -0.000459     |
|    value_loss           | 0.000179      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.748         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 143           |
|    time_elapsed         | 97            |
|    total_timesteps      | 73216         |
| train/                  |               |
|    approx_kl            | 2.6542693e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00439      |
|    explained_variance   | 0.814         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.29e-05     |
|    n_updates            | 1420          |
|    policy_gradient_loss | -1.31e-06     |
|    value_loss           | 0.000147      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 0.72        |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 144         |
|    time_elapsed         | 97          |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.001125675 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00475    |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00697    |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.000738   |
|    value_loss           | 0.000109    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.698         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 145           |
|    time_elapsed         | 98            |
|    total_timesteps      | 74240         |
| train/                  |               |
|    approx_kl            | 2.4330802e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00468      |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.88e-05     |
|    n_updates            | 1440          |
|    policy_gradient_loss | -3.72e-06     |
|    value_loss           | 7e-05         |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.683         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 146           |
|    time_elapsed         | 99            |
|    total_timesteps      | 74752         |
| train/                  |               |
|    approx_kl            | 3.3760443e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00448      |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.0003        |
|    loss                 | -3.42e-05     |
|    n_updates            | 1450          |
|    policy_gradient_loss | -1.82e-07     |
|    value_loss           | 3.78e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.676         |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 147           |
|    time_elapsed         | 99            |
|    total_timesteps      | 75264         |
| train/                  |               |
|    approx_kl            | 1.8056016e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00504      |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.0003        |
|    loss                 | -1.8e-05      |
|    n_updates            | 1460          |
|    policy_gradient_loss | -6.61e-06     |
|    value_loss           | 1.33e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.6        |
|    ep_rew_mean          | 0.671       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 148         |
|    time_elapsed         | 100         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.002391934 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0316     |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00549    |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 3.74e-06    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 14.4         |
|    ep_rew_mean          | 0.67         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 149          |
|    time_elapsed         | 100          |
|    total_timesteps      | 76288        |
| train/                  |              |
|    approx_kl            | 0.0036580567 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.06        |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0477      |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.00151      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 0.683        |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 150          |
|    time_elapsed         | 101          |
|    total_timesteps      | 76800        |
| train/                  |              |
|    approx_kl            | 0.0021976295 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0439      |
|    explained_variance   | 0.414        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000606    |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 0.00226      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 0.71         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 151          |
|    time_elapsed         | 102          |
|    total_timesteps      | 77312        |
| train/                  |              |
|    approx_kl            | 0.0024616243 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.51         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00116     |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00429     |
|    value_loss           | 0.00136      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.4        |
|    ep_rew_mean          | 0.742       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 152         |
|    time_elapsed         | 102         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.003509398 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00792    |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00307     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 0.00081     |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.777         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 153           |
|    time_elapsed         | 103           |
|    total_timesteps      | 78336         |
| train/                  |               |
|    approx_kl            | 1.2568198e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00583      |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.35e-05      |
|    n_updates            | 1520          |
|    policy_gradient_loss | -8.27e-05     |
|    value_loss           | 0.00017       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.815        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 154          |
|    time_elapsed         | 104          |
|    total_timesteps      | 78848        |
| train/                  |              |
|    approx_kl            | 9.080395e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00392     |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.0003       |
|    loss                 | -6.53e-05    |
|    n_updates            | 1530         |
|    policy_gradient_loss | -2.13e-05    |
|    value_loss           | 0.000163     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.857         |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 155           |
|    time_elapsed         | 104           |
|    total_timesteps      | 79360         |
| train/                  |               |
|    approx_kl            | 1.2223609e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00361      |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.0003        |
|    loss                 | 7.23e-06      |
|    n_updates            | 1540          |
|    policy_gradient_loss | -3.54e-06     |
|    value_loss           | 0.000182      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.9          |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 156          |
|    time_elapsed         | 105          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 4.782423e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00363     |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.37e-05     |
|    n_updates            | 1550         |
|    policy_gradient_loss | -8.86e-06    |
|    value_loss           | 0.000214     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.947         |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 157           |
|    time_elapsed         | 106           |
|    total_timesteps      | 80384         |
| train/                  |               |
|    approx_kl            | 3.0267984e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00374      |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.2e-07      |
|    n_updates            | 1560          |
|    policy_gradient_loss | -1.27e-07     |
|    value_loss           | 0.000234      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.992        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 158          |
|    time_elapsed         | 106          |
|    total_timesteps      | 80896        |
| train/                  |              |
|    approx_kl            | 5.820766e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00381     |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.91e-05     |
|    n_updates            | 1570         |
|    policy_gradient_loss | -9.03e-09    |
|    value_loss           | 0.000243     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 159          |
|    time_elapsed         | 107          |
|    total_timesteps      | 81408        |
| train/                  |              |
|    approx_kl            | 4.831236e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0042      |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | -3.76e-05    |
|    n_updates            | 1580         |
|    policy_gradient_loss | -4.06e-06    |
|    value_loss           | 0.000263     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.08          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 160           |
|    time_elapsed         | 108           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 3.9301813e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00516      |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.47e-05     |
|    n_updates            | 1590          |
|    policy_gradient_loss | -1.08e-05     |
|    value_loss           | 0.000239      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1.12          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 161           |
|    time_elapsed         | 108           |
|    total_timesteps      | 82432         |
| train/                  |               |
|    approx_kl            | 8.3187595e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000647      |
|    n_updates            | 1600          |
|    policy_gradient_loss | -0.000124     |
|    value_loss           | 0.000203      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 1.15         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 162          |
|    time_elapsed         | 109          |
|    total_timesteps      | 82944        |
| train/                  |              |
|    approx_kl            | 7.317134e-05 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0343      |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000607     |
|    n_updates            | 1610         |
|    policy_gradient_loss | 0.00644      |
|    value_loss           | 0.000927     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 1.18         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 163          |
|    time_elapsed         | 110          |
|    total_timesteps      | 83456        |
| train/                  |              |
|    approx_kl            | 0.0029059942 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0368      |
|    explained_variance   | 0.612        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00634     |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00879     |
|    value_loss           | 0.0031       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1.21          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 164           |
|    time_elapsed         | 110           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00053482584 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0191       |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000435      |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.00165      |
|    value_loss           | 0.00168       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 1.23         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 165          |
|    time_elapsed         | 111          |
|    total_timesteps      | 84480        |
| train/                  |              |
|    approx_kl            | 0.0002639644 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.731        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.26e-05    |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 0.00105      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 166           |
|    time_elapsed         | 112           |
|    total_timesteps      | 84992         |
| train/                  |               |
|    approx_kl            | 1.8976512e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00488      |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.004        |
|    n_updates            | 1650          |
|    policy_gradient_loss | -0.0016       |
|    value_loss           | 7.46e-05      |
-------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 11         |
|    ep_rew_mean          | 1.25       |
| time/                   |            |
|    fps                  | 758        |
|    iterations           | 167        |
|    time_elapsed         | 112        |
|    total_timesteps      | 85504      |
| train/                  |            |
|    approx_kl            | 2.8871e-08 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0027    |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -9.85e-05  |
|    n_updates            | 1660       |
|    policy_gradient_loss | -3.44e-05  |
|    value_loss           | 3.49e-05   |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.25          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 168           |
|    time_elapsed         | 113           |
|    total_timesteps      | 86016         |
| train/                  |               |
|    approx_kl            | 1.1525117e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00268      |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.53e-05     |
|    n_updates            | 1670          |
|    policy_gradient_loss | -1.26e-05     |
|    value_loss           | 2.11e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.23          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 169           |
|    time_elapsed         | 114           |
|    total_timesteps      | 86528         |
| train/                  |               |
|    approx_kl            | 1.5716068e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00292      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.99e-05     |
|    n_updates            | 1680          |
|    policy_gradient_loss | -7.17e-06     |
|    value_loss           | 2.35e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.22          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 170           |
|    time_elapsed         | 114           |
|    total_timesteps      | 87040         |
| train/                  |               |
|    approx_kl            | 2.3748726e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00317      |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.76e-05     |
|    n_updates            | 1690          |
|    policy_gradient_loss | -4.45e-06     |
|    value_loss           | 3.86e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.19          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 171           |
|    time_elapsed         | 115           |
|    total_timesteps      | 87552         |
| train/                  |               |
|    approx_kl            | 1.9557774e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0036       |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.41e-05     |
|    n_updates            | 1700          |
|    policy_gradient_loss | -1.72e-06     |
|    value_loss           | 7.18e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.16         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 172          |
|    time_elapsed         | 115          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 7.566996e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00401     |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0003       |
|    loss                 | -2.9e-05     |
|    n_updates            | 1710         |
|    policy_gradient_loss | -4.6e-06     |
|    value_loss           | 0.000105     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1.13          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 173           |
|    time_elapsed         | 116           |
|    total_timesteps      | 88576         |
| train/                  |               |
|    approx_kl            | 2.4680048e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00472      |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0003        |
|    loss                 | 5.13e-06      |
|    n_updates            | 1720          |
|    policy_gradient_loss | 8.31e-07      |
|    value_loss           | 0.000137      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 174          |
|    time_elapsed         | 117          |
|    total_timesteps      | 89088        |
| train/                  |              |
|    approx_kl            | 4.931353e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0055      |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0003       |
|    loss                 | -3.98e-05    |
|    n_updates            | 1730         |
|    policy_gradient_loss | -9.14e-06    |
|    value_loss           | 0.000178     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 1.05        |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 175         |
|    time_elapsed         | 117         |
|    total_timesteps      | 89600       |
| train/                  |             |
|    approx_kl            | 1.44355e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00684    |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.0003      |
|    loss                 | -2.96e-05   |
|    n_updates            | 1740        |
|    policy_gradient_loss | 4.25e-06    |
|    value_loss           | 0.000213    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 1             |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 176           |
|    time_elapsed         | 118           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 1.5423866e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00777      |
|    explained_variance   | 0.807         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000429     |
|    n_updates            | 1750          |
|    policy_gradient_loss | -7.96e-06     |
|    value_loss           | 0.000244      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.1         |
|    ep_rew_mean          | 0.955        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 177          |
|    time_elapsed         | 119          |
|    total_timesteps      | 90624        |
| train/                  |              |
|    approx_kl            | 0.0028702214 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00177     |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.000718    |
|    value_loss           | 0.000263     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.1         |
|    ep_rew_mean          | 0.909        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 178          |
|    time_elapsed         | 119          |
|    total_timesteps      | 91136        |
| train/                  |              |
|    approx_kl            | 0.0052535133 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00834     |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00115     |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.000879    |
|    value_loss           | 0.000269     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.864        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 179          |
|    time_elapsed         | 120          |
|    total_timesteps      | 91648        |
| train/                  |              |
|    approx_kl            | 0.0011992437 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00352     |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00479     |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.000594    |
|    value_loss           | 0.000236     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.823         |
| time/                   |               |
|    fps                  | 761           |
|    iterations           | 180           |
|    time_elapsed         | 121           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 2.2817403e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00318      |
|    explained_variance   | 0.778         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.09e-05     |
|    n_updates            | 1790          |
|    policy_gradient_loss | -2.26e-06     |
|    value_loss           | 0.000201      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.784        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 181          |
|    time_elapsed         | 121          |
|    total_timesteps      | 92672        |
| train/                  |              |
|    approx_kl            | 9.778887e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00355     |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.7e-07     |
|    n_updates            | 1800         |
|    policy_gradient_loss | 5.54e-07     |
|    value_loss           | 0.000183     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.75         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 182          |
|    time_elapsed         | 122          |
|    total_timesteps      | 93184        |
| train/                  |              |
|    approx_kl            | 1.344597e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00392     |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.0003       |
|    loss                 | 3.54e-05     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -6.2e-06     |
|    value_loss           | 0.000143     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.722         |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 183           |
|    time_elapsed         | 123           |
|    total_timesteps      | 93696         |
| train/                  |               |
|    approx_kl            | 4.5867637e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00484      |
|    explained_variance   | 0.857         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.88e-05     |
|    n_updates            | 1820          |
|    policy_gradient_loss | 2.13e-06      |
|    value_loss           | 0.000108      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11            |
|    ep_rew_mean          | 0.7           |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 184           |
|    time_elapsed         | 123           |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 5.6086574e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00657      |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000291     |
|    n_updates            | 1830          |
|    policy_gradient_loss | -3.66e-05     |
|    value_loss           | 6.92e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11          |
|    ep_rew_mean          | 0.684       |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 185         |
|    time_elapsed         | 124         |
|    total_timesteps      | 94720       |
| train/                  |             |
|    approx_kl            | 0.006394451 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00415    |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00612    |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.000708   |
|    value_loss           | 3.93e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11           |
|    ep_rew_mean          | 0.676        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 186          |
|    time_elapsed         | 125          |
|    total_timesteps      | 95232        |
| train/                  |              |
|    approx_kl            | 0.0006396421 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00249     |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.32e-05    |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 1.85e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.2          |
|    ep_rew_mean          | 0.674         |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 187           |
|    time_elapsed         | 126           |
|    total_timesteps      | 95744         |
| train/                  |               |
|    approx_kl            | 1.7819111e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00437      |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00298      |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.000275     |
|    value_loss           | 4.16e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.679        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 188          |
|    time_elapsed         | 126          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0010956711 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0326      |
|    explained_variance   | 0.718        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000127     |
|    n_updates            | 1870         |
|    policy_gradient_loss | 0.0193       |
|    value_loss           | 0.000229     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 0.692        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 189          |
|    time_elapsed         | 127          |
|    total_timesteps      | 96768        |
| train/                  |              |
|    approx_kl            | 0.0026252596 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.562        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00976     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00242     |
|    value_loss           | 0.000603     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.4         |
|    ep_rew_mean          | 0.714        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 190          |
|    time_elapsed         | 128          |
|    total_timesteps      | 97280        |
| train/                  |              |
|    approx_kl            | 0.0005922414 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.0003       |
|    loss                 | -1.01e-05    |
|    n_updates            | 1890         |
|    policy_gradient_loss | 0.00444      |
|    value_loss           | 0.000303     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.739        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 191          |
|    time_elapsed         | 128          |
|    total_timesteps      | 97792        |
| train/                  |              |
|    approx_kl            | 0.0005814332 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00828     |
|    explained_variance   | 0.616        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0151      |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.000574     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.2         |
|    ep_rew_mean          | 0.774        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 192          |
|    time_elapsed         | 129          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0022581243 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00538     |
|    explained_variance   | 0.769        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.88e-05     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.000414     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.2        |
|    ep_rew_mean          | 0.812       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 193         |
|    time_elapsed         | 130         |
|    total_timesteps      | 98816       |
| train/                  |             |
|    approx_kl            | 9.96748e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00415    |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.0003      |
|    loss                 | -3.97e-06   |
|    n_updates            | 1920        |
|    policy_gradient_loss | -3.53e-05   |
|    value_loss           | 0.000169    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.5         |
|    ep_rew_mean          | 0.827        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 194          |
|    time_elapsed         | 131          |
|    total_timesteps      | 99328        |
| train/                  |              |
|    approx_kl            | 0.0026938904 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0246      |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00435     |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 0.000199     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.7        |
|    ep_rew_mean          | 0.879       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 195         |
|    time_elapsed         | 131         |
|    total_timesteps      | 99840       |
| train/                  |             |
|    approx_kl            | 0.021103099 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0632     |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0111     |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.00392     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.1        |
|    ep_rew_mean          | 0.939       |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 196         |
|    time_elapsed         | 132         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.010974935 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00668    |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00236    |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 0.00109     |
-----------------------------------------
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
wandb: updating run metadata
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json
wandb: uploading model.zip; uploading output.log; uploading logs/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124_0/events.out.tfevents.1766027913.hungchan-Precision-7560.507011.0
wandb: uploading logs/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124_0/events.out.tfevents.1766027913.hungchan-Precision-7560.507011.0
wandb: uploading history steps 3561-4106, summary, console lines 5848-6479
wandb: uploading data
wandb: 
wandb: Run history:
wandb: adaptive/adaptation_factor â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/base_clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           adaptive/base_lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        adaptive/clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/drift_magnitude â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          adaptive/ent_coef â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     adaptive/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             env/base_value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           env/reward_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                global_step â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                        +12 ...
wandb: 
wandb: Run summary:
wandb: adaptive/adaptation_factor 1
wandb:   adaptive/base_clip_range 0.2
wandb:           adaptive/base_lr 0.0003
wandb:        adaptive/clip_range 0.2
wandb:   adaptive/drift_magnitude 0
wandb:          adaptive/ent_coef 0.01
wandb:     adaptive/learning_rate 0.0003
wandb:             env/base_value 9.8
wandb:           env/reward_scale 9.8
wandb:                global_step 100352
wandb:                        +12 ...
wandb: 
wandb: ðŸš€ View run MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124 at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/jxrbx6wp
wandb: â­ï¸ View project at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: logs/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124/wandb/run-20251218_101831-jxrbx6wp/logs
>>> [DriftAdaptiveCallback] Training Ended
    Final LR: 0.000300
    Last Drift Magnitude: 0.0000
    Final Clip Range: 0.2000
Model saved locally to: models/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124.zip
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at /home/hungchan/Work/Deep-RL/videos/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: sine
Loading PPO model from: models/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124.zip

Recording Episode 1/1...
  Episode finished: 11 steps, reward = 1.0

Videos saved to: videos/MiniGrid_Empty-8x8_reward_scale_sine_Adaptive_20251218_101124
