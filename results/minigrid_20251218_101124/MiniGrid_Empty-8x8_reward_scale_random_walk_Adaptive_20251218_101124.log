wandb: Currently logged in as: hungtrab (hungtrab-hanoi-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run ckhe5oz3
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124/wandb/run-20251218_102801-ckhe5oz3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124
wandb: â­ï¸ View project at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: ðŸš€ View run at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/ckhe5oz3
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
--- Training Start: MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124 ---
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: random_walk
>>> Initializing PPO with kwargs: ['policy', 'env', 'learning_rate', 'gamma', 'verbose', 'tensorboard_log', 'n_steps', 'batch_size']
Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Logging to logs/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124_0
>>> [DriftAdaptiveCallback] Training Started
    Algorithm: PPO
    Target Param: reward_scale (base=9.8)
    Scale Factor: 0.15
    
    Adaptive Hyperparameters:
      - Learning Rate: 0.000300
      - Clip Range: 0.200 (adapt=True)
      - Entropy Coef: 0.0100 (adapt=True)
-----------------------------------
| adaptive/            |          |
|    adaptation_factor | 1        |
|    algorithm         | PPO      |
|    base_clip_range   | 0.2      |
|    base_lr           | 0.0003   |
|    clip_range        | 0.2      |
|    drift_magnitude   | 0        |
|    ent_coef          | 0.01     |
|    learning_rate     | 0.0003   |
| env/                 |          |
|    base_value        | 9.8      |
|    reward_scale      | 9.8      |
| rollout/             |          |
|    ep_len_mean       | 256      |
|    ep_rew_mean       | 0        |
| time/                |          |
|    fps               | 696      |
|    iterations        | 1        |
|    time_elapsed      | 0        |
|    total_timesteps   | 512      |
-----------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 234          |
|    ep_rew_mean          | 0.0643       |
| time/                   |              |
|    fps                  | 674          |
|    iterations           | 2            |
|    time_elapsed         | 1            |
|    total_timesteps      | 1024         |
| train/                  |              |
|    approx_kl            | 0.0011929488 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -7.78        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0281      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 3.68e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 241          |
|    ep_rew_mean          | 0.0428       |
| time/                   |              |
|    fps                  | 713          |
|    iterations           | 3            |
|    time_elapsed         | 2            |
|    total_timesteps      | 1536         |
| train/                  |              |
|    approx_kl            | 0.0011862615 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -0.0464      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0278      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 0.000907     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 226          |
|    ep_rew_mean          | 0.15         |
| time/                   |              |
|    fps                  | 728          |
|    iterations           | 4            |
|    time_elapsed         | 2            |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0010169536 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -4.58        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0283      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 3.03e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 232          |
|    ep_rew_mean          | 0.12         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 5            |
|    time_elapsed         | 3            |
|    total_timesteps      | 2560         |
| train/                  |              |
|    approx_kl            | 0.0011860305 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.0062       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0136      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000987    |
|    value_loss           | 0.0122       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 236         |
|    ep_rew_mean          | 0.0999      |
| time/                   |             |
|    fps                  | 744         |
|    iterations           | 6           |
|    time_elapsed         | 4           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.001609719 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -4.08       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0202     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 0.000142    |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 0.0857     |
| time/                   |            |
|    fps                  | 753        |
|    iterations           | 7          |
|    time_elapsed         | 4          |
|    total_timesteps      | 3584       |
| train/                  |            |
|    approx_kl            | 0.01104179 |
|    clip_fraction        | 0.0482     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | -0.818     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0457    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00638   |
|    value_loss           | 3.87e-05   |
----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 240         |
|    ep_rew_mean          | 0.0728      |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 8           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010788812 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | -3.24       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0175     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 3.75e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 240         |
|    ep_rew_mean          | 0.0701      |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 9           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.009758526 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.574      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00939    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 1.98e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 236         |
|    ep_rew_mean          | 0.0856      |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 10          |
|    time_elapsed         | 6           |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.010251619 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | -0.117      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0462     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 9.48e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 238         |
|    ep_rew_mean          | 0.0781      |
| time/                   |             |
|    fps                  | 763         |
|    iterations           | 11          |
|    time_elapsed         | 7           |
|    total_timesteps      | 5632        |
| train/                  |             |
|    approx_kl            | 0.014373602 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.0579      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0381     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0079     |
|    value_loss           | 0.00298     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 239         |
|    ep_rew_mean          | 0.0719      |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 12          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.010594442 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | -8.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0109      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 4.37e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 230         |
|    ep_rew_mean          | 0.121       |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 13          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6656        |
| train/                  |             |
|    approx_kl            | 0.012612686 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | -3.09       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.006      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00809    |
|    value_loss           | 1.75e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 229         |
|    ep_rew_mean          | 0.141       |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 14          |
|    time_elapsed         | 9           |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.006204554 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.000134    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0301     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 0.019       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 225         |
|    ep_rew_mean          | 0.166       |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 15          |
|    time_elapsed         | 10          |
|    total_timesteps      | 7680        |
| train/                  |             |
|    approx_kl            | 0.009006351 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.0284     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00257     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 217         |
|    ep_rew_mean          | 0.231       |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 16          |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016563153 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.0188      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0481     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00575     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 219          |
|    ep_rew_mean          | 0.22         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 17           |
|    time_elapsed         | 11           |
|    total_timesteps      | 8704         |
| train/                  |              |
|    approx_kl            | 0.0136592705 |
|    clip_fraction        | 0.09         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.0246       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0161      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00811     |
|    value_loss           | 0.0517       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 208         |
|    ep_rew_mean          | 0.284       |
| time/                   |             |
|    fps                  | 764         |
|    iterations           | 18          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.013995051 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | -7.51       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0388     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.000694    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 0.387       |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 19          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9728        |
| train/                  |             |
|    approx_kl            | 0.004692683 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.0579      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0149      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 0.0466      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | 0.424        |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 20           |
|    time_elapsed         | 13           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0050713895 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.48        |
|    explained_variance   | 0.0697       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0107       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00539     |
|    value_loss           | 0.0686       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 0.434       |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 21          |
|    time_elapsed         | 14          |
|    total_timesteps      | 10752       |
| train/                  |             |
|    approx_kl            | 0.009502441 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | -0.00896    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00626    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.008      |
|    value_loss           | 0.0311      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 176          |
|    ep_rew_mean          | 0.471        |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 22           |
|    time_elapsed         | 14           |
|    total_timesteps      | 11264        |
| train/                  |              |
|    approx_kl            | 0.0039882376 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0104      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00284     |
|    value_loss           | 0.0283       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 173          |
|    ep_rew_mean          | 0.487        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 23           |
|    time_elapsed         | 15           |
|    total_timesteps      | 11776        |
| train/                  |              |
|    approx_kl            | 0.0065997397 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00574     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 0.0392       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | 0.549       |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 24          |
|    time_elapsed         | 16          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.007533603 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0192     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 0.0124      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | 0.583        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 25           |
|    time_elapsed         | 16           |
|    total_timesteps      | 12800        |
| train/                  |              |
|    approx_kl            | 0.0042904913 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00118      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00423     |
|    value_loss           | 0.065        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 154          |
|    ep_rew_mean          | 0.595        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 26           |
|    time_elapsed         | 17           |
|    total_timesteps      | 13312        |
| train/                  |              |
|    approx_kl            | 0.0040961513 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00258     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 0.0248       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 147          |
|    ep_rew_mean          | 0.648        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 27           |
|    time_elapsed         | 18           |
|    total_timesteps      | 13824        |
| train/                  |              |
|    approx_kl            | 0.0028212443 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00481     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00489     |
|    value_loss           | 0.0205       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 0.657       |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 28          |
|    time_elapsed         | 19          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.003817337 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00589     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 0.0611      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 134         |
|    ep_rew_mean          | 0.666       |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 29          |
|    time_elapsed         | 20          |
|    total_timesteps      | 14848       |
| train/                  |             |
|    approx_kl            | 0.004709157 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.987      |
|    explained_variance   | -0.00267    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0143     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 0.0246      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 0.679       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 30          |
|    time_elapsed         | 20          |
|    total_timesteps      | 15360       |
| train/                  |             |
|    approx_kl            | 0.002926192 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -1.12       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0116     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000535   |
|    value_loss           | 0.00926     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 0.702       |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 31          |
|    time_elapsed         | 21          |
|    total_timesteps      | 15872       |
| train/                  |             |
|    approx_kl            | 0.004046023 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | -0.157      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00258    |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 0.0107      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 94.1        |
|    ep_rew_mean          | 0.714       |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 32          |
|    time_elapsed         | 22          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.003631098 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | -0.223      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0166     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00618    |
|    value_loss           | 0.00299     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 92           |
|    ep_rew_mean          | 0.687        |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 33           |
|    time_elapsed         | 22           |
|    total_timesteps      | 16896        |
| train/                  |              |
|    approx_kl            | 0.0015386507 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.00969      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0007       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 0.0129       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | 0.658       |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 34          |
|    time_elapsed         | 23          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.005737745 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | -0.655      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0274     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 0.00238     |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 82           |
|    ep_rew_mean          | 0.629        |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 35           |
|    time_elapsed         | 24           |
|    total_timesteps      | 17920        |
| train/                  |              |
|    approx_kl            | 0.0041071507 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | -0.122       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0182      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 0.00271      |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 80.8        |
|    ep_rew_mean          | 0.63        |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 36          |
|    time_elapsed         | 25          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.003122501 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.0915      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0131     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00376    |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 79.5        |
|    ep_rew_mean          | 0.609       |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 37          |
|    time_elapsed         | 25          |
|    total_timesteps      | 18944       |
| train/                  |             |
|    approx_kl            | 0.008220273 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00787     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 0.0896      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 74.2        |
|    ep_rew_mean          | 0.568       |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 38          |
|    time_elapsed         | 26          |
|    total_timesteps      | 19456       |
| train/                  |             |
|    approx_kl            | 0.003543283 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | -5.94       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0292     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.000223   |
|    value_loss           | 0.00389     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 73.5        |
|    ep_rew_mean          | 0.576       |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 39          |
|    time_elapsed         | 27          |
|    total_timesteps      | 19968       |
| train/                  |             |
|    approx_kl            | 0.009349106 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | -0.238      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0172      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00419    |
|    value_loss           | 0.0112      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 75.6        |
|    ep_rew_mean          | 0.57        |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 40          |
|    time_elapsed         | 27          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.014599969 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.0904      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.094       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 74.9         |
|    ep_rew_mean          | 0.584        |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 41           |
|    time_elapsed         | 28           |
|    total_timesteps      | 20992        |
| train/                  |              |
|    approx_kl            | 0.0059642214 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.0147       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0138      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00613     |
|    value_loss           | 0.0219       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 73.2         |
|    ep_rew_mean          | 0.563        |
| time/                   |              |
|    fps                  | 734          |
|    iterations           | 42           |
|    time_elapsed         | 29           |
|    total_timesteps      | 21504        |
| train/                  |              |
|    approx_kl            | 0.0035271458 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.022        |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 0.0885       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 70.6         |
|    ep_rew_mean          | 0.593        |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 43           |
|    time_elapsed         | 29           |
|    total_timesteps      | 22016        |
| train/                  |              |
|    approx_kl            | 0.0066777444 |
|    clip_fraction        | 0.0539       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.12        |
|    explained_variance   | -0.0287      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00708      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00608     |
|    value_loss           | 0.0355       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 72.8         |
|    ep_rew_mean          | 0.633        |
| time/                   |              |
|    fps                  | 735          |
|    iterations           | 44           |
|    time_elapsed         | 30           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0043780264 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0401      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.0362       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 74           |
|    ep_rew_mean          | 0.637        |
| time/                   |              |
|    fps                  | 736          |
|    iterations           | 45           |
|    time_elapsed         | 31           |
|    total_timesteps      | 23040        |
| train/                  |              |
|    approx_kl            | 0.0056930757 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.0403       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0179       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00939     |
|    value_loss           | 0.0443       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 75.1        |
|    ep_rew_mean          | 0.661       |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 46          |
|    time_elapsed         | 31          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.009584667 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | -1.21       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0195     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00697    |
|    value_loss           | 0.00783     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 75.7        |
|    ep_rew_mean          | 0.67        |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 47          |
|    time_elapsed         | 32          |
|    total_timesteps      | 24064       |
| train/                  |             |
|    approx_kl            | 0.004411781 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.0985      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 0.0325      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 73.4        |
|    ep_rew_mean          | 0.747       |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 48          |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.006664237 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0308     |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 0.0115      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 74.2         |
|    ep_rew_mean          | 0.758        |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 49           |
|    time_elapsed         | 34           |
|    total_timesteps      | 25088        |
| train/                  |              |
|    approx_kl            | 0.0058528697 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0416       |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00728     |
|    value_loss           | 0.0955       |
------------------------------------------
---------------------------------------
| adaptive/               |           |
|    adaptation_factor    | 1         |
|    algorithm            | PPO       |
|    base_clip_range      | 0.2       |
|    base_lr              | 0.0003    |
|    clip_range           | 0.2       |
|    drift_magnitude      | 0         |
|    ent_coef             | 0.01      |
|    learning_rate        | 0.0003    |
| env/                    |           |
|    base_value           | 9.8       |
|    reward_scale         | 9.8       |
| rollout/                |           |
|    ep_len_mean          | 71.8      |
|    ep_rew_mean          | 0.785     |
| time/                   |           |
|    fps                  | 737       |
|    iterations           | 50        |
|    time_elapsed         | 34        |
|    total_timesteps      | 25600     |
| train/                  |           |
|    approx_kl            | 0.0072657 |
|    clip_fraction        | 0.0695    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.23     |
|    explained_variance   | -0.615    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.000972  |
|    n_updates            | 490       |
|    policy_gradient_loss | -0.00308  |
|    value_loss           | 0.0141    |
---------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 69.4         |
|    ep_rew_mean          | 0.824        |
| time/                   |              |
|    fps                  | 737          |
|    iterations           | 51           |
|    time_elapsed         | 35           |
|    total_timesteps      | 26112        |
| train/                  |              |
|    approx_kl            | 0.0037349043 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0108       |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00845     |
|    value_loss           | 0.056        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 71           |
|    ep_rew_mean          | 0.84         |
| time/                   |              |
|    fps                  | 739          |
|    iterations           | 52           |
|    time_elapsed         | 36           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0052694683 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.0458       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00598     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00583     |
|    value_loss           | 0.0348       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 69.6        |
|    ep_rew_mean          | 0.813       |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 53          |
|    time_elapsed         | 36          |
|    total_timesteps      | 27136       |
| train/                  |             |
|    approx_kl            | 0.005107001 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.0762      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000573   |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 0.0153      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 65.3         |
|    ep_rew_mean          | 0.758        |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 54           |
|    time_elapsed         | 37           |
|    total_timesteps      | 27648        |
| train/                  |              |
|    approx_kl            | 0.0029590859 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0259      |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00462     |
|    value_loss           | 0.015        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 57.4        |
|    ep_rew_mean          | 0.866       |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 55          |
|    time_elapsed         | 37          |
|    total_timesteps      | 28160       |
| train/                  |             |
|    approx_kl            | 0.004385785 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000322   |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 0.0233      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 50.3        |
|    ep_rew_mean          | 0.971       |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 56          |
|    time_elapsed         | 38          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.004048522 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.998      |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0338      |
|    n_updates            | 550         |
|    policy_gradient_loss | -0.0061     |
|    value_loss           | 0.109       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 37.6        |
|    ep_rew_mean          | 1.09        |
| time/                   |             |
|    fps                  | 742         |
|    iterations           | 57          |
|    time_elapsed         | 39          |
|    total_timesteps      | 29184       |
| train/                  |             |
|    approx_kl            | 0.004999752 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00192    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0395      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 31.9        |
|    ep_rew_mean          | 1.04        |
| time/                   |             |
|    fps                  | 742         |
|    iterations           | 58          |
|    time_elapsed         | 39          |
|    total_timesteps      | 29696       |
| train/                  |             |
|    approx_kl            | 0.003187635 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00596     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00619    |
|    value_loss           | 0.0461      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 26.6        |
|    ep_rew_mean          | 1.17        |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 59          |
|    time_elapsed         | 40          |
|    total_timesteps      | 30208       |
| train/                  |             |
|    approx_kl            | 0.002102314 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.94       |
|    explained_variance   | -0.599      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00402    |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 0.048       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 23.8         |
|    ep_rew_mean          | 1.29         |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 60           |
|    time_elapsed         | 41           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0025119088 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.932       |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00372      |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 0.0628       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 22.5         |
|    ep_rew_mean          | 1.3          |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 61           |
|    time_elapsed         | 41           |
|    total_timesteps      | 31232        |
| train/                  |              |
|    approx_kl            | 0.0041230586 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.871       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0154      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.0471       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 22.1         |
|    ep_rew_mean          | 1.31         |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 62           |
|    time_elapsed         | 42           |
|    total_timesteps      | 31744        |
| train/                  |              |
|    approx_kl            | 0.0012787087 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.804       |
|    explained_variance   | -0.0984      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0154       |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 0.0584       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 20.9         |
|    ep_rew_mean          | 1.2          |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 63           |
|    time_elapsed         | 43           |
|    total_timesteps      | 32256        |
| train/                  |              |
|    approx_kl            | 0.0009308738 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.744       |
|    explained_variance   | -0.0652      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0265       |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000456    |
|    value_loss           | 0.0826       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 21.4         |
|    ep_rew_mean          | 1.17         |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 64           |
|    time_elapsed         | 44           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0029824802 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.751       |
|    explained_variance   | -0.134       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00352      |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 0.0691       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 1.19        |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 65          |
|    time_elapsed         | 44          |
|    total_timesteps      | 33280       |
| train/                  |             |
|    approx_kl            | 0.003929096 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0227      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 0.0994      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 19.9         |
|    ep_rew_mean          | 1.05         |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 66           |
|    time_elapsed         | 45           |
|    total_timesteps      | 33792        |
| train/                  |              |
|    approx_kl            | 0.0026823268 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.659       |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00659     |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00514     |
|    value_loss           | 0.0301       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 18.5         |
|    ep_rew_mean          | 1            |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 67           |
|    time_elapsed         | 46           |
|    total_timesteps      | 34304        |
| train/                  |              |
|    approx_kl            | 0.0025210446 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.626       |
|    explained_variance   | -0.429       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00821      |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 0.0934       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 17.9         |
|    ep_rew_mean          | 0.782        |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 68           |
|    time_elapsed         | 46           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0015527306 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.0655       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.024        |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 0.0337       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 0.852       |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 69          |
|    time_elapsed         | 47          |
|    total_timesteps      | 35328       |
| train/                  |             |
|    approx_kl            | 0.004705596 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.031       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00699     |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 0.0865      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 0.884        |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 70           |
|    time_elapsed         | 48           |
|    total_timesteps      | 35840        |
| train/                  |              |
|    approx_kl            | 0.0031608564 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.715       |
|    explained_variance   | 0.0225       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0541       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 0.129        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 0.865        |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 71           |
|    time_elapsed         | 48           |
|    total_timesteps      | 36352        |
| train/                  |              |
|    approx_kl            | 0.0025568102 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.683       |
|    explained_variance   | -0.0199      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0375       |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00554     |
|    value_loss           | 0.135        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 0.611        |
| time/                   |              |
|    fps                  | 745          |
|    iterations           | 72           |
|    time_elapsed         | 49           |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0033712233 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.652       |
|    explained_variance   | -0.0319      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0693       |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00306     |
|    value_loss           | 0.128        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 0.856        |
| time/                   |              |
|    fps                  | 745          |
|    iterations           | 73           |
|    time_elapsed         | 50           |
|    total_timesteps      | 37376        |
| train/                  |              |
|    approx_kl            | 0.0040776394 |
|    clip_fraction        | 0.0705       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.67        |
|    explained_variance   | -0.255       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0052       |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00772     |
|    value_loss           | 0.0267       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 746          |
|    iterations           | 74           |
|    time_elapsed         | 50           |
|    total_timesteps      | 37888        |
| train/                  |              |
|    approx_kl            | 0.0044075334 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.659       |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0278       |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00533     |
|    value_loss           | 0.16         |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.3         |
|    ep_rew_mean          | 1.48         |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 75           |
|    time_elapsed         | 51           |
|    total_timesteps      | 38400        |
| train/                  |              |
|    approx_kl            | 0.0038593016 |
|    clip_fraction        | 0.0461       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00622      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00627     |
|    value_loss           | 0.0453       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 14.3        |
|    ep_rew_mean          | 1.48        |
| time/                   |             |
|    fps                  | 747         |
|    iterations           | 76          |
|    time_elapsed         | 52          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.004991399 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0167     |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.0219      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 14.5         |
|    ep_rew_mean          | 1.2          |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 77           |
|    time_elapsed         | 52           |
|    total_timesteps      | 39424        |
| train/                  |              |
|    approx_kl            | 0.0011495728 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.0621       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0189       |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.0041      |
|    value_loss           | 0.0589       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 1.21         |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 78           |
|    time_elapsed         | 53           |
|    total_timesteps      | 39936        |
| train/                  |              |
|    approx_kl            | 0.0009641695 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.511       |
|    explained_variance   | -0.109       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0559       |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00181     |
|    value_loss           | 0.141        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 14.4        |
|    ep_rew_mean          | 1.23        |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 79          |
|    time_elapsed         | 53          |
|    total_timesteps      | 40448       |
| train/                  |             |
|    approx_kl            | 0.002236105 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.194       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0101      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 0.0608      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 14.3         |
|    ep_rew_mean          | 1.48         |
| time/                   |              |
|    fps                  | 750          |
|    iterations           | 80           |
|    time_elapsed         | 54           |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0036964007 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.0482       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0256       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00646     |
|    value_loss           | 0.0775       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 14.1        |
|    ep_rew_mean          | 1.57        |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 81          |
|    time_elapsed         | 55          |
|    total_timesteps      | 41472       |
| train/                  |             |
|    approx_kl            | 0.003325311 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.389      |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00435    |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 0.023       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1.31         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 82           |
|    time_elapsed         | 55           |
|    total_timesteps      | 41984        |
| train/                  |              |
|    approx_kl            | 0.0026730068 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.345       |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0111       |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00447     |
|    value_loss           | 0.0329       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.787        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 83           |
|    time_elapsed         | 56           |
|    total_timesteps      | 42496        |
| train/                  |              |
|    approx_kl            | 0.0016595294 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.291       |
|    explained_variance   | -0.0647      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0661       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 0.167        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.4        |
|    ep_rew_mean          | 0.357       |
| time/                   |             |
|    fps                  | 751         |
|    iterations           | 84          |
|    time_elapsed         | 57          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.002973219 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | -0.458      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0203      |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 0.0824      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.7         |
|    ep_rew_mean          | 0.722        |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 85           |
|    time_elapsed         | 57           |
|    total_timesteps      | 43520        |
| train/                  |              |
|    approx_kl            | 0.0013889137 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | -0.106       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00513      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 0.0222       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 86           |
|    time_elapsed         | 58           |
|    total_timesteps      | 44032        |
| train/                  |              |
|    approx_kl            | 0.0022670417 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.374       |
|    explained_variance   | 0.0474       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.069        |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.25         |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1.27         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 87           |
|    time_elapsed         | 59           |
|    total_timesteps      | 44544        |
| train/                  |              |
|    approx_kl            | 0.0014654911 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.0579       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0353       |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00506     |
|    value_loss           | 0.0916       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 0.827        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 88           |
|    time_elapsed         | 59           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0015262956 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | -0.129       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00935      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00087     |
|    value_loss           | 0.0451       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 0.819        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 89           |
|    time_elapsed         | 60           |
|    total_timesteps      | 45568        |
| train/                  |              |
|    approx_kl            | 0.0024347547 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | -0.0751      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0056       |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 0.0579       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.999        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 90           |
|    time_elapsed         | 61           |
|    total_timesteps      | 46080        |
| train/                  |              |
|    approx_kl            | 0.0007653879 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.0567       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0214       |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.000479    |
|    value_loss           | 0.0528       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 1.07         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 91           |
|    time_elapsed         | 61           |
|    total_timesteps      | 46592        |
| train/                  |              |
|    approx_kl            | 0.0011912667 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.42        |
|    explained_variance   | 0.0421       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0798       |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 0.172        |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 13            |
|    ep_rew_mean          | 0.733         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 92            |
|    time_elapsed         | 62            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00083632255 |
|    clip_fraction        | 0.0041        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.339        |
|    explained_variance   | -0.0402       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0394        |
|    n_updates            | 910           |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 0.12          |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.777        |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 93           |
|    time_elapsed         | 63           |
|    total_timesteps      | 47616        |
| train/                  |              |
|    approx_kl            | 0.0011987855 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | -0.108       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0108       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000968    |
|    value_loss           | 0.0436       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.4        |
|    ep_rew_mean          | 0.978       |
| time/                   |             |
|    fps                  | 751         |
|    iterations           | 94          |
|    time_elapsed         | 64          |
|    total_timesteps      | 48128       |
| train/                  |             |
|    approx_kl            | 0.003077757 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.0823      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00682     |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 0.0574      |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 1.3           |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 95            |
|    time_elapsed         | 64            |
|    total_timesteps      | 48640         |
| train/                  |               |
|    approx_kl            | 0.00084608817 |
|    clip_fraction        | 0.00742       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.305        |
|    explained_variance   | 0.0256        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.04          |
|    n_updates            | 940           |
|    policy_gradient_loss | -0.00182      |
|    value_loss           | 0.104         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1.44         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 96           |
|    time_elapsed         | 65           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0014924468 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0.196        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0105       |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 0.0499       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.9        |
|    ep_rew_mean          | 1.5         |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 97          |
|    time_elapsed         | 65          |
|    total_timesteps      | 49664       |
| train/                  |             |
|    approx_kl            | 0.000576935 |
|    clip_fraction        | 0.0084      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | -0.0237     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.057       |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.000795   |
|    value_loss           | 0.0948      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1.56         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 98           |
|    time_elapsed         | 66           |
|    total_timesteps      | 50176        |
| train/                  |              |
|    approx_kl            | 0.0019180122 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00594     |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 0.0222       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 1.61          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 99            |
|    time_elapsed         | 67            |
|    total_timesteps      | 50688         |
| train/                  |               |
|    approx_kl            | 0.00089369563 |
|    clip_fraction        | 0.0184        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.216        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00321       |
|    n_updates            | 980           |
|    policy_gradient_loss | -0.00276      |
|    value_loss           | 0.022         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1.53         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 100          |
|    time_elapsed         | 68           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0013609745 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.235       |
|    explained_variance   | 0.119        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0118       |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000822    |
|    value_loss           | 0.0207       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1.52         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 101          |
|    time_elapsed         | 68           |
|    total_timesteps      | 51712        |
| train/                  |              |
|    approx_kl            | 0.0014736446 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.0134       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0285       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 0.0594       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1.53         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 102          |
|    time_elapsed         | 69           |
|    total_timesteps      | 52224        |
| train/                  |              |
|    approx_kl            | 0.0010516522 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.0986       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0184       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.0436       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1.15          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 103           |
|    time_elapsed         | 69            |
|    total_timesteps      | 52736         |
| train/                  |               |
|    approx_kl            | 0.00059088913 |
|    clip_fraction        | 0.00762       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.231        |
|    explained_variance   | 0.0204        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0243        |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.00124      |
|    value_loss           | 0.0518        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.958        |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 104          |
|    time_elapsed         | 70           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0013459083 |
|    clip_fraction        | 0.00918      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | -0.173       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0267       |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 0.0867       |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 12.8       |
|    ep_rew_mean          | 1.17       |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 105        |
|    time_elapsed         | 71         |
|    total_timesteps      | 53760      |
| train/                  |            |
|    approx_kl            | 0.00108764 |
|    clip_fraction        | 0.0125     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.0168     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0462     |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.00133   |
|    value_loss           | 0.112      |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 1.42         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 106          |
|    time_elapsed         | 71           |
|    total_timesteps      | 54272        |
| train/                  |              |
|    approx_kl            | 0.0018367957 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0242       |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.0043      |
|    value_loss           | 0.0767       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.2          |
|    ep_rew_mean          | 1.07          |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 107           |
|    time_elapsed         | 72            |
|    total_timesteps      | 54784         |
| train/                  |               |
|    approx_kl            | 0.00086657563 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.163        |
|    explained_variance   | -0.0257       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0457        |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00201      |
|    value_loss           | 0.101         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1.02         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 108          |
|    time_elapsed         | 73           |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0007403488 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | -0.126       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0207       |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.0553       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1.24         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 109          |
|    time_elapsed         | 73           |
|    total_timesteps      | 55808        |
| train/                  |              |
|    approx_kl            | 0.0004418043 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.177       |
|    explained_variance   | 0.0708       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0383       |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.0797       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.3          |
|    ep_rew_mean          | 1.31          |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 110           |
|    time_elapsed         | 74            |
|    total_timesteps      | 56320         |
| train/                  |               |
|    approx_kl            | 0.00022302591 |
|    clip_fraction        | 0.00664       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.159        |
|    explained_variance   | 0.00991       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0636        |
|    n_updates            | 1090          |
|    policy_gradient_loss | -0.00153      |
|    value_loss           | 0.121         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.43         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 111          |
|    time_elapsed         | 75           |
|    total_timesteps      | 56832        |
| train/                  |              |
|    approx_kl            | 0.0006211909 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.16        |
|    explained_variance   | 0.0931       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0167       |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 0.0414       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 1.58         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 112          |
|    time_elapsed         | 75           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0006298503 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.0715       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.048        |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.0638       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 1.44         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 113          |
|    time_elapsed         | 76           |
|    total_timesteps      | 57856        |
| train/                  |              |
|    approx_kl            | 0.0010774707 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00214      |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 0.0113       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.82         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 114          |
|    time_elapsed         | 77           |
|    total_timesteps      | 58368        |
| train/                  |              |
|    approx_kl            | 0.0008831179 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | -0.0957      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0304       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00212     |
|    value_loss           | 0.105        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.8        |
|    ep_rew_mean          | 0.481       |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 115         |
|    time_elapsed         | 77          |
|    total_timesteps      | 58880       |
| train/                  |             |
|    approx_kl            | 0.002350409 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | -0.218      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0138      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 0.0988      |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.4          |
|    ep_rew_mean          | 0.604         |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 116           |
|    time_elapsed         | 78            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00023312017 |
|    clip_fraction        | 0.00508       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.167        |
|    explained_variance   | -0.0498       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00914       |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.000413     |
|    value_loss           | 0.0231        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.1         |
|    ep_rew_mean          | 0.647        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 117          |
|    time_elapsed         | 79           |
|    total_timesteps      | 59904        |
| train/                  |              |
|    approx_kl            | 0.0013515746 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.164       |
|    explained_variance   | 0.0319       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0593       |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00238     |
|    value_loss           | 0.116        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | 1.05        |
| time/                   |             |
|    fps                  | 756         |
|    iterations           | 118         |
|    time_elapsed         | 79          |
|    total_timesteps      | 60416       |
| train/                  |             |
|    approx_kl            | 0.001035333 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | -0.0257     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0155      |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 0.0453      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 119          |
|    time_elapsed         | 80           |
|    total_timesteps      | 60928        |
| train/                  |              |
|    approx_kl            | 0.0007103444 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.0712       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0185       |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00161     |
|    value_loss           | 0.146        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.884        |
| time/                   |              |
|    fps                  | 756          |
|    iterations           | 120          |
|    time_elapsed         | 81           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0014963356 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | -0.0199      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0318       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 0.0868       |
------------------------------------------
---------------------------------------
| adaptive/               |           |
|    adaptation_factor    | 1         |
|    algorithm            | PPO       |
|    base_clip_range      | 0.2       |
|    base_lr              | 0.0003    |
|    clip_range           | 0.2       |
|    drift_magnitude      | 0         |
|    ent_coef             | 0.01      |
|    learning_rate        | 0.0003    |
| env/                    |           |
|    base_value           | 9.8       |
|    reward_scale         | 9.8       |
| rollout/                |           |
|    ep_len_mean          | 13        |
|    ep_rew_mean          | 0.667     |
| time/                   |           |
|    fps                  | 757       |
|    iterations           | 121       |
|    time_elapsed         | 81        |
|    total_timesteps      | 61952     |
| train/                  |           |
|    approx_kl            | 0.0115282 |
|    clip_fraction        | 0.0229    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.267    |
|    explained_variance   | -0.252    |
|    learning_rate        | 0.0003    |
|    loss                 | -0.00102  |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.00257  |
|    value_loss           | 0.0418    |
---------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 0.747        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 122          |
|    time_elapsed         | 82           |
|    total_timesteps      | 62464        |
| train/                  |              |
|    approx_kl            | 0.0011034164 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.00669      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0167       |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 0.0634       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.8         |
|    ep_rew_mean          | 0.826        |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 123          |
|    time_elapsed         | 83           |
|    total_timesteps      | 62976        |
| train/                  |              |
|    approx_kl            | 0.0013885355 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.346       |
|    explained_variance   | 0.0186       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.07         |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.151        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 1.05         |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 124          |
|    time_elapsed         | 83           |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0022602573 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.00111      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0244       |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.0668       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.9         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 125          |
|    time_elapsed         | 84           |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 0.0015761763 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.326       |
|    explained_variance   | 0.0784       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0294       |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.0033      |
|    value_loss           | 0.09         |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 1.42         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 126          |
|    time_elapsed         | 85           |
|    total_timesteps      | 64512        |
| train/                  |              |
|    approx_kl            | 0.0004904659 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.303       |
|    explained_variance   | 0.0239       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0668       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 0.156        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.6        |
|    ep_rew_mean          | 1.38        |
| time/                   |             |
|    fps                  | 758         |
|    iterations           | 127         |
|    time_elapsed         | 85          |
|    total_timesteps      | 65024       |
| train/                  |             |
|    approx_kl            | 0.002775169 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.231       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00277     |
|    n_updates            | 1260        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.0418      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1.29         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 128          |
|    time_elapsed         | 86           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0005685047 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | -0.109       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0365       |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000976    |
|    value_loss           | 0.112        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 0.976        |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 129          |
|    time_elapsed         | 87           |
|    total_timesteps      | 66048        |
| train/                  |              |
|    approx_kl            | 0.0014181421 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0.0587       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0364       |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.0912       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 13            |
|    ep_rew_mean          | 0.934         |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 130           |
|    time_elapsed         | 87            |
|    total_timesteps      | 66560         |
| train/                  |               |
|    approx_kl            | 0.00037330086 |
|    clip_fraction        | 0.0125        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.262        |
|    explained_variance   | -0.118        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0259        |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 0.0901        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.1         |
|    ep_rew_mean          | 1.18         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 131          |
|    time_elapsed         | 88           |
|    total_timesteps      | 67072        |
| train/                  |              |
|    approx_kl            | 0.0006236745 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.0461       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0282       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 0.055        |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 13         |
|    ep_rew_mean          | 1.42       |
| time/                   |            |
|    fps                  | 759        |
|    iterations           | 132        |
|    time_elapsed         | 89         |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.00265022 |
|    clip_fraction        | 0.0316     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.259     |
|    explained_variance   | 0.149      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00829    |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.00543   |
|    value_loss           | 0.089      |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.7          |
|    ep_rew_mean          | 1.03          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 133           |
|    time_elapsed         | 89            |
|    total_timesteps      | 68096         |
| train/                  |               |
|    approx_kl            | 0.00086497783 |
|    clip_fraction        | 0.00918       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.266        |
|    explained_variance   | 0.015         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0277        |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.00248      |
|    value_loss           | 0.0712        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 0.641        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 134          |
|    time_elapsed         | 90           |
|    total_timesteps      | 68608        |
| train/                  |              |
|    approx_kl            | 0.0010952906 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.3         |
|    explained_variance   | -0.176       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0382       |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.145        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.7        |
|    ep_rew_mean          | 0.814       |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 135         |
|    time_elapsed         | 90          |
|    total_timesteps      | 69120       |
| train/                  |             |
|    approx_kl            | 0.003052886 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.00758     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0168      |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 0.0645      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 136          |
|    time_elapsed         | 91           |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0035215192 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.365       |
|    explained_variance   | 0.0436       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0314       |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 0.114        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 0.788        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 137          |
|    time_elapsed         | 92           |
|    total_timesteps      | 70144        |
| train/                  |              |
|    approx_kl            | 0.0013809511 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.357       |
|    explained_variance   | 0.0786       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0053       |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00272     |
|    value_loss           | 0.0154       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.6        |
|    ep_rew_mean          | 0.919       |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 138         |
|    time_elapsed         | 92          |
|    total_timesteps      | 70656       |
| train/                  |             |
|    approx_kl            | 0.001308799 |
|    clip_fraction        | 0.0113      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.416      |
|    explained_variance   | -0.112      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0107      |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 0.0842      |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 12.7       |
|    ep_rew_mean          | 1.07       |
| time/                   |            |
|    fps                  | 760        |
|    iterations           | 139        |
|    time_elapsed         | 93         |
|    total_timesteps      | 71168      |
| train/                  |            |
|    approx_kl            | 0.00188873 |
|    clip_fraction        | 0.0146     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.0506     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0535     |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.00271   |
|    value_loss           | 0.149      |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.5         |
|    ep_rew_mean          | 1.01         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 140          |
|    time_elapsed         | 94           |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0013045938 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.00605      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.039        |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.112        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 0.839        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 141          |
|    time_elapsed         | 94           |
|    total_timesteps      | 72192        |
| train/                  |              |
|    approx_kl            | 0.0025147528 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | -0.0552      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0225       |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.0048      |
|    value_loss           | 0.0671       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 1.23         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 142          |
|    time_elapsed         | 95           |
|    total_timesteps      | 72704        |
| train/                  |              |
|    approx_kl            | 0.0013386825 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0.0179       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0756       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 0.131        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 1.47         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 143          |
|    time_elapsed         | 96           |
|    total_timesteps      | 73216        |
| train/                  |              |
|    approx_kl            | 0.0023307481 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0027       |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 0.0811       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.9         |
|    ep_rew_mean          | 1.33         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 144          |
|    time_elapsed         | 96           |
|    total_timesteps      | 73728        |
| train/                  |              |
|    approx_kl            | 0.0018504731 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.374       |
|    explained_variance   | -0.0753      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0162       |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.0569       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 145          |
|    time_elapsed         | 97           |
|    total_timesteps      | 74240        |
| train/                  |              |
|    approx_kl            | 0.0018489569 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.0247       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0335       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 0.0892       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.4         |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 146          |
|    time_elapsed         | 98           |
|    total_timesteps      | 74752        |
| train/                  |              |
|    approx_kl            | 0.0038910508 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | -0.0361      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0308       |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 0.0989       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.5        |
|    ep_rew_mean          | 1.15        |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 147         |
|    time_elapsed         | 98          |
|    total_timesteps      | 75264       |
| train/                  |             |
|    approx_kl            | 0.003082422 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.386      |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.011      |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 0.0551      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 0.771        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 148          |
|    time_elapsed         | 99           |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0015370323 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | -0.132       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00504      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00484     |
|    value_loss           | 0.0651       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 0.962        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 149          |
|    time_elapsed         | 100          |
|    total_timesteps      | 76288        |
| train/                  |              |
|    approx_kl            | 0.0012494433 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.335       |
|    explained_variance   | -0.0939      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0111       |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00568     |
|    value_loss           | 0.0458       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.994        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 150          |
|    time_elapsed         | 100          |
|    total_timesteps      | 76800        |
| train/                  |              |
|    approx_kl            | 0.0022098757 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0.0721       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.019        |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 0.158        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 1.06         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 151          |
|    time_elapsed         | 101          |
|    total_timesteps      | 77312        |
| train/                  |              |
|    approx_kl            | 0.0011406869 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | -0.0523      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0244       |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.104        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.6         |
|    ep_rew_mean          | 1.27         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 152          |
|    time_elapsed         | 102          |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0016440696 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.31        |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00754      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00384     |
|    value_loss           | 0.0289       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1.31          |
| time/                   |               |
|    fps                  | 761           |
|    iterations           | 153           |
|    time_elapsed         | 102           |
|    total_timesteps      | 78336         |
| train/                  |               |
|    approx_kl            | 0.00013633945 |
|    clip_fraction        | 0.00469       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.265        |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0226        |
|    n_updates            | 1520          |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 0.0753        |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.6          |
|    ep_rew_mean          | 1.42          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 154           |
|    time_elapsed         | 103           |
|    total_timesteps      | 78848         |
| train/                  |               |
|    approx_kl            | 0.00090080686 |
|    clip_fraction        | 0.0166        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.31         |
|    explained_variance   | -0.0361       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0338        |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.0025       |
|    value_loss           | 0.0744        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1.37         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 155          |
|    time_elapsed         | 104          |
|    total_timesteps      | 79360        |
| train/                  |              |
|    approx_kl            | 0.0023170528 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0075       |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00551     |
|    value_loss           | 0.0549       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.3          |
|    ep_rew_mean          | 0.934         |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 156           |
|    time_elapsed         | 104           |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00068633526 |
|    clip_fraction        | 0.0084        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.267        |
|    explained_variance   | -0.00677      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0686        |
|    n_updates            | 1550          |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 0.161         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.468        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 157          |
|    time_elapsed         | 105          |
|    total_timesteps      | 80384        |
| train/                  |              |
|    approx_kl            | 0.0010962867 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.311       |
|    explained_variance   | -0.341       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.013        |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00398     |
|    value_loss           | 0.167        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.898        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 158          |
|    time_elapsed         | 106          |
|    total_timesteps      | 80896        |
| train/                  |              |
|    approx_kl            | 0.0021634172 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.268       |
|    explained_variance   | 0.00189      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0171       |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 0.0537       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1.18          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 159           |
|    time_elapsed         | 107           |
|    total_timesteps      | 81408         |
| train/                  |               |
|    approx_kl            | 0.00064345216 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.259        |
|    explained_variance   | 0.0242        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0823        |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 0.229         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.8         |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 160          |
|    time_elapsed         | 107          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0010690407 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.00701      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0477       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 0.104        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.2         |
|    ep_rew_mean          | 0.916        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 161          |
|    time_elapsed         | 108          |
|    total_timesteps      | 82432        |
| train/                  |              |
|    approx_kl            | 0.0016059992 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | -0.0239      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0476       |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 0.0819       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.3         |
|    ep_rew_mean          | 1.03         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 162          |
|    time_elapsed         | 108          |
|    total_timesteps      | 82944        |
| train/                  |              |
|    approx_kl            | 0.0016269132 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | 0.0421       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.015        |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 0.0464       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.8        |
|    ep_rew_mean          | 1.07        |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 163         |
|    time_elapsed         | 109         |
|    total_timesteps      | 83456       |
| train/                  |             |
|    approx_kl            | 0.002715895 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.0647      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0275      |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 0.0828      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.8         |
|    ep_rew_mean          | 0.904        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 164          |
|    time_elapsed         | 110          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0008639897 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | -0.00537     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0728       |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 0.185        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.1        |
|    ep_rew_mean          | 0.861       |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 165         |
|    time_elapsed         | 110         |
|    total_timesteps      | 84480       |
| train/                  |             |
|    approx_kl            | 0.001523739 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | -0.0149     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.051       |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 0.111       |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 12.1          |
|    ep_rew_mean          | 1.31          |
| time/                   |               |
|    fps                  | 762           |
|    iterations           | 166           |
|    time_elapsed         | 111           |
|    total_timesteps      | 84992         |
| train/                  |               |
|    approx_kl            | 0.00087807374 |
|    clip_fraction        | 0.0143        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.262        |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00954       |
|    n_updates            | 1650          |
|    policy_gradient_loss | -0.00283      |
|    value_loss           | 0.0548        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 167          |
|    time_elapsed         | 112          |
|    total_timesteps      | 85504        |
| train/                  |              |
|    approx_kl            | 0.0023072765 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.0567       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0239       |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00391     |
|    value_loss           | 0.0687       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 1.18         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 168          |
|    time_elapsed         | 112          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0014494659 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | -0.0424      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0341       |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 0.134        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1.39         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 169          |
|    time_elapsed         | 113          |
|    total_timesteps      | 86528        |
| train/                  |              |
|    approx_kl            | 0.0012368268 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0239       |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.0958       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.2         |
|    ep_rew_mean          | 0.932        |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 170          |
|    time_elapsed         | 114          |
|    total_timesteps      | 87040        |
| train/                  |              |
|    approx_kl            | 0.0014838598 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | -0.00354     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0656       |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 0.127        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 0.982        |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 171          |
|    time_elapsed         | 114          |
|    total_timesteps      | 87552        |
| train/                  |              |
|    approx_kl            | 0.0014097067 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | -0.126       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0222       |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 0.132        |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 1.28          |
| time/                   |               |
|    fps                  | 763           |
|    iterations           | 172           |
|    time_elapsed         | 115           |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 0.00070404937 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.257        |
|    explained_variance   | 0.0707        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0353        |
|    n_updates            | 1710          |
|    policy_gradient_loss | -0.000955     |
|    value_loss           | 0.195         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1.26         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 173          |
|    time_elapsed         | 116          |
|    total_timesteps      | 88576        |
| train/                  |              |
|    approx_kl            | 0.0014744354 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | -9.27e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00452      |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 0.0299       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.8          |
|    ep_rew_mean          | 1.17          |
| time/                   |               |
|    fps                  | 762           |
|    iterations           | 174           |
|    time_elapsed         | 116           |
|    total_timesteps      | 89088         |
| train/                  |               |
|    approx_kl            | 0.00043229316 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.23         |
|    explained_variance   | 0.00946       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0267        |
|    n_updates            | 1730          |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 0.0909        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 175          |
|    time_elapsed         | 117          |
|    total_timesteps      | 89600        |
| train/                  |              |
|    approx_kl            | 0.0017805637 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.0116       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0206       |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00353     |
|    value_loss           | 0.0593       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.7          |
|    ep_rew_mean          | 0.818         |
| time/                   |               |
|    fps                  | 762           |
|    iterations           | 176           |
|    time_elapsed         | 118           |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 0.00061683357 |
|    clip_fraction        | 0.00664       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.206        |
|    explained_variance   | -0.0116       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0175        |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.000475     |
|    value_loss           | 0.0699        |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 1.09          |
| time/                   |               |
|    fps                  | 762           |
|    iterations           | 177           |
|    time_elapsed         | 118           |
|    total_timesteps      | 90624         |
| train/                  |               |
|    approx_kl            | 0.00048625958 |
|    clip_fraction        | 0.00566       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.199        |
|    explained_variance   | -0.0314       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0128        |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.000335     |
|    value_loss           | 0.0513        |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.4          |
|    ep_rew_mean          | 0.85          |
| time/                   |               |
|    fps                  | 762           |
|    iterations           | 178           |
|    time_elapsed         | 119           |
|    total_timesteps      | 91136         |
| train/                  |               |
|    approx_kl            | 0.00093403086 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.2          |
|    explained_variance   | 0.0459        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0426        |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.00146      |
|    value_loss           | 0.149         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.509        |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 179          |
|    time_elapsed         | 120          |
|    total_timesteps      | 91648        |
| train/                  |              |
|    approx_kl            | 0.0001696482 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | -0.212       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0264       |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.000489    |
|    value_loss           | 0.181        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 0.878        |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 180          |
|    time_elapsed         | 120          |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0006433992 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | -0.00202     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0206       |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.000746    |
|    value_loss           | 0.0538       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.1         |
|    ep_rew_mean          | 1.17         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 181          |
|    time_elapsed         | 121          |
|    total_timesteps      | 92672        |
| train/                  |              |
|    approx_kl            | 0.0010569905 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.0378       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0441       |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 0.202        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 182          |
|    time_elapsed         | 122          |
|    total_timesteps      | 93184        |
| train/                  |              |
|    approx_kl            | 0.0009941726 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | -0.00219     |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0527       |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 0.106        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12.3         |
|    ep_rew_mean          | 1.22         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 183          |
|    time_elapsed         | 122          |
|    total_timesteps      | 93696        |
| train/                  |              |
|    approx_kl            | 0.0012682911 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0.0104       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0706       |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 0.158        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 12.1        |
|    ep_rew_mean          | 1.34        |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 184         |
|    time_elapsed         | 123         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.004532963 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.075       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0141      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 0.038       |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 11.9        |
|    ep_rew_mean          | 1.27        |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 185         |
|    time_elapsed         | 124         |
|    total_timesteps      | 94720       |
| train/                  |             |
|    approx_kl            | 0.001089684 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.0389      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0341      |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 0.104       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 12           |
|    ep_rew_mean          | 0.879        |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 186          |
|    time_elapsed         | 124          |
|    total_timesteps      | 95232        |
| train/                  |              |
|    approx_kl            | 0.0014182468 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | -0.0382      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0209       |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.121        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.934        |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 187          |
|    time_elapsed         | 125          |
|    total_timesteps      | 95744        |
| train/                  |              |
|    approx_kl            | 0.0017773317 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | -0.0243      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0168       |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.104        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.88         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 188          |
|    time_elapsed         | 126          |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0009600938 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | 0.0125       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.101        |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00176     |
|    value_loss           | 0.22         |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.638        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 189          |
|    time_elapsed         | 127          |
|    total_timesteps      | 96768        |
| train/                  |              |
|    approx_kl            | 0.0013385171 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | -0.0131      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0457       |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00269     |
|    value_loss           | 0.137        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.5         |
|    ep_rew_mean          | 0.418        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 190          |
|    time_elapsed         | 127          |
|    total_timesteps      | 97280        |
| train/                  |              |
|    approx_kl            | 0.0006004558 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.215       |
|    explained_variance   | -0.165       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0111       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.0442       |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 11.9          |
|    ep_rew_mean          | 0.774         |
| time/                   |               |
|    fps                  | 761           |
|    iterations           | 191           |
|    time_elapsed         | 128           |
|    total_timesteps      | 97792         |
| train/                  |               |
|    approx_kl            | 0.00055584556 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.231        |
|    explained_variance   | 0.00848       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0172        |
|    n_updates            | 1900          |
|    policy_gradient_loss | -0.000967     |
|    value_loss           | 0.0503        |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.9         |
|    ep_rew_mean          | 1.1          |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 192          |
|    time_elapsed         | 129          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0007976034 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.263       |
|    explained_variance   | 0.023        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0559       |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.201        |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.95         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 193          |
|    time_elapsed         | 129          |
|    total_timesteps      | 98816        |
| train/                  |              |
|    approx_kl            | 0.0010582469 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0.0359       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0269       |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 0.0568       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.667        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 194          |
|    time_elapsed         | 130          |
|    total_timesteps      | 99328        |
| train/                  |              |
|    approx_kl            | 0.0006049811 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.224       |
|    explained_variance   | -0.127       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0131       |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00299     |
|    value_loss           | 0.0929       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.6         |
|    ep_rew_mean          | 0.8          |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 195          |
|    time_elapsed         | 131          |
|    total_timesteps      | 99840        |
| train/                  |              |
|    approx_kl            | 0.0007705068 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.258       |
|    explained_variance   | 0.0131       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0243       |
|    n_updates            | 1940         |
|    policy_gradient_loss | 2.33e-05     |
|    value_loss           | 0.0616       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 11.7         |
|    ep_rew_mean          | 0.938        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 196          |
|    time_elapsed         | 131          |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0013518579 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.257       |
|    explained_variance   | 0.00735      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0892       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.182        |
------------------------------------------
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading model.zip
wandb: uploading output.log; uploading model.zip; uploading config.yaml; uploading logs/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124_0/events.out.tfevents.1766028483.hungchan-Precision-7560.517507.0
wandb: uploading model.zip; uploading logs/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124_0/events.out.tfevents.1766028483.hungchan-Precision-7560.517507.0
wandb: uploading history steps 3582-4106, summary, console lines 5848-6479
wandb: 
wandb: Run history:
wandb: adaptive/adaptation_factor â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/base_clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           adaptive/base_lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        adaptive/clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/drift_magnitude â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          adaptive/ent_coef â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     adaptive/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             env/base_value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           env/reward_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                        +12 ...
wandb: 
wandb: Run summary:
wandb: adaptive/adaptation_factor 1
wandb:   adaptive/base_clip_range 0.2
wandb:           adaptive/base_lr 0.0003
wandb:        adaptive/clip_range 0.2
wandb:   adaptive/drift_magnitude 0
wandb:          adaptive/ent_coef 0.01
wandb:     adaptive/learning_rate 0.0003
wandb:             env/base_value 9.8
wandb:           env/reward_scale 9.8
wandb:                global_step 100352
wandb:                        +12 ...
wandb: 
wandb: ðŸš€ View run MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124 at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/ckhe5oz3
wandb: â­ï¸ View project at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: logs/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124/wandb/run-20251218_102801-ckhe5oz3/logs
>>> [DriftAdaptiveCallback] Training Ended
    Final LR: 0.000300
    Last Drift Magnitude: 0.0000
    Final Clip Range: 0.2000
Model saved locally to: models/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124.zip
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at /home/hungchan/Work/Deep-RL/videos/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: random_walk
Loading PPO model from: models/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124.zip

Recording Episode 1/1...
  Episode finished: 11 steps, reward = 1.0

Videos saved to: videos/MiniGrid_Empty-8x8_reward_scale_random_walk_Adaptive_20251218_101124
