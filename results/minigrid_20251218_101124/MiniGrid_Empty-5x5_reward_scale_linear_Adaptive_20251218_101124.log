wandb: Currently logged in as: hungtrab (hungtrab-hanoi-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run z1cpksky
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in logs/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124/wandb/run-20251218_102313-z1cpksky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124
wandb: â­ï¸ View project at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: ðŸš€ View run at https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/z1cpksky
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
--- Training Start: MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124 ---
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: linear
>>> Initializing PPO with kwargs: ['policy', 'env', 'learning_rate', 'gamma', 'verbose', 'tensorboard_log', 'n_steps', 'batch_size']
Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Logging to logs/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124_0
>>> [DriftAdaptiveCallback] Training Started
    Algorithm: PPO
    Target Param: reward_scale (base=9.8)
    Scale Factor: 0.15
    
    Adaptive Hyperparameters:
      - Learning Rate: 0.000300
      - Clip Range: 0.200 (adapt=True)
      - Entropy Coef: 0.0100 (adapt=True)
-----------------------------------
| adaptive/            |          |
|    adaptation_factor | 1        |
|    algorithm         | PPO      |
|    base_clip_range   | 0.2      |
|    base_lr           | 0.0003   |
|    clip_range        | 0.2      |
|    drift_magnitude   | 0        |
|    ent_coef          | 0.01     |
|    learning_rate     | 0.0003   |
| env/                 |          |
|    base_value        | 9.8      |
|    reward_scale      | 9.8      |
| rollout/             |          |
|    ep_len_mean       | 91.2     |
|    ep_rew_mean       | 0.0995   |
| time/                |          |
|    fps               | 674      |
|    iterations        | 1        |
|    time_elapsed      | 0        |
|    total_timesteps   | 512      |
-----------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 92.8          |
|    ep_rew_mean          | 0.0927        |
| time/                   |               |
|    fps                  | 674           |
|    iterations           | 2             |
|    time_elapsed         | 1             |
|    total_timesteps      | 1024          |
| train/                  |               |
|    approx_kl            | 0.00048762036 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.95         |
|    explained_variance   | 0.0144        |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0223       |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.000859     |
|    value_loss           | 0.00409       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 83.6          |
|    ep_rew_mean          | 0.19          |
| time/                   |               |
|    fps                  | 712           |
|    iterations           | 3             |
|    time_elapsed         | 2             |
|    total_timesteps      | 1536          |
| train/                  |               |
|    approx_kl            | 0.00077597157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.95         |
|    explained_variance   | -0.0928       |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0168       |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00151      |
|    value_loss           | 0.00261       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 83.9         |
|    ep_rew_mean          | 0.186        |
| time/                   |              |
|    fps                  | 721          |
|    iterations           | 4            |
|    time_elapsed         | 2            |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0070081786 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.0161       |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0315      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00659     |
|    value_loss           | 0.0217       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 78.3        |
|    ep_rew_mean          | 0.247       |
| time/                   |             |
|    fps                  | 729         |
|    iterations           | 5           |
|    time_elapsed         | 3           |
|    total_timesteps      | 2560        |
| train/                  |             |
|    approx_kl            | 0.015462814 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.0257      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0503     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.00546     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 78.3        |
|    ep_rew_mean          | 0.251       |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 6           |
|    time_elapsed         | 4           |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.013697488 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.0251      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.043      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0238      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 74.8        |
|    ep_rew_mean          | 0.288       |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 7           |
|    time_elapsed         | 4           |
|    total_timesteps      | 3584        |
| train/                  |             |
|    approx_kl            | 0.008023667 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.099       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.033      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00537    |
|    value_loss           | 0.00794     |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 73          |
|    ep_rew_mean          | 0.311       |
| time/                   |             |
|    fps                  | 744         |
|    iterations           | 8           |
|    time_elapsed         | 5           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011440189 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.0257      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0369     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0087     |
|    value_loss           | 0.0238      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 62.9        |
|    ep_rew_mean          | 0.422       |
| time/                   |             |
|    fps                  | 746         |
|    iterations           | 9           |
|    time_elapsed         | 6           |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.016787026 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0407     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 0.0153      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 58.6        |
|    ep_rew_mean          | 0.472       |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 10          |
|    time_elapsed         | 6           |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.016970918 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.116       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00282    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.049       |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 57.2         |
|    ep_rew_mean          | 0.49         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 11           |
|    time_elapsed         | 7            |
|    total_timesteps      | 5632         |
| train/                  |              |
|    approx_kl            | 0.0076518506 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0127      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00486     |
|    value_loss           | 0.0288       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 53.8        |
|    ep_rew_mean          | 0.53        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 12          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.008682521 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.139       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00521    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00807    |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 46.9        |
|    ep_rew_mean          | 0.613       |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 13          |
|    time_elapsed         | 8           |
|    total_timesteps      | 6656        |
| train/                  |             |
|    approx_kl            | 0.010289644 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.028      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 0.0184      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 46.4        |
|    ep_rew_mean          | 0.624       |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 14          |
|    time_elapsed         | 9           |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.007816849 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0269     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00969    |
|    value_loss           | 0.0262      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 41.5        |
|    ep_rew_mean          | 0.686       |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 15          |
|    time_elapsed         | 10          |
|    total_timesteps      | 7680        |
| train/                  |             |
|    approx_kl            | 0.013079623 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | -0.0693     |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0201     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 0.0088      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 39.9         |
|    ep_rew_mean          | 0.711        |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 16           |
|    time_elapsed         | 10           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0054402435 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.43        |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0257      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00345     |
|    value_loss           | 0.0237       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 42.1        |
|    ep_rew_mean          | 0.694       |
| time/                   |             |
|    fps                  | 766         |
|    iterations           | 17          |
|    time_elapsed         | 11          |
|    total_timesteps      | 8704        |
| train/                  |             |
|    approx_kl            | 0.009065945 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0122     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.0177      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 38.6        |
|    ep_rew_mean          | 0.744       |
| time/                   |             |
|    fps                  | 764         |
|    iterations           | 18          |
|    time_elapsed         | 12          |
|    total_timesteps      | 9216        |
| train/                  |             |
|    approx_kl            | 0.006954672 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0886      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0154     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 0.0249      |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 36.2       |
|    ep_rew_mean          | 0.779      |
| time/                   |            |
|    fps                  | 765        |
|    iterations           | 19         |
|    time_elapsed         | 12         |
|    total_timesteps      | 9728       |
| train/                  |            |
|    approx_kl            | 0.01211396 |
|    clip_fraction        | 0.0789     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.177      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0115     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 0.0399     |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 37.1         |
|    ep_rew_mean          | 0.774        |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 20           |
|    time_elapsed         | 13           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0050247214 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00216      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 0.028        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 32.1        |
|    ep_rew_mean          | 0.839       |
| time/                   |             |
|    fps                  | 763         |
|    iterations           | 21          |
|    time_elapsed         | 14          |
|    total_timesteps      | 10752       |
| train/                  |             |
|    approx_kl            | 0.005992924 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0015     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00614    |
|    value_loss           | 0.0143      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 31.4         |
|    ep_rew_mean          | 0.848        |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 22           |
|    time_elapsed         | 14           |
|    total_timesteps      | 11264        |
| train/                  |              |
|    approx_kl            | 0.0053912434 |
|    clip_fraction        | 0.0934       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0297      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.0289       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 29.1         |
|    ep_rew_mean          | 0.882        |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 23           |
|    time_elapsed         | 15           |
|    total_timesteps      | 11776        |
| train/                  |              |
|    approx_kl            | 0.0030399263 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.995       |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00489      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 0.0212       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 32.4         |
|    ep_rew_mean          | 0.848        |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 24           |
|    time_elapsed         | 16           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0030297628 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00978      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 0.0269       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 34.1        |
|    ep_rew_mean          | 0.849       |
| time/                   |             |
|    fps                  | 762         |
|    iterations           | 25          |
|    time_elapsed         | 16          |
|    total_timesteps      | 12800       |
| train/                  |             |
|    approx_kl            | 0.005842422 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00291    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00671    |
|    value_loss           | 0.0197      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 31.7         |
|    ep_rew_mean          | 0.926        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 26           |
|    time_elapsed         | 17           |
|    total_timesteps      | 13312        |
| train/                  |              |
|    approx_kl            | 0.0041694073 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.927       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00934     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 0.0433       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 31.2         |
|    ep_rew_mean          | 0.983        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 27           |
|    time_elapsed         | 18           |
|    total_timesteps      | 13824        |
| train/                  |              |
|    approx_kl            | 0.0022787855 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.964       |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0078       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 0.0584       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 29.1        |
|    ep_rew_mean          | 1.05        |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 28          |
|    time_elapsed         | 18          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.004139148 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0086      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00515    |
|    value_loss           | 0.0356      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 28.9         |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 762          |
|    iterations           | 29           |
|    time_elapsed         | 19           |
|    total_timesteps      | 14848        |
| train/                  |              |
|    approx_kl            | 0.0042721676 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.936       |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0205      |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.0072      |
|    value_loss           | 0.0187       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 24.7         |
|    ep_rew_mean          | 1.14         |
| time/                   |              |
|    fps                  | 763          |
|    iterations           | 30           |
|    time_elapsed         | 20           |
|    total_timesteps      | 15360        |
| train/                  |              |
|    approx_kl            | 0.0032552255 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.987       |
|    explained_variance   | 0.435        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.002        |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00495     |
|    value_loss           | 0.0298       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 25.3         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 764          |
|    iterations           | 31           |
|    time_elapsed         | 20           |
|    total_timesteps      | 15872        |
| train/                  |              |
|    approx_kl            | 0.0020406973 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.9         |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0136       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00236     |
|    value_loss           | 0.0445       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 24.5         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 765          |
|    iterations           | 32           |
|    time_elapsed         | 21           |
|    total_timesteps      | 16384        |
| train/                  |              |
|    approx_kl            | 0.0033679644 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.812       |
|    explained_variance   | 0.0573       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00565      |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0055      |
|    value_loss           | 0.0367       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 24.7         |
|    ep_rew_mean          | 1.11         |
| time/                   |              |
|    fps                  | 765          |
|    iterations           | 33           |
|    time_elapsed         | 22           |
|    total_timesteps      | 16896        |
| train/                  |              |
|    approx_kl            | 0.0015317983 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.848       |
|    explained_variance   | 0.379        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00122     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 0.035        |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 25.9        |
|    ep_rew_mean          | 1.09        |
| time/                   |             |
|    fps                  | 766         |
|    iterations           | 34          |
|    time_elapsed         | 22          |
|    total_timesteps      | 17408       |
| train/                  |             |
|    approx_kl            | 0.004553072 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.014      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00754    |
|    value_loss           | 0.03        |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 24.6        |
|    ep_rew_mean          | 1.1         |
| time/                   |             |
|    fps                  | 767         |
|    iterations           | 35          |
|    time_elapsed         | 23          |
|    total_timesteps      | 17920       |
| train/                  |             |
|    approx_kl            | 0.002888998 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00911    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 0.0329      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 24.7         |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 767          |
|    iterations           | 36           |
|    time_elapsed         | 24           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0031409217 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1           |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.013        |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00354     |
|    value_loss           | 0.0335       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 26.9         |
|    ep_rew_mean          | 1.06         |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 37           |
|    time_elapsed         | 24           |
|    total_timesteps      | 18944        |
| train/                  |              |
|    approx_kl            | 0.0039282124 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.927       |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0068       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00786     |
|    value_loss           | 0.0257       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 25.5         |
|    ep_rew_mean          | 1.07         |
| time/                   |              |
|    fps                  | 765          |
|    iterations           | 38           |
|    time_elapsed         | 25           |
|    total_timesteps      | 19456        |
| train/                  |              |
|    approx_kl            | 0.0034673053 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.863       |
|    explained_variance   | 0.0616       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00279      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0051      |
|    value_loss           | 0.0275       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 23.3         |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 765          |
|    iterations           | 39           |
|    time_elapsed         | 26           |
|    total_timesteps      | 19968        |
| train/                  |              |
|    approx_kl            | 0.0034414656 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.895       |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00338     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00488     |
|    value_loss           | 0.0264       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 19.7         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 40           |
|    time_elapsed         | 26           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0017385597 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.893       |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00709      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00605     |
|    value_loss           | 0.0366       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 1.15         |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 41           |
|    time_elapsed         | 27           |
|    total_timesteps      | 20992        |
| train/                  |              |
|    approx_kl            | 0.0022234702 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.782       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00141      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.0335       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13.4         |
|    ep_rew_mean          | 1.17         |
| time/                   |              |
|    fps                  | 766          |
|    iterations           | 42           |
|    time_elapsed         | 28           |
|    total_timesteps      | 21504        |
| train/                  |              |
|    approx_kl            | 0.0032580728 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.842       |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000734     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.0074      |
|    value_loss           | 0.0183       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 13.8        |
|    ep_rew_mean          | 1.16        |
| time/                   |             |
|    fps                  | 766         |
|    iterations           | 43          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22016       |
| train/                  |             |
|    approx_kl            | 0.012137275 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0317     |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.0198      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 13           |
|    ep_rew_mean          | 1.16         |
| time/                   |              |
|    fps                  | 767          |
|    iterations           | 44           |
|    time_elapsed         | 29           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0084742885 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.945       |
|    explained_variance   | 0.384        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.013       |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.0101      |
|    value_loss           | 0.0173       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 9.54         |
|    ep_rew_mean          | 1.19         |
| time/                   |              |
|    fps                  | 767          |
|    iterations           | 45           |
|    time_elapsed         | 30           |
|    total_timesteps      | 23040        |
| train/                  |              |
|    approx_kl            | 0.0052002044 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.864       |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0112      |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.0159       |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 7.46        |
|    ep_rew_mean          | 1.2         |
| time/                   |             |
|    fps                  | 768         |
|    iterations           | 46          |
|    time_elapsed         | 30          |
|    total_timesteps      | 23552       |
| train/                  |             |
|    approx_kl            | 0.009530198 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0237     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.0069      |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 7.22        |
|    ep_rew_mean          | 1.19        |
| time/                   |             |
|    fps                  | 767         |
|    iterations           | 47          |
|    time_elapsed         | 31          |
|    total_timesteps      | 24064       |
| train/                  |             |
|    approx_kl            | 0.009265593 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0353     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.00424     |
-----------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 6.32       |
|    ep_rew_mean          | 1.19       |
| time/                   |            |
|    fps                  | 763        |
|    iterations           | 48         |
|    time_elapsed         | 32         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01138931 |
|    clip_fraction        | 0.0633     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0178    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.00406    |
----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.97         |
|    ep_rew_mean          | 1.15         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 49           |
|    time_elapsed         | 32           |
|    total_timesteps      | 25088        |
| train/                  |              |
|    approx_kl            | 0.0064961873 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0186      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 0.00307      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.95         |
|    ep_rew_mean          | 0.952        |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 50           |
|    time_elapsed         | 33           |
|    total_timesteps      | 25600        |
| train/                  |              |
|    approx_kl            | 0.0016929811 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | -0.0171      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0181      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 0.00847      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.51         |
|    ep_rew_mean          | 0.966        |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 51           |
|    time_elapsed         | 34           |
|    total_timesteps      | 26112        |
| train/                  |              |
|    approx_kl            | 0.0015967495 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.00653      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0105      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00676     |
|    value_loss           | 0.00329      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.35         |
|    ep_rew_mean          | 0.978        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 52           |
|    time_elapsed         | 35           |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0012264707 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0.521        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00891     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.0081      |
|    value_loss           | 0.000799     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.31         |
|    ep_rew_mean          | 0.988        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 53           |
|    time_elapsed         | 35           |
|    total_timesteps      | 27136        |
| train/                  |              |
|    approx_kl            | 0.0013812614 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.341        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.028       |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 0.000384     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.21          |
|    ep_rew_mean          | 0.999         |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 54            |
|    time_elapsed         | 36            |
|    total_timesteps      | 27648         |
| train/                  |               |
|    approx_kl            | 0.00044521864 |
|    clip_fraction        | 0.00781       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.149        |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0151       |
|    n_updates            | 530           |
|    policy_gradient_loss | -0.00496      |
|    value_loss           | 0.000618      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.2          |
|    ep_rew_mean          | 1.01         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 55           |
|    time_elapsed         | 37           |
|    total_timesteps      | 28160        |
| train/                  |              |
|    approx_kl            | 0.0009479263 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.729        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00106      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00578     |
|    value_loss           | 0.000206     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.15          |
|    ep_rew_mean          | 1.02          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 56            |
|    time_elapsed         | 37            |
|    total_timesteps      | 28672         |
| train/                  |               |
|    approx_kl            | 0.00080648344 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0988       |
|    explained_variance   | 0.529         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0229       |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.00616      |
|    value_loss           | 0.000373      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.07         |
|    ep_rew_mean          | 1.03         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 57           |
|    time_elapsed         | 38           |
|    total_timesteps      | 29184        |
| train/                  |              |
|    approx_kl            | 0.0005702431 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0769      |
|    explained_variance   | 0.596        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00858     |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00391     |
|    value_loss           | 0.000144     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.15          |
|    ep_rew_mean          | 1.04          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 58            |
|    time_elapsed         | 39            |
|    total_timesteps      | 29696         |
| train/                  |               |
|    approx_kl            | 0.00068386237 |
|    clip_fraction        | 0.00996       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0618       |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0109       |
|    n_updates            | 570           |
|    policy_gradient_loss | -0.0076       |
|    value_loss           | 2.99e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.04         |
|    ep_rew_mean          | 1.05         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 59           |
|    time_elapsed         | 39           |
|    total_timesteps      | 30208        |
| train/                  |              |
|    approx_kl            | 0.0003284762 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.057       |
|    explained_variance   | 0.487        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00514     |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.000474     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.34         |
|    ep_rew_mean          | 1.06         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 60           |
|    time_elapsed         | 40           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0037101507 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0603      |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00321     |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00623     |
|    value_loss           | 3.31e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.56          |
|    ep_rew_mean          | 1.06          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 61            |
|    time_elapsed         | 41            |
|    total_timesteps      | 31232         |
| train/                  |               |
|    approx_kl            | 0.00046522624 |
|    clip_fraction        | 0.00645       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0747       |
|    explained_variance   | 0.412         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0185       |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.00483      |
|    value_loss           | 0.00104       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.1          |
|    ep_rew_mean          | 1.08         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 62           |
|    time_elapsed         | 41           |
|    total_timesteps      | 31744        |
| train/                  |              |
|    approx_kl            | 0.0066386415 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0882      |
|    explained_variance   | 0.531        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0121      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.00334      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.09          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 63            |
|    time_elapsed         | 42            |
|    total_timesteps      | 32256         |
| train/                  |               |
|    approx_kl            | 0.00026437477 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.044        |
|    explained_variance   | 0.501         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000223     |
|    n_updates            | 620           |
|    policy_gradient_loss | -0.00371      |
|    value_loss           | 0.000391      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.16          |
|    ep_rew_mean          | 1.1           |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 64            |
|    time_elapsed         | 43            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 5.4587144e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0391       |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000427     |
|    n_updates            | 630           |
|    policy_gradient_loss | -3.2e-05      |
|    value_loss           | 1.49e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.03         |
|    ep_rew_mean          | 1.11         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 65           |
|    time_elapsed         | 43           |
|    total_timesteps      | 33280        |
| train/                  |              |
|    approx_kl            | 0.0004122276 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0434      |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00392     |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00362     |
|    value_loss           | 0.000346     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.02         |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 66           |
|    time_elapsed         | 44           |
|    total_timesteps      | 33792        |
| train/                  |              |
|    approx_kl            | 0.0006186927 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0317      |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000541    |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00474     |
|    value_loss           | 2.32e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.02          |
|    ep_rew_mean          | 1.13          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 67            |
|    time_elapsed         | 45            |
|    total_timesteps      | 34304         |
| train/                  |               |
|    approx_kl            | 0.00047609815 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0287       |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0215       |
|    n_updates            | 660           |
|    policy_gradient_loss | -0.00494      |
|    value_loss           | 3.05e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.28         |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 68           |
|    time_elapsed         | 45           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0005466698 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0264      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0162      |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 1.41e-05     |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 5.03       |
|    ep_rew_mean          | 1.15       |
| time/                   |            |
|    fps                  | 759        |
|    iterations           | 69         |
|    time_elapsed         | 46         |
|    total_timesteps      | 35328      |
| train/                  |            |
|    approx_kl            | 0.00099225 |
|    clip_fraction        | 0.0221     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0431    |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0347    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.00795   |
|    value_loss           | 0.000634   |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.16          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 70            |
|    time_elapsed         | 47            |
|    total_timesteps      | 35840         |
| train/                  |               |
|    approx_kl            | 0.00030326506 |
|    clip_fraction        | 0.00508       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0155       |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.00469      |
|    value_loss           | 3.42e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.17          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 71            |
|    time_elapsed         | 47            |
|    total_timesteps      | 36352         |
| train/                  |               |
|    approx_kl            | 0.00025545724 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0253       |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000182     |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.00181      |
|    value_loss           | 2.23e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.18          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 72            |
|    time_elapsed         | 48            |
|    total_timesteps      | 36864         |
| train/                  |               |
|    approx_kl            | 0.00019339705 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0221       |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000228     |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.00186      |
|    value_loss           | 1.78e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.05          |
|    ep_rew_mean          | 1.19          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 73            |
|    time_elapsed         | 49            |
|    total_timesteps      | 37376         |
| train/                  |               |
|    approx_kl            | 4.1704858e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0222       |
|    explained_variance   | 0.611         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000214     |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.000589     |
|    value_loss           | 0.000162      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.37          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 74            |
|    time_elapsed         | 49            |
|    total_timesteps      | 37888         |
| train/                  |               |
|    approx_kl            | 8.6694025e-05 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0207       |
|    explained_variance   | 0.737         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00011      |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.00169      |
|    value_loss           | 0.000107      |
-------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.42           |
| time/                   |                |
|    fps                  | 760            |
|    iterations           | 75             |
|    time_elapsed         | 50             |
|    total_timesteps      | 38400          |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.018         |
|    explained_variance   | 0.0832         |
|    learning_rate        | 0.0003         |
|    loss                 | 0.00426        |
|    n_updates            | 740            |
|    policy_gradient_loss | -5.56e-07      |
|    value_loss           | 0.00997        |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.04         |
|    ep_rew_mean          | 1.41         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 76           |
|    time_elapsed         | 51           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 4.459021e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0269      |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00352     |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.000688    |
|    value_loss           | 0.000225     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 6.36        |
|    ep_rew_mean          | 1.38        |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 77          |
|    time_elapsed         | 51          |
|    total_timesteps      | 39424       |
| train/                  |             |
|    approx_kl            | 0.017319439 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.043      |
|    n_updates            | 760         |
|    policy_gradient_loss | 0.0436      |
|    value_loss           | 0.000129    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 7.51         |
|    ep_rew_mean          | 1.36         |
| time/                   |              |
|    fps                  | 761          |
|    iterations           | 78           |
|    time_elapsed         | 52           |
|    total_timesteps      | 39936        |
| train/                  |              |
|    approx_kl            | 0.0029744464 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | -0.163       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00328      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0185      |
|    value_loss           | 0.00433      |
------------------------------------------
----------------------------------------
| adaptive/               |            |
|    adaptation_factor    | 1          |
|    algorithm            | PPO        |
|    base_clip_range      | 0.2        |
|    base_lr              | 0.0003     |
|    clip_range           | 0.2        |
|    drift_magnitude      | 0          |
|    ent_coef             | 0.01       |
|    learning_rate        | 0.0003     |
| env/                    |            |
|    base_value           | 9.8        |
|    reward_scale         | 9.8        |
| rollout/                |            |
|    ep_len_mean          | 5.08       |
|    ep_rew_mean          | 1.38       |
| time/                   |            |
|    fps                  | 761        |
|    iterations           | 79         |
|    time_elapsed         | 53         |
|    total_timesteps      | 40448      |
| train/                  |            |
|    approx_kl            | 0.16972023 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0351    |
|    n_updates            | 780        |
|    policy_gradient_loss | -6.09e-05  |
|    value_loss           | 0.00618    |
----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.14          |
|    ep_rew_mean          | 1.37          |
| time/                   |               |
|    fps                  | 761           |
|    iterations           | 80            |
|    time_elapsed         | 53            |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00036127726 |
|    clip_fraction        | 0.00332       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | 0.632         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0197       |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.00347      |
|    value_loss           | 0.000213      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5.01        |
|    ep_rew_mean          | 1.36        |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 81          |
|    time_elapsed         | 54          |
|    total_timesteps      | 41472       |
| train/                  |             |
|    approx_kl            | 7.86708e-05 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0184     |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00033     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 0.000615    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.17          |
|    ep_rew_mean          | 1.35          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 82            |
|    time_elapsed         | 55            |
|    total_timesteps      | 41984         |
| train/                  |               |
|    approx_kl            | 0.00011717179 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.0003        |
|    loss                 | -1.17e-07     |
|    n_updates            | 810           |
|    policy_gradient_loss | -0.00175      |
|    value_loss           | 1.1e-05       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.01         |
|    ep_rew_mean          | 1.34         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 83           |
|    time_elapsed         | 55           |
|    total_timesteps      | 42496        |
| train/                  |              |
|    approx_kl            | 0.0013785731 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0256      |
|    explained_variance   | 0.58         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0138      |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 0.00112      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.14          |
|    ep_rew_mean          | 1.33          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 84            |
|    time_elapsed         | 56            |
|    total_timesteps      | 43008         |
| train/                  |               |
|    approx_kl            | 0.00014011248 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0135       |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000419     |
|    n_updates            | 830           |
|    policy_gradient_loss | -0.00179      |
|    value_loss           | 1.5e-05       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.32          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 85            |
|    time_elapsed         | 57            |
|    total_timesteps      | 43520         |
| train/                  |               |
|    approx_kl            | 1.8021208e-05 |
|    clip_fraction        | 0.0105        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0254       |
|    explained_variance   | 0.751         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.000188      |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 0.000543      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.31          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 86            |
|    time_elapsed         | 57            |
|    total_timesteps      | 44032         |
| train/                  |               |
|    approx_kl            | 4.1909516e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000143     |
|    n_updates            | 850           |
|    policy_gradient_loss | 1e-05         |
|    value_loss           | 7.94e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.3          |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 87           |
|    time_elapsed         | 58           |
|    total_timesteps      | 44544        |
| train/                  |              |
|    approx_kl            | 3.760215e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0139      |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000166    |
|    n_updates            | 860          |
|    policy_gradient_loss | -1.87e-06    |
|    value_loss           | 1.21e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.29         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 88           |
|    time_elapsed         | 59           |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 1.557637e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000195    |
|    n_updates            | 870          |
|    policy_gradient_loss | -7.74e-06    |
|    value_loss           | 1.31e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.28          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 89            |
|    time_elapsed         | 59            |
|    total_timesteps      | 45568         |
| train/                  |               |
|    approx_kl            | 3.1851232e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00019      |
|    n_updates            | 880           |
|    policy_gradient_loss | -4.78e-06     |
|    value_loss           | 1.19e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.27          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 90            |
|    time_elapsed         | 60            |
|    total_timesteps      | 46080         |
| train/                  |               |
|    approx_kl            | 0.00022179435 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0.756         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.58e-06     |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.00207      |
|    value_loss           | 0.000106      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.2           |
|    ep_rew_mean          | 1.26          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 91            |
|    time_elapsed         | 61            |
|    total_timesteps      | 46592         |
| train/                  |               |
|    approx_kl            | 1.0902528e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 0.754         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000152     |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 0.000102      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.25         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 92           |
|    time_elapsed         | 61           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0034424248 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | 0.565        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0203      |
|    n_updates            | 910          |
|    policy_gradient_loss | 0.000252     |
|    value_loss           | 0.00108      |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.05          |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 93            |
|    time_elapsed         | 62            |
|    total_timesteps      | 47616         |
| train/                  |               |
|    approx_kl            | 1.3504177e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.82e-05     |
|    n_updates            | 920           |
|    policy_gradient_loss | 2.09e-07      |
|    value_loss           | 7.37e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.23         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 94           |
|    time_elapsed         | 63           |
|    total_timesteps      | 48128        |
| train/                  |              |
|    approx_kl            | 0.0002548258 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00641     |
|    n_updates            | 930          |
|    policy_gradient_loss | 0.000282     |
|    value_loss           | 0.000161     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.22          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 95            |
|    time_elapsed         | 63            |
|    total_timesteps      | 48640         |
| train/                  |               |
|    approx_kl            | 2.3515895e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0087       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.94e-05     |
|    n_updates            | 940           |
|    policy_gradient_loss | -1.22e-07     |
|    value_loss           | 1.08e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.21          |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 96            |
|    time_elapsed         | 64            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 1.9790605e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0091       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.53e-05     |
|    n_updates            | 950           |
|    policy_gradient_loss | -1.42e-06     |
|    value_loss           | 1.24e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.2           |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 97            |
|    time_elapsed         | 65            |
|    total_timesteps      | 49664         |
| train/                  |               |
|    approx_kl            | 0.00054868904 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.0003        |
|    loss                 | -8.86e-05     |
|    n_updates            | 960           |
|    policy_gradient_loss | -0.00177      |
|    value_loss           | 1.53e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.11         |
| time/                   |              |
|    fps                  | 760          |
|    iterations           | 98           |
|    time_elapsed         | 65           |
|    total_timesteps      | 50176        |
| train/                  |              |
|    approx_kl            | 3.365567e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0135      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.26e-05    |
|    n_updates            | 970          |
|    policy_gradient_loss | -1.21e-05    |
|    value_loss           | 1.35e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 0.963         |
| time/                   |               |
|    fps                  | 760           |
|    iterations           | 99            |
|    time_elapsed         | 66            |
|    total_timesteps      | 50688         |
| train/                  |               |
|    approx_kl            | 1.7568236e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0193       |
|    explained_variance   | 0.00565       |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00585       |
|    n_updates            | 980           |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 0.011         |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.22         |
|    ep_rew_mean          | 0.971        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 100          |
|    time_elapsed         | 67           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.0002297943 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | -0.374       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00012      |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000531    |
|    value_loss           | 0.0012       |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 0.983        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 101          |
|    time_elapsed         | 68           |
|    total_timesteps      | 51712        |
| train/                  |              |
|    approx_kl            | 0.0006007239 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0168      |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000364    |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00652     |
|    value_loss           | 0.00035      |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.04         |
|    ep_rew_mean          | 0.992        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 102          |
|    time_elapsed         | 68           |
|    total_timesteps      | 52224        |
| train/                  |              |
|    approx_kl            | 1.384411e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00749     |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000774    |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000451    |
|    value_loss           | 2.42e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 103         |
|    time_elapsed         | 69          |
|    total_timesteps      | 52736       |
| train/                  |             |
|    approx_kl            | 0.000368568 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.005      |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000114   |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 5.7e-05     |
-----------------------------------------
---------------------------------------
| adaptive/               |           |
|    adaptation_factor    | 1         |
|    algorithm            | PPO       |
|    base_clip_range      | 0.2       |
|    base_lr              | 0.0003    |
|    clip_range           | 0.2       |
|    drift_magnitude      | 0         |
|    ent_coef             | 0.01      |
|    learning_rate        | 0.0003    |
| env/                    |           |
|    base_value           | 9.8       |
|    reward_scale         | 9.8       |
| rollout/                |           |
|    ep_len_mean          | 5         |
|    ep_rew_mean          | 1.01      |
| time/                   |           |
|    fps                  | 759       |
|    iterations           | 104       |
|    time_elapsed         | 70        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00481  |
|    explained_variance   | 0.968     |
|    learning_rate        | 0.0003    |
|    loss                 | -4.19e-05 |
|    n_updates            | 1030      |
|    policy_gradient_loss | -1.64e-08 |
|    value_loss           | 1.45e-05  |
---------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.02         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 105          |
|    time_elapsed         | 70           |
|    total_timesteps      | 53760        |
| train/                  |              |
|    approx_kl            | 5.820766e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00487     |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0003       |
|    loss                 | -5.24e-05    |
|    n_updates            | 1040         |
|    policy_gradient_loss | 6.62e-09     |
|    value_loss           | 1.23e-05     |
------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5.01           |
|    ep_rew_mean          | 1.03           |
| time/                   |                |
|    fps                  | 759            |
|    iterations           | 106            |
|    time_elapsed         | 71             |
|    total_timesteps      | 54272          |
| train/                  |                |
|    approx_kl            | -2.1187589e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00488       |
|    explained_variance   | 0.972          |
|    learning_rate        | 0.0003         |
|    loss                 | -4.3e-05       |
|    n_updates            | 1050           |
|    policy_gradient_loss | 4.4e-08        |
|    value_loss           | 1.23e-05       |
--------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.04          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 107           |
|    time_elapsed         | 72            |
|    total_timesteps      | 54784         |
| train/                  |               |
|    approx_kl            | 0.00021708035 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00499      |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.78e-05     |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00176      |
|    value_loss           | 1.37e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.05          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 108           |
|    time_elapsed         | 72            |
|    total_timesteps      | 55296         |
| train/                  |               |
|    approx_kl            | 2.2700988e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00513      |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.69e-05     |
|    n_updates            | 1070          |
|    policy_gradient_loss | 3.03e-08      |
|    value_loss           | 1.34e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.06          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 109           |
|    time_elapsed         | 73            |
|    total_timesteps      | 55808         |
| train/                  |               |
|    approx_kl            | 1.1175871e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00527      |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.07e-05     |
|    n_updates            | 1080          |
|    policy_gradient_loss | 1.79e-07      |
|    value_loss           | 1.31e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.07         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 110          |
|    time_elapsed         | 74           |
|    total_timesteps      | 56320        |
| train/                  |              |
|    approx_kl            | 7.334619e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0056      |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00312     |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.000981    |
|    value_loss           | 9.03e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.08          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 111           |
|    time_elapsed         | 74            |
|    total_timesteps      | 56832         |
| train/                  |               |
|    approx_kl            | 2.9685907e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00484      |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0003        |
|    loss                 | -4e-05        |
|    n_updates            | 1100          |
|    policy_gradient_loss | 5.2e-06       |
|    value_loss           | 1.26e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.09          |
| time/                   |               |
|    fps                  | 759           |
|    iterations           | 112           |
|    time_elapsed         | 75            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 2.8288923e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00501      |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.5e-05      |
|    n_updates            | 1110          |
|    policy_gradient_loss | 2.25e-07      |
|    value_loss           | 1.21e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.1          |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 113          |
|    time_elapsed         | 76           |
|    total_timesteps      | 57856        |
| train/                  |              |
|    approx_kl            | 0.0005705714 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00786     |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000118    |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 1.55e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.11          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 114           |
|    time_elapsed         | 76            |
|    total_timesteps      | 58368         |
| train/                  |               |
|    approx_kl            | 1.2095552e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00999      |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000115     |
|    n_updates            | 1130          |
|    policy_gradient_loss | 7.53e-07      |
|    value_loss           | 1.19e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.08          |
|    ep_rew_mean          | 1.12          |
| time/                   |               |
|    fps                  | 758           |
|    iterations           | 115           |
|    time_elapsed         | 77            |
|    total_timesteps      | 58880         |
| train/                  |               |
|    approx_kl            | 2.9068906e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000128     |
|    n_updates            | 1140          |
|    policy_gradient_loss | 5.92e-06      |
|    value_loss           | 1.35e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.13         |
| time/                   |              |
|    fps                  | 758          |
|    iterations           | 116          |
|    time_elapsed         | 78           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0020484496 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00989     |
|    explained_variance   | 0.583        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0179      |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.000181     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.14          |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 117           |
|    time_elapsed         | 79            |
|    total_timesteps      | 59904         |
| train/                  |               |
|    approx_kl            | 4.8195943e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00515      |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00011      |
|    n_updates            | 1160          |
|    policy_gradient_loss | -2.86e-05     |
|    value_loss           | 1.46e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.15          |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 118           |
|    time_elapsed         | 79            |
|    total_timesteps      | 60416         |
| train/                  |               |
|    approx_kl            | 1.8975697e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00487      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.32e-05     |
|    n_updates            | 1170          |
|    policy_gradient_loss | -2.63e-07     |
|    value_loss           | 1.34e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.16          |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 119           |
|    time_elapsed         | 80            |
|    total_timesteps      | 60928         |
| train/                  |               |
|    approx_kl            | 2.2933818e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00535      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.12e-05     |
|    n_updates            | 1180          |
|    policy_gradient_loss | 1.65e-07      |
|    value_loss           | 1.34e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.17         |
| time/                   |              |
|    fps                  | 757          |
|    iterations           | 120          |
|    time_elapsed         | 81           |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 2.574257e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00503     |
|    explained_variance   | 0.813        |
|    learning_rate        | 0.0003       |
|    loss                 | -4.88e-05    |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 7.69e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.18          |
| time/                   |               |
|    fps                  | 757           |
|    iterations           | 121           |
|    time_elapsed         | 81            |
|    total_timesteps      | 61952         |
| train/                  |               |
|    approx_kl            | 1.0244548e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00372      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.03e-05     |
|    n_updates            | 1200          |
|    policy_gradient_loss | -1.41e-05     |
|    value_loss           | 1.4e-05       |
-------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.19           |
| time/                   |                |
|    fps                  | 756            |
|    iterations           | 122            |
|    time_elapsed         | 82             |
|    total_timesteps      | 62464          |
| train/                  |                |
|    approx_kl            | -2.6775524e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00361       |
|    explained_variance   | 0.977          |
|    learning_rate        | 0.0003         |
|    loss                 | -3.23e-05      |
|    n_updates            | 1210           |
|    policy_gradient_loss | 3.31e-08       |
|    value_loss           | 1.35e-05       |
--------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.42          |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 123           |
|    time_elapsed         | 83            |
|    total_timesteps      | 62976         |
| train/                  |               |
|    approx_kl            | 1.4202669e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00372      |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.22e-05     |
|    n_updates            | 1220          |
|    policy_gradient_loss | -9.31e-08     |
|    value_loss           | 1.31e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.42          |
| time/                   |               |
|    fps                  | 756           |
|    iterations           | 124           |
|    time_elapsed         | 83            |
|    total_timesteps      | 63488         |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00383      |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00245       |
|    n_updates            | 1230          |
|    policy_gradient_loss | -1.61e-07     |
|    value_loss           | 0.00523       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.41         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 125          |
|    time_elapsed         | 84           |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 6.263144e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00442     |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0003       |
|    loss                 | -9.43e-05    |
|    n_updates            | 1240         |
|    policy_gradient_loss | -3e-05       |
|    value_loss           | 0.000126     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.24          |
|    ep_rew_mean          | 1.4           |
| time/                   |               |
|    fps                  | 755           |
|    iterations           | 126           |
|    time_elapsed         | 85            |
|    total_timesteps      | 64512         |
| train/                  |               |
|    approx_kl            | 5.7548983e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0139       |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00494      |
|    n_updates            | 1250          |
|    policy_gradient_loss | -0.00039      |
|    value_loss           | 2.27e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.16         |
|    ep_rew_mean          | 1.39         |
| time/                   |              |
|    fps                  | 755          |
|    iterations           | 127          |
|    time_elapsed         | 86           |
|    total_timesteps      | 65024        |
| train/                  |              |
|    approx_kl            | 0.0005406828 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0444      |
|    explained_variance   | 0.447        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000637    |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00668     |
|    value_loss           | 0.000486     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5.13        |
|    ep_rew_mean          | 1.38        |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 128         |
|    time_elapsed         | 86          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.008246683 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0263     |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00747    |
|    value_loss           | 0.000325    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.08         |
|    ep_rew_mean          | 1.37         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 129          |
|    time_elapsed         | 87           |
|    total_timesteps      | 66048        |
| train/                  |              |
|    approx_kl            | 0.0010440644 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0111      |
|    explained_variance   | 0.606        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000361    |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00671     |
|    value_loss           | 0.000243     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.36          |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 130           |
|    time_elapsed         | 88            |
|    total_timesteps      | 66560         |
| train/                  |               |
|    approx_kl            | 0.00040553452 |
|    clip_fraction        | 0.00371       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00792      |
|    explained_variance   | 0.704         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000128     |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.00427      |
|    value_loss           | 0.000167      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.35          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 131           |
|    time_elapsed         | 88            |
|    total_timesteps      | 67072         |
| train/                  |               |
|    approx_kl            | 0.00028722454 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00586      |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.29e-05     |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.00209      |
|    value_loss           | 0.00011       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.34          |
| time/                   |               |
|    fps                  | 754           |
|    iterations           | 132           |
|    time_elapsed         | 89            |
|    total_timesteps      | 67584         |
| train/                  |               |
|    approx_kl            | 1.7113052e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00663      |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.83e-05     |
|    n_updates            | 1310          |
|    policy_gradient_loss | 3.46e-06      |
|    value_loss           | 1.1e-05       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.33         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 133          |
|    time_elapsed         | 90           |
|    total_timesteps      | 68096        |
| train/                  |              |
|    approx_kl            | 2.789311e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00828     |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0003       |
|    loss                 | -7.78e-05    |
|    n_updates            | 1320         |
|    policy_gradient_loss | -4.95e-06    |
|    value_loss           | 1.22e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.28         |
|    ep_rew_mean          | 1.32         |
| time/                   |              |
|    fps                  | 754          |
|    iterations           | 134          |
|    time_elapsed         | 90           |
|    total_timesteps      | 68608        |
| train/                  |              |
|    approx_kl            | 4.690315e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0213      |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00102     |
|    n_updates            | 1330         |
|    policy_gradient_loss | -4.71e-05    |
|    value_loss           | 1.34e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5.04        |
|    ep_rew_mean          | 1.31        |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 135         |
|    time_elapsed         | 91          |
|    total_timesteps      | 69120       |
| train/                  |             |
|    approx_kl            | 0.012265722 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0154     |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.015      |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.0005      |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.01         |
|    ep_rew_mean          | 1.3          |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 136          |
|    time_elapsed         | 92           |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0013931639 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00686     |
|    explained_variance   | 0.856        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000398    |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 6.09e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.29         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 137          |
|    time_elapsed         | 93           |
|    total_timesteps      | 70144        |
| train/                  |              |
|    approx_kl            | 0.0011851147 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00322     |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | -3.61e-05    |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 1.88e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.28          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 138           |
|    time_elapsed         | 93            |
|    total_timesteps      | 70656         |
| train/                  |               |
|    approx_kl            | 2.8405339e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00281      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.59e-05     |
|    n_updates            | 1370          |
|    policy_gradient_loss | -3.03e-08     |
|    value_loss           | 1.26e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.27          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 139           |
|    time_elapsed         | 94            |
|    total_timesteps      | 71168         |
| train/                  |               |
|    approx_kl            | 2.0256266e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0029       |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.68e-05     |
|    n_updates            | 1380          |
|    policy_gradient_loss | -6.7e-08      |
|    value_loss           | 1.25e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.26          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 140           |
|    time_elapsed         | 95            |
|    total_timesteps      | 71680         |
| train/                  |               |
|    approx_kl            | 1.1990778e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00302      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.73e-05     |
|    n_updates            | 1390          |
|    policy_gradient_loss | -1.37e-07     |
|    value_loss           | 1.37e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.25          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 141           |
|    time_elapsed         | 95            |
|    total_timesteps      | 72192         |
| train/                  |               |
|    approx_kl            | 1.8975697e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00322      |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.69e-05     |
|    n_updates            | 1400          |
|    policy_gradient_loss | -5.8e-07      |
|    value_loss           | 1.34e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 142           |
|    time_elapsed         | 96            |
|    total_timesteps      | 72704         |
| train/                  |               |
|    approx_kl            | 1.7695129e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00348      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.33e-05     |
|    n_updates            | 1410          |
|    policy_gradient_loss | 4.28e-08      |
|    value_loss           | 1.41e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.23          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 143           |
|    time_elapsed         | 97            |
|    total_timesteps      | 73216         |
| train/                  |               |
|    approx_kl            | 2.3166649e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00379      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.99e-05     |
|    n_updates            | 1420          |
|    policy_gradient_loss | -1.67e-07     |
|    value_loss           | 1.22e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.22          |
| time/                   |               |
|    fps                  | 751           |
|    iterations           | 144           |
|    time_elapsed         | 98            |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 2.0721927e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00437      |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.79e-05     |
|    n_updates            | 1430          |
|    policy_gradient_loss | -1.2e-06      |
|    value_loss           | 1.37e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.21         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 145          |
|    time_elapsed         | 98           |
|    total_timesteps      | 74240        |
| train/                  |              |
|    approx_kl            | 6.752089e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00564     |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | -5.74e-05    |
|    n_updates            | 1440         |
|    policy_gradient_loss | -1.95e-06    |
|    value_loss           | 1.35e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.08          |
|    ep_rew_mean          | 1.2           |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 146           |
|    time_elapsed         | 99            |
|    total_timesteps      | 74752         |
| train/                  |               |
|    approx_kl            | 1.7004786e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00991      |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000268     |
|    n_updates            | 1450          |
|    policy_gradient_loss | -1.51e-05     |
|    value_loss           | 1.37e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.01         |
|    ep_rew_mean          | 1.07         |
| time/                   |              |
|    fps                  | 752          |
|    iterations           | 147          |
|    time_elapsed         | 100          |
|    total_timesteps      | 75264        |
| train/                  |              |
|    approx_kl            | 0.0010197412 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00839     |
|    explained_variance   | 0.702        |
|    learning_rate        | 0.0003       |
|    loss                 | -2.57e-05    |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00427     |
|    value_loss           | 0.000126     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 0.965         |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 148           |
|    time_elapsed         | 100           |
|    total_timesteps      | 75776         |
| train/                  |               |
|    approx_kl            | 0.00097405375 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00482      |
|    explained_variance   | -0.00402      |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00542       |
|    n_updates            | 1470          |
|    policy_gradient_loss | -0.000427     |
|    value_loss           | 0.0123        |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.08          |
|    ep_rew_mean          | 0.974         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 149           |
|    time_elapsed         | 101           |
|    total_timesteps      | 76288         |
| train/                  |               |
|    approx_kl            | 1.4248304e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0092       |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.0021       |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 0.000867      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 0.985       |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 150         |
|    time_elapsed         | 101         |
|    total_timesteps      | 76800       |
| train/                  |             |
|    approx_kl            | 0.008224593 |
|    clip_fraction        | 0.00371     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00878    |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00343    |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 0.000112    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 0.994         |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 151           |
|    time_elapsed         | 102           |
|    total_timesteps      | 77312         |
| train/                  |               |
|    approx_kl            | 5.1688403e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00402      |
|    explained_variance   | 0.895         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000132     |
|    n_updates            | 1500          |
|    policy_gradient_loss | -4.08e-05     |
|    value_loss           | 1.96e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 1           |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 152         |
|    time_elapsed         | 103         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 5.47152e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00321    |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -4.02e-05   |
|    n_updates            | 1510        |
|    policy_gradient_loss | -4.12e-06   |
|    value_loss           | 1.38e-05    |
-----------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.01         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 153          |
|    time_elapsed         | 103          |
|    total_timesteps      | 78336        |
| train/                  |              |
|    approx_kl            | 3.410969e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00316     |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0003       |
|    loss                 | -2.49e-05    |
|    n_updates            | 1520         |
|    policy_gradient_loss | 5.92e-08     |
|    value_loss           | 1.39e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.02          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 154           |
|    time_elapsed         | 104           |
|    total_timesteps      | 78848         |
| train/                  |               |
|    approx_kl            | 1.4319085e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0033       |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.0003        |
|    loss                 | -3.04e-05     |
|    n_updates            | 1530          |
|    policy_gradient_loss | 1.46e-07      |
|    value_loss           | 1.3e-05       |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.03          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 155           |
|    time_elapsed         | 105           |
|    total_timesteps      | 79360         |
| train/                  |               |
|    approx_kl            | 2.1071173e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00348      |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.0003        |
|    loss                 | -3.52e-05     |
|    n_updates            | 1540          |
|    policy_gradient_loss | 2.59e-07      |
|    value_loss           | 1.39e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.04         |
| time/                   |              |
|    fps                  | 753          |
|    iterations           | 156          |
|    time_elapsed         | 105          |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 1.618173e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00375     |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0003       |
|    loss                 | -4.5e-05     |
|    n_updates            | 1550         |
|    policy_gradient_loss | 2.72e-07     |
|    value_loss           | 1.29e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.05          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 157           |
|    time_elapsed         | 106           |
|    total_timesteps      | 80384         |
| train/                  |               |
|    approx_kl            | 1.7462298e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00413      |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.2e-05      |
|    n_updates            | 1560          |
|    policy_gradient_loss | 5.98e-07      |
|    value_loss           | 1.35e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.06          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 158           |
|    time_elapsed         | 107           |
|    total_timesteps      | 80896         |
| train/                  |               |
|    approx_kl            | 2.5262125e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00473      |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.63e-05     |
|    n_updates            | 1570          |
|    policy_gradient_loss | 2.05e-07      |
|    value_loss           | 1.32e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.07          |
| time/                   |               |
|    fps                  | 753           |
|    iterations           | 159           |
|    time_elapsed         | 108           |
|    total_timesteps      | 81408         |
| train/                  |               |
|    approx_kl            | 5.3318217e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00602      |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.8e-05      |
|    n_updates            | 1580          |
|    policy_gradient_loss | 1.64e-06      |
|    value_loss           | 1.28e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.08          |
| time/                   |               |
|    fps                  | 752           |
|    iterations           | 160           |
|    time_elapsed         | 108           |
|    total_timesteps      | 81920         |
| train/                  |               |
|    approx_kl            | 8.2247425e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00962      |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.43e-05     |
|    n_updates            | 1590          |
|    policy_gradient_loss | 4.35e-06      |
|    value_loss           | 1.28e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.09         |
| time/                   |              |
|    fps                  | 751          |
|    iterations           | 161          |
|    time_elapsed         | 109          |
|    total_timesteps      | 82432        |
| train/                  |              |
|    approx_kl            | 0.0013244125 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00569     |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000168    |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 7.13e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.08          |
|    ep_rew_mean          | 1.1           |
| time/                   |               |
|    fps                  | 750           |
|    iterations           | 162           |
|    time_elapsed         | 110           |
|    total_timesteps      | 82944         |
| train/                  |               |
|    approx_kl            | 1.1990778e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0064       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.0003        |
|    loss                 | -7.36e-05     |
|    n_updates            | 1610          |
|    policy_gradient_loss | 3.74e-06      |
|    value_loss           | 1.68e-05      |
-------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.11           |
| time/                   |                |
|    fps                  | 750            |
|    iterations           | 163            |
|    time_elapsed         | 111            |
|    total_timesteps      | 83456          |
| train/                  |                |
|    approx_kl            | 0.000102728256 |
|    clip_fraction        | 0.00352        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00475       |
|    explained_variance   | 0.713          |
|    learning_rate        | 0.0003         |
|    loss                 | 6.65e-05       |
|    n_updates            | 1620           |
|    policy_gradient_loss | -0.00452       |
|    value_loss           | 0.000115       |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.12         |
| time/                   |              |
|    fps                  | 749          |
|    iterations           | 164          |
|    time_elapsed         | 111          |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 3.958121e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0029      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0003       |
|    loss                 | -8.78e-06    |
|    n_updates            | 1630         |
|    policy_gradient_loss | -1.73e-05    |
|    value_loss           | 1.74e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.13          |
| time/                   |               |
|    fps                  | 749           |
|    iterations           | 165           |
|    time_elapsed         | 112           |
|    total_timesteps      | 84480         |
| train/                  |               |
|    approx_kl            | 2.4447218e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00305      |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.63e-05     |
|    n_updates            | 1640          |
|    policy_gradient_loss | -4.37e-07     |
|    value_loss           | 1.29e-05      |
-------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.14           |
| time/                   |                |
|    fps                  | 749            |
|    iterations           | 166            |
|    time_elapsed         | 113            |
|    total_timesteps      | 84992          |
| train/                  |                |
|    approx_kl            | -1.7345883e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00317       |
|    explained_variance   | 0.977          |
|    learning_rate        | 0.0003         |
|    loss                 | -1.89e-05      |
|    n_updates            | 1650           |
|    policy_gradient_loss | 2.22e-07       |
|    value_loss           | 1.33e-05       |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.15         |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 167          |
|    time_elapsed         | 114          |
|    total_timesteps      | 85504        |
| train/                  |              |
|    approx_kl            | 5.122274e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00327     |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | -1.88e-05    |
|    n_updates            | 1660         |
|    policy_gradient_loss | 1.44e-07     |
|    value_loss           | 1.39e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.16         |
| time/                   |              |
|    fps                  | 748          |
|    iterations           | 168          |
|    time_elapsed         | 114          |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 3.259629e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00346     |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | -1.48e-05    |
|    n_updates            | 1670         |
|    policy_gradient_loss | -2.3e-07     |
|    value_loss           | 1.37e-05     |
------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.17           |
| time/                   |                |
|    fps                  | 747            |
|    iterations           | 169            |
|    time_elapsed         | 115            |
|    total_timesteps      | 86528          |
| train/                  |                |
|    approx_kl            | -1.4319085e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00363       |
|    explained_variance   | 0.978          |
|    learning_rate        | 0.0003         |
|    loss                 | -3.47e-05      |
|    n_updates            | 1680           |
|    policy_gradient_loss | 2.86e-07       |
|    value_loss           | 1.27e-05       |
--------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.18         |
| time/                   |              |
|    fps                  | 747          |
|    iterations           | 170          |
|    time_elapsed         | 116          |
|    total_timesteps      | 87040        |
| train/                  |              |
|    approx_kl            | 9.662472e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00387     |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0003       |
|    loss                 | -2.34e-05    |
|    n_updates            | 1690         |
|    policy_gradient_loss | -4.01e-07    |
|    value_loss           | 1.27e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.21          |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 171           |
|    time_elapsed         | 117           |
|    total_timesteps      | 87552         |
| train/                  |               |
|    approx_kl            | 3.0267984e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00411      |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.93e-05     |
|    n_updates            | 1700          |
|    policy_gradient_loss | 5.42e-07      |
|    value_loss           | 1.32e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.43          |
| time/                   |               |
|    fps                  | 746           |
|    iterations           | 172           |
|    time_elapsed         | 118           |
|    total_timesteps      | 88064         |
| train/                  |               |
|    approx_kl            | 1.0128133e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00434      |
|    explained_variance   | 0.0802        |
|    learning_rate        | 0.0003        |
|    loss                 | 0.00317       |
|    n_updates            | 1710          |
|    policy_gradient_loss | 1.07e-07      |
|    value_loss           | 0.00426       |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.42         |
| time/                   |              |
|    fps                  | 745          |
|    iterations           | 173          |
|    time_elapsed         | 118          |
|    total_timesteps      | 88576        |
| train/                  |              |
|    approx_kl            | 6.810296e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00506     |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.0003       |
|    loss                 | -3.81e-05    |
|    n_updates            | 1720         |
|    policy_gradient_loss | -6.9e-06     |
|    value_loss           | 0.00203      |
------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.41           |
| time/                   |                |
|    fps                  | 745            |
|    iterations           | 174            |
|    time_elapsed         | 119            |
|    total_timesteps      | 89088          |
| train/                  |                |
|    approx_kl            | -1.6298145e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00592       |
|    explained_variance   | 0.85           |
|    learning_rate        | 0.0003         |
|    loss                 | -4.3e-05       |
|    n_updates            | 1730           |
|    policy_gradient_loss | 1.03e-06       |
|    value_loss           | 4.1e-05        |
--------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.4           |
| time/                   |               |
|    fps                  | 744           |
|    iterations           | 175           |
|    time_elapsed         | 120           |
|    total_timesteps      | 89600         |
| train/                  |               |
|    approx_kl            | 1.2456439e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00581      |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.0003        |
|    loss                 | -9.37e-05     |
|    n_updates            | 1740          |
|    policy_gradient_loss | -3.64e-06     |
|    value_loss           | 1.82e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.01         |
|    ep_rew_mean          | 1.39         |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 176          |
|    time_elapsed         | 121          |
|    total_timesteps      | 90112        |
| train/                  |              |
|    approx_kl            | 3.434252e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00593     |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | -6.68e-05    |
|    n_updates            | 1750         |
|    policy_gradient_loss | -1.32e-07    |
|    value_loss           | 1.38e-05     |
------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.38         |
| time/                   |              |
|    fps                  | 744          |
|    iterations           | 177          |
|    time_elapsed         | 121          |
|    total_timesteps      | 90624        |
| train/                  |              |
|    approx_kl            | 0.0010947421 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00556     |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.000115     |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 1.59e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.37          |
| time/                   |               |
|    fps                  | 743           |
|    iterations           | 178           |
|    time_elapsed         | 122           |
|    total_timesteps      | 91136         |
| train/                  |               |
|    approx_kl            | 2.4149194e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00981      |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.0003        |
|    loss                 | -0.000618     |
|    n_updates            | 1770          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 1.34e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.36         |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 179          |
|    time_elapsed         | 123          |
|    total_timesteps      | 91648        |
| train/                  |              |
|    approx_kl            | 0.0011550684 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00761     |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.0003       |
|    loss                 | -4.94e-05    |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 9.72e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.04          |
|    ep_rew_mean          | 1.35          |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 180           |
|    time_elapsed         | 124           |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 6.6903885e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.43e-06     |
|    n_updates            | 1790          |
|    policy_gradient_loss | 1.6e-05       |
|    value_loss           | 9.88e-06      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.34         |
| time/                   |              |
|    fps                  | 742          |
|    iterations           | 181          |
|    time_elapsed         | 124          |
|    total_timesteps      | 92672        |
| train/                  |              |
|    approx_kl            | 0.0017913722 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00667     |
|    explained_variance   | 0.79         |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000121    |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00224     |
|    value_loss           | 9.92e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.33          |
| time/                   |               |
|    fps                  | 742           |
|    iterations           | 182           |
|    time_elapsed         | 125           |
|    total_timesteps      | 93184         |
| train/                  |               |
|    approx_kl            | 1.7834827e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00789      |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -2.81e-05     |
|    n_updates            | 1810          |
|    policy_gradient_loss | 3.37e-06      |
|    value_loss           | 1.39e-05      |
-------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 1.32        |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 183         |
|    time_elapsed         | 126         |
|    total_timesteps      | 93696       |
| train/                  |             |
|    approx_kl            | 0.002720292 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00482    |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -6.99e-07   |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 1.86e-05    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.31          |
| time/                   |               |
|    fps                  | 741           |
|    iterations           | 184           |
|    time_elapsed         | 127           |
|    total_timesteps      | 94208         |
| train/                  |               |
|    approx_kl            | 1.3504177e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00463      |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.0003        |
|    loss                 | -9.29e-06     |
|    n_updates            | 1830          |
|    policy_gradient_loss | -4.09e-07     |
|    value_loss           | 1.29e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.01          |
|    ep_rew_mean          | 1.3           |
| time/                   |               |
|    fps                  | 740           |
|    iterations           | 185           |
|    time_elapsed         | 127           |
|    total_timesteps      | 94720         |
| train/                  |               |
|    approx_kl            | 2.0489097e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00523      |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.66e-05     |
|    n_updates            | 1840          |
|    policy_gradient_loss | 1.62e-07      |
|    value_loss           | 1.31e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5.06         |
|    ep_rew_mean          | 1.29         |
| time/                   |              |
|    fps                  | 740          |
|    iterations           | 186          |
|    time_elapsed         | 128          |
|    total_timesteps      | 95232        |
| train/                  |              |
|    approx_kl            | 0.0012314877 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00721     |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.000225    |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 1.59e-05     |
------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 1.28        |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 187         |
|    time_elapsed         | 129         |
|    total_timesteps      | 95744       |
| train/                  |             |
|    approx_kl            | 0.002637712 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00358    |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.000346    |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00411    |
|    value_loss           | 7.28e-05    |
-----------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.27          |
| time/                   |               |
|    fps                  | 739           |
|    iterations           | 188           |
|    time_elapsed         | 130           |
|    total_timesteps      | 96256         |
| train/                  |               |
|    approx_kl            | 2.4680048e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00358      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -6.74e-05     |
|    n_updates            | 1870          |
|    policy_gradient_loss | -3.36e-06     |
|    value_loss           | 1.16e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.26         |
| time/                   |              |
|    fps                  | 739          |
|    iterations           | 189          |
|    time_elapsed         | 130          |
|    total_timesteps      | 96768        |
| train/                  |              |
|    approx_kl            | 6.170012e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00483     |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0003       |
|    loss                 | -6.32e-05    |
|    n_updates            | 1880         |
|    policy_gradient_loss | -3.14e-06    |
|    value_loss           | 1.41e-05     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.25          |
| time/                   |               |
|    fps                  | 739           |
|    iterations           | 190           |
|    time_elapsed         | 131           |
|    total_timesteps      | 97280         |
| train/                  |               |
|    approx_kl            | 1.8061837e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00933      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0003        |
|    loss                 | -0.00032      |
|    n_updates            | 1890          |
|    policy_gradient_loss | -4.36e-06     |
|    value_loss           | 1.31e-05      |
-------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5.09          |
|    ep_rew_mean          | 1.24          |
| time/                   |               |
|    fps                  | 738           |
|    iterations           | 191           |
|    time_elapsed         | 132           |
|    total_timesteps      | 97792         |
| train/                  |               |
|    approx_kl            | 7.0536043e-07 |
|    clip_fraction        | 0.0197        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0565       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.0003        |
|    loss                 | -4.97e-05     |
|    n_updates            | 1900          |
|    policy_gradient_loss | 0.000422      |
|    value_loss           | 1.32e-05      |
-------------------------------------------
------------------------------------------
| adaptive/               |              |
|    adaptation_factor    | 1            |
|    algorithm            | PPO          |
|    base_clip_range      | 0.2          |
|    base_lr              | 0.0003       |
|    clip_range           | 0.2          |
|    drift_magnitude      | 0            |
|    ent_coef             | 0.01         |
|    learning_rate        | 0.0003       |
| env/                    |              |
|    base_value           | 9.8          |
|    reward_scale         | 9.8          |
| rollout/                |              |
|    ep_len_mean          | 5            |
|    ep_rew_mean          | 1.23         |
| time/                   |              |
|    fps                  | 738          |
|    iterations           | 192          |
|    time_elapsed         | 133          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0028372414 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0056      |
|    explained_variance   | 0.704        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.33e-05     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00582     |
|    value_loss           | 0.000139     |
------------------------------------------
-------------------------------------------
| adaptive/               |               |
|    adaptation_factor    | 1             |
|    algorithm            | PPO           |
|    base_clip_range      | 0.2           |
|    base_lr              | 0.0003        |
|    clip_range           | 0.2           |
|    drift_magnitude      | 0             |
|    ent_coef             | 0.01          |
|    learning_rate        | 0.0003        |
| env/                    |               |
|    base_value           | 9.8           |
|    reward_scale         | 9.8           |
| rollout/                |               |
|    ep_len_mean          | 5             |
|    ep_rew_mean          | 1.22          |
| time/                   |               |
|    fps                  | 738           |
|    iterations           | 193           |
|    time_elapsed         | 133           |
|    total_timesteps      | 98816         |
| train/                  |               |
|    approx_kl            | 3.7252903e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00558      |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.0003        |
|    loss                 | -5.46e-05     |
|    n_updates            | 1920          |
|    policy_gradient_loss | -5.83e-07     |
|    value_loss           | 1.05e-05      |
-------------------------------------------
--------------------------------------------
| adaptive/               |                |
|    adaptation_factor    | 1              |
|    algorithm            | PPO            |
|    base_clip_range      | 0.2            |
|    base_lr              | 0.0003         |
|    clip_range           | 0.2            |
|    drift_magnitude      | 0              |
|    ent_coef             | 0.01           |
|    learning_rate        | 0.0003         |
| env/                    |                |
|    base_value           | 9.8            |
|    reward_scale         | 9.8            |
| rollout/                |                |
|    ep_len_mean          | 5              |
|    ep_rew_mean          | 1.21           |
| time/                   |                |
|    fps                  | 738            |
|    iterations           | 194            |
|    time_elapsed         | 134            |
|    total_timesteps      | 99328          |
| train/                  |                |
|    approx_kl            | 1.00932084e-07 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00625       |
|    explained_variance   | 0.977          |
|    learning_rate        | 0.0003         |
|    loss                 | 9.58e-06       |
|    n_updates            | 1930           |
|    policy_gradient_loss | -2.56e-06      |
|    value_loss           | 1.35e-05       |
--------------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5.13        |
|    ep_rew_mean          | 1.2         |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 195         |
|    time_elapsed         | 135         |
|    total_timesteps      | 99840       |
| train/                  |             |
|    approx_kl            | 4.23406e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0172     |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000692   |
|    n_updates            | 1940        |
|    policy_gradient_loss | -5.37e-05   |
|    value_loss           | 1.35e-05    |
-----------------------------------------
-----------------------------------------
| adaptive/               |             |
|    adaptation_factor    | 1           |
|    algorithm            | PPO         |
|    base_clip_range      | 0.2         |
|    base_lr              | 0.0003      |
|    clip_range           | 0.2         |
|    drift_magnitude      | 0           |
|    ent_coef             | 0.01        |
|    learning_rate        | 0.0003      |
| env/                    |             |
|    base_value           | 9.8         |
|    reward_scale         | 9.8         |
| rollout/                |             |
|    ep_len_mean          | 5           |
|    ep_rew_mean          | 1.03        |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 196         |
|    time_elapsed         | 136         |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.012977483 |
|    clip_fraction        | 0.00742     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0113     |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00305     |
|    n_updates            | 1950        |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 0.000152    |
-----------------------------------------
wandb: WARNING Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.
wandb: updating run metadata
wandb: uploading model.zip; uploading output.log; uploading wandb-summary.json
wandb: uploading model.zip; uploading output.log; uploading config.yaml; uploading logs/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124_0/events.out.tfevents.1766028195.hungchan-Precision-7560.511965.0
wandb: uploading logs/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124_0/events.out.tfevents.1766028195.hungchan-Precision-7560.511965.0
wandb: uploading history steps 3918-4106, summary, console lines 6376-6479
wandb: 
wandb: Run history:
wandb: adaptive/adaptation_factor â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/base_clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           adaptive/base_lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        adaptive/clip_range â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   adaptive/drift_magnitude â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          adaptive/ent_coef â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     adaptive/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             env/base_value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           env/reward_scale â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                global_step â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                        +12 ...
wandb: 
wandb: Run summary:
wandb: adaptive/adaptation_factor 1
wandb:   adaptive/base_clip_range 0.2
wandb:           adaptive/base_lr 0.0003
wandb:        adaptive/clip_range 0.2
wandb:   adaptive/drift_magnitude 0
wandb:          adaptive/ent_coef 0.01
wandb:     adaptive/learning_rate 0.0003
wandb:             env/base_value 9.8
wandb:           env/reward_scale 9.8
wandb:                global_step 100352
wandb:                        +12 ...
wandb: 
wandb: ðŸš€ View run MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124 at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research/runs/z1cpksky
wandb: â­ï¸ View project at: https://wandb.ai/hungtrab-hanoi-university-of-science-and-technology/MiniGrid_Drift_Research
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: logs/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124/wandb/run-20251218_102313-z1cpksky/logs
>>> [DriftAdaptiveCallback] Training Ended
    Final LR: 0.000300
    Last Drift Magnitude: 0.0000
    Final Clip Range: 0.2000
Model saved locally to: models/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124.zip
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: [33mWARN: Overwriting existing videos at /home/hungchan/Work/Deep-RL/videos/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
/home/hungchan/miniconda3/envs/rl_hf_course/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
>>> [Wrapper] Initialized Non-Stationary MiniGrid
    - reward_scale: linear
Loading PPO model from: models/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124.zip

Recording Episode 1/1...
  Episode finished: 5 steps, reward = 1.0

Videos saved to: videos/MiniGrid_Empty-5x5_reward_scale_linear_Adaptive_20251218_101124
